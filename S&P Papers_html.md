# Security & Privacy Papers

> Including S&P, CCS, USENIX Security and NDSS conferences from 2020 to 2022.



## Markdown File

### 2023

#### S&P 2023

|             | S&P 2023 [<a href="https://www.ieee-security.org/TC/SP2023/index.html">Homepage</a>] [<a href="https://www.ieee-security.org/TC/SP2023/program-papers.html">Accepted</a>] |                                                    |
| ----------- | ------------------------------------------------------------ | -------------------------------------------------- |
| First Cycle | FedRecover: Recovering from Poisoning Attacks in Federated Learning using Historical Information | <a href="https://arxiv.org/pdf/2210.10936">PDF</a> |
|             | Private, Efficient, and Accurate: Protecting Models Trained by Multi-party Learning with Differential Privacy | <a href="https://arxiv.org/pdf/2208.08662">PDF</a> |
|             | D-DAE: Defense-Penetrating Model Extraction Attacks          |                                                    |
|             | Examining Zero-Shot Vulnerability Repair with Large Language Models | <a href="https://arxiv.org/pdf/2112.02125">PDF</a> |
|             | SoK: Certified Robustness for Deep Neural Networks           | <a href="https://arxiv.org/pdf/2009.04131">PDF</a> |
|             | RAB: Provable Robustness Against Backdoor Attacks            | <a href="https://arxiv.org/pdf/2003.08904">PDF</a> |
|             | SoK: Anti-Facial Recognition Technology                      | <a href="https://arxiv.org/pdf/2112.04558">PDF</a> |
|             | Deepfake Text Detection: Limitations and Opportunities       | <a href="https://arxiv.org/pdf/2210.09421">PDF</a> |

#### SEC 2023


|              | USENIX Security 2023  [<a href="https://www.usenix.org/conference/usenixsecurity23">Homepage</a>] [<a href="https://www.usenix.org/conference/usenixsecurity23/summer-accepted-papers">Accepted</a>] |                                                              |
| ------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| Summer Cycle | V-Cloak: Intelligibility-, Naturalness- & Timbre-Preserving Real-Time Voice Anonymization | <a href="https://www.usenix.org/conference/usenixsecurity23/presentation/deng">Link</a><br/><a href="https://arxiv.org/pdf/2210.15140">PDF</a> |
|              | TPatch: A Triggered Physical Adversarial Patch               | <a href="https://www.usenix.org/conference/usenixsecurity23/presentation/zhu">Link</a><br/><a href="https://www.usenix.org/system/files/sec23summer_123-zhu-prepub.pdf">PDF</a> |
|              | UnGANable: Defending Against GAN-based Face Manipulation     | <a href="https://www.usenix.org/conference/usenixsecurity23/presentation/lizheng">Link</a><br/><a href="https://www.usenix.org/system/files/sec23summer_136-li_zheng-prepub.pdf">PDF</a> |
|              | Squint Hard Enough: Attacking Perceptual Hashing with Adversarial Machine Learning | <a href="https://www.usenix.org/conference/usenixsecurity23/presentation/prokos">Link</a><br/><a href="https://www.usenix.org/system/files/sec23summer_146-prokos-prepub.pdf">PDF</a> |
|              | PrivTrace: Differentially Private Trajectory Synthesis by Adaptive Markov Models | <a href="https://www.usenix.org/conference/usenixsecurity23/presentation/wanghaiming">Link</a><br/><a href="https://www.usenix.org/system/files/sec23summer_179-wang_haiming-prepub.pdf">PDF</a> |
|              | The Space of Adversarial Strategies                          | <a href="https://www.usenix.org/conference/usenixsecurity23/presentation/sheatsley">Link</a><br/><a href="https://www.usenix.org/system/files/sec23summer_256-sheatsley-prepub.pdf">PDF</a> |
|              | That Person Moves Like A Car: Misclassification Attack Detection for Autonomous Systems Using Spatiotemporal Consistency | <a href="https://www.usenix.org/conference/usenixsecurity23/presentation/man">Link</a><br/><a href="https://www.usenix.org/system/files/sec23summer_278-man-prepub.pdf">PDF</a> |
|              | NeuroPots: Realtime Proactive Defense against Bit-Flip Attacks in Neural Networks | <a href="https://www.usenix.org/conference/usenixsecurity23/presentation/liuqi">Link</a><br/><a href="https://www.usenix.org/system/files/sec23summer_334-liu_qi-prepub.pdf">PDF</a> |
|              | URET: Universal Robustness Evaluation Toolkit (for Evasion)  | <a href="https://www.usenix.org/conference/usenixsecurity23/presentation/eykholt">Link</a><br/><a href="https://www.usenix.org/system/files/sec23summer_347-eykholt-prepub.pdf">PDF</a> |
|              | Towards a General Video-based Keystroke Inference Attack     | <a href="https://www.usenix.org/conference/usenixsecurity23/presentation/yangzhuolin">Link</a><br/><a href="https://www.usenix.org/system/files/sec23summer_338-yang_zhuolin-prepub.pdf">PDF</a> |
|              | You Can't See Me: Physical Removal Attacks on LiDAR-based Autonomous Vehicles Driving Frameworks | <a href="https://www.usenix.org/conference/usenixsecurity23/presentation/cao">Link</a><br/><a href="https://www.usenix.org/system/files/sec23summer_349-cao-prepub.pdf">PDF</a> |
|              | SMACK: Semantically Meaningful Adversarial Audio Attack      | <a href="https://www.usenix.org/conference/usenixsecurity23/presentation/yuzhiyuan">Link</a><br/><a href="https://www.usenix.org/system/files/sec23summer_371-yu_zhiyuan-prepub.pdf">PDF</a> |
|              | Gradient Obfuscation Gives a False Sense of Security in Federated Learning | <a href="https://www.usenix.org/conference/usenixsecurity23/presentation/yue">Link</a><br/><a href="https://www.usenix.org/system/files/sec23summer_372-yue-prepub.pdf">PDF</a> |
|              | Fairness Properties of Face Recognition and Obfuscation Systems | <a href="https://www.usenix.org/conference/usenixsecurity23/presentation/rosenberg">Link</a><br/><a href="https://www.usenix.org/system/files/sec23summer_388-rosenberg-prepub.pdf">PDF</a> |
|              | Decompiling x86 Deep Neural Network Executables              | <a href="https://www.usenix.org/conference/usenixsecurity23/presentation/liuzhibo">Link</a><br/><a href="https://www.usenix.org/system/files/sec23summer_406-liu_zhibo-prepub.pdf">PDF</a> |
|              | PCAT: Functionality and Data Stealing from Split Learning by Pseudo-Client Attack | <a href="https://www.usenix.org/conference/usenixsecurity23/presentation/gao">Link</a><br/><a href="https://www.usenix.org/system/files/sec23summer_445-gao-prepub.pdf">PDF</a> |



### 2022

#### S&P 2022

|                            | S&P 2022 &nbsp; [<a href="https://www.ieee-security.org/TC/SP2022/index.html">Homepage</a>] [<a href="https://www.ieee-security.org/TC/SP2022/program-papers.html">Accepted</a>] |                                                              |
| -------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| ML Attacks                 | Back to the Drawing Board: A Critical Evaluation of Poisoning Attacks on Production Federated Learning | <a href="https://ieeexplore.ieee.org/document/9833647/">Link</a><br/><a href="https://arxiv.org/pdf/2108.10241">PDF</a> |
|                            | Model Orthogonalization: Class Distance Hardening in Neural Networks for Better Security | <a href="https://ieeexplore.ieee.org/document/9833688/">Link</a><br/><a href="https://www.cs.purdue.edu/homes/zhan3299/res/SP22_Tao.pdf">PDF</a> |
|                            | Universal 3-Dimensional Perturbations for Black-Box Attacks on Video Recognition Systems | <a href="https://ieeexplore.ieee.org/document/9833776/">Link</a><br/><a href="https://arxiv.org/pdf/2107.04284">PDF</a> |
|                            | "Adversarial Examples" for Proof-of-Learning                 | <a href="https://ieeexplore.ieee.org/document/9833596/">Link</a><br/><a href="https://arxiv.org/pdf/2108.09454">PDF</a> |
|                            | Transfer Attacks Revisited: A Large-Scale Empirical Study in Real Computer Vision Settings | <a href="https://ieeexplore.ieee.org/document/9833783/">Link</a><br/><a href="https://arxiv.org/pdf/2204.04063">PDF</a> |
|                            | Membership Inference Attacks From First Principles           | <a href="https://ieeexplore.ieee.org/document/9833649/">Link</a><br/><a href="https://arxiv.org/pdf/2112.03570">PDF</a> |
|                            | Bad Characters: Imperceptible NLP Attacks                    | <a href="https://ieeexplore.ieee.org/document/9833641/">Link</a><br/><a href="https://arxiv.org/pdf/2106.09898">PDF</a> |
|                            | LinkTeller: Recovering Private Edges from Graph Neural Networks via Influence Analysis | <a href="https://ieeexplore.ieee.org/document/9833806/">Link</a><br/><a href="https://arxiv.org/pdf/2108.06504">PDF</a> |
|                            | PICCOLO: Exposing Complex Backdoors in NLP Transformer Models | <a href="https://ieeexplore.ieee.org/document/9833579/">Link</a><br/><a href="https://www.cs.purdue.edu/homes/taog/docs/SP22_Liu.pdf">PDF</a> |
|                            | BadEncoder: Backdoor Attacks to Pre-trained Encoders in Self-Supervised Learning | <a href="https://ieeexplore.ieee.org/document/9833644/">Link</a><br/><a href="https://arxiv.org/pdf/2108.00352">PDF</a> |
| Poisoning & Model Stealing | Property Inference from Poisoning                            | <a href="https://ieeexplore.ieee.org/document/9833623/">Link</a><br/><a href="https://arxiv.org/pdf/2101.11073">PDF</a> |
|                            | Reconstructing Training Data with Informed Adversaries       | <a href="https://ieeexplore.ieee.org/document/9833677/">Link</a><br/><a href="https://arxiv.org/pdf/2201.04845">PDF</a> |
|                            | DeepSteal: Advanced Model Extractions Leveraging Efficient Weight Stealing in Memories | <a href="https://ieeexplore.ieee.org/document/9833743/">Link</a><br/><a href="https://arxiv.org/pdf/2111.04625">PDF</a> |
|                            | Model Stealing Attacks Against Inductive Graph Neural Networks | <a href="https://ieeexplore.ieee.org/document/9833607/">Link</a><br/><a href="https://arxiv.org/pdf/2112.08331">PDF</a> |
| Applications of ML         | Asleep at the Keyboard? Assessing the Security of GitHub Copilot’s Code Contributions | <a href="https://ieeexplore.ieee.org/document/9833571/">Link</a><br/><a href="https://arxiv.org/pdf/2108.09293">PDF</a> |
|                            | Spinning Language Models: Risks of Propaganda-as-a-Service and Countermeasures | <a href="https://ieeexplore.ieee.org/document/9833572/">Link</a><br/><a href="https://arxiv.org/pdf/2112.05224">PDF</a> |
|                            | SoK: How Robust is Image Classification Deep Neural Network Watermarking? | <a href="https://ieeexplore.ieee.org/document/9833693/">Link</a><br/><a href="https://arxiv.org/pdf/2108.04974">PDF</a> |
|                            | Transcending TRANSCEND: Revisiting Malware Classification in the Presence of Concept Drift | <a href="https://ieeexplore.ieee.org/document/9833659/">Link</a><br/><a href="https://arxiv.org/pdf/2010.03856">PDF</a> |
|                            | Copy, Right? A Testing Framework for Copyright Protection of Deep Learning Models | <a href="https://ieeexplore.ieee.org/document/9833747/">Link</a><br/><a href="https://arxiv.org/pdf/2112.05588">PDF</a> |
| Differential Privacy       | Statistical Quantification of Differential Privacy: A Local Approach | <a href="https://ieeexplore.ieee.org/document/9833689/">Link</a><br/><a href="https://arxiv.org/pdf/2108.09528">PDF</a> |
|                            | Locally Differentially Private Sparse Vector Aggregation     | <a href="https://ieeexplore.ieee.org/document/9833635/">Link</a><br/><a href="https://arxiv.org/pdf/2112.03449">PDF</a> |
|                            | Differentially Private Histograms in the Shuffle Model from Fake Users | <a href="https://ieeexplore.ieee.org/document/9833614/">Link</a><br/><a href="https://arxiv.org/pdf/2104.02739">PDF</a> |
|                            | Differential Privacy and Swapping: Examining De-Identification's Impact on Minority Representation and Privacy Preservation in the U.S. Census | <a href="https://ieeexplore.ieee.org/document/9833668/">Link</a><br/><a href="https://www.cs.columbia.edu/~mchrist/dp_swap_census.pdf">PDF</a> |
|                            | Are We There Yet? Timing and Floating-Point Attacks on Differential Privacy Systems | <a href="https://ieeexplore.ieee.org/document/9833672/">Link</a><br/><a href="https://arxiv.org/pdf/2112.05307">PDF</a> |
| Privacy Applications       | Sphinx: Enabling Privacy-Preserving Online Learning over the Cloud | <a href="https://ieeexplore.ieee.org/document/9833648/">Link</a><br/><a href="https://cse.hkust.edu.hk/~kaichen/papers/sphinx-sp22.pdf">PDF</a> |

#### CCS 2022

|                                | CCS 2022 [<a href="https://www.sigsac.org/ccs/CCS2022/home.html">Homepage</a>] [<a href="https://www.sigsac.org/ccs/CCS2022/program/accepted-papers.html">Accepted</a>] |                                                              |
| ------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| Inference Attacks to ML        | Group Property Inference Attacks Against Graph Neural Networks | <a href="https://dl.acm.org/doi/10.1145/3548606.3560662">Link</a><br/><a href="https://arxiv.org/pdf/2209.01100">PDF</a> |
|                                | Are Attribute Inference Attacks Just Imputation?             | <a href="https://dl.acm.org/doi/10.1145/3548606.3560663">Link</a><br/><a href="https://arxiv.org/pdf/2209.01292">PDF</a> |
|                                | Enhanced Membership Inference Attacks against Machine Learning Models | <a href="https://dl.acm.org/doi/10.1145/3548606.3560675">Link</a><br/><a href="https://arxiv.org/pdf/2111.09679">PDF</a> |
|                                | Membership Inference Attacks and Generalization: A Causal Perspective | <a href="https://dl.acm.org/doi/10.1145/3548606.3560694">Link</a><br/><a href="https://arxiv.org/pdf/2209.08615">PDF</a> |
| Privacy Attacks in ML          | SSLGuard: A Watermarking Scheme for Self-supervised Learning Pre-trained Encoders | <a href="https://dl.acm.org/doi/10.1145/3548606.3559355">Link</a><br/><a href="https://arxiv.org/pdf/2201.11692">PDF</a> |
|                                | Auditing Membership Leakages of Multi-Exit Networks          | <a href="https://dl.acm.org/doi/10.1145/3548606.3559359">Link</a><br/><a href="https://arxiv.org/pdf/2208.11180">PDF</a> |
|                                | StolenEncoder: Stealing Pre-trained Encoders                 | <a href="https://dl.acm.org/doi/10.1145/3548606.3560586">Link</a><br/><a href="https://arxiv.org/pdf/2201.05889">PDF</a> |
|                                | Membership Inference Attacks by Exploiting Loss Trajectory   | <a href="https://dl.acm.org/doi/10.1145/3548606.3560684">Link</a><br/><a href="https://arxiv.org/pdf/2208.14933">PDF</a> |
| Privacy in Graphs              | Graph Unlearning                                             | <a href="https://dl.acm.org/doi/10.1145/3548606.3559352">Link</a><br/><a href="https://arxiv.org/pdf/2103.14991">PDF</a> |
|                                | Finding MNEMON: Reviving Memories of Node Embeddings         | <a href="https://dl.acm.org/doi/10.1145/3548606.3559358">Link</a><br/><a href="https://arxiv.org/pdf/2204.06963">PDF</a> |
| Privacy Preserving ML          | DPIS: an Enhanced Mechanism for Differentially Private SGD with Importance Sampling | <a href="https://dl.acm.org/doi/10.1145/3548606.3560562">Link</a><br/><a href="https://arxiv.org/pdf/2210.09634">PDF</a> |
|                                | NFGen: Automatic Non-linear Function Evaluation Code Generator for General-purpose MPC Platforms | <a href="https://dl.acm.org/doi/10.1145/3548606.3560565">Link</a><br/><a href="https://arxiv.org/pdf/2210.09802">PDF</a> |
|                                | On the Privacy Risks of Cell-Based NAS Architectures         | <a href="https://dl.acm.org/doi/10.1145/3548606.3560619">Link</a><br/><a href="https://arxiv.org/pdf/2209.01688">PDF</a> |
|                                | LPGNet: Link Private Graph Networks for Node Classification  | <a href="https://dl.acm.org/doi/10.1145/3548606.3560705">Link</a><br/><a href="https://arxiv.org/pdf/2205.03105">PDF</a> |
| Poisoning & Backdooring ML     | Identifying a Training-Set Attack’s Target Using Renormalized Influence Estimation | <a href="https://dl.acm.org/doi/10.1145/3548606.3559335">Link</a><br/><a href="https://arxiv.org/pdf/2201.10055">PDF</a> |
|                                | FenceSitter: Black-box, Content-Agnostic, and Synchronization-Free Enrollment-Phase Attacks on Speaker Recognition Systems | <a href="https://dl.acm.org/doi/10.1145/3548606.3559357">Link</a><br/><a href="">PDF</a> |
|                                | Truth Serum: Poisoning Machine Learning Models to Reveal Their Secrets | <a href="https://dl.acm.org/doi/10.1145/3548606.3560554">Link</a><br/><a href="https://arxiv.org/pdf/2204.00032">PDF</a> |
|                                | LoneNeuron: a Highly-effective Feature-domain Neural Trojan using Invisible and Polymorphic Watermarks | <a href="https://dl.acm.org/doi/10.1145/3548606.3560678">Link</a><br/><a href="https://www.ittc.ku.edu/~bluo/download/liu2022ccs.pdf">PDF</a> |
| Adversarial Examples in ML     | Perception-Aware Attack: Creating Adversarial Music via Reverse-Engineering Human Perception | <a href="https://dl.acm.org/doi/10.1145/3548606.3559350">Link</a><br/><a href="https://arxiv.org/pdf/2207.13192">PDF</a> |
|                                | "Is your explanation stable?": A Robustness Evaluation Framework for Feature Attribution | <a href="https://dl.acm.org/doi/10.1145/3548606.3559392">Link</a><br/><a href="https://arxiv.org/pdf/2209.01782">PDF</a> |
|                                | Post-breach Recovery: Protection against White-box Adversarial Examples for Leaked DNN Models | <a href="https://dl.acm.org/doi/10.1145/3548606.3560561">Link</a><br/><a href="https://arxiv.org/pdf/2205.10686">PDF</a> |
|                                | Harnessing Perceptual Adversarial Patches for Crowd Counting | <a href="https://dl.acm.org/doi/10.1145/3548606.3560566">Link<br/><a href="https://arxiv.org/pdf/2109.07986">PDF</a></a> |
| Attacks to ML                  | Physical Hijacking Attacks against Object Trackers           | <a href="https://dl.acm.org/doi/10.1145/3548606.3559390">Link</a><br/><a href="https://dl.acm.org/doi/pdf/10.1145/3548606.3559390">PDF</a> |
|                                | Feature Inference Attack on Shapley Values                   | <a href="https://dl.acm.org/doi/10.1145/3548606.3560573">Link</a><br/><a href="https://dl.acm.org/doi/pdf/10.1145/3548606.3560573">PDF</a> |
|                                | When Evil Calls: Targeted Adversarial Voice over IP-Telephony Network | <a href="https://dl.acm.org/doi/10.1145/3548606.3560671">Link</a><br/><a href="https://dl.acm.org/doi/pdf/10.1145/3548606.3560671">PDF</a> |
|                                | Order-Disorder: Imitation Adversarial Attacks for Black-box Neural Ranking Models | <a href="https://dl.acm.org/doi/10.1145/3548606.3560683">Link<br/><a href="https://arxiv.org/pdf/2209.06506">PDF</a></a> |
| FL Security                    | Eluding Secure Aggregation in Federated Learning via Model Inconsistency | <a href="https://dl.acm.org/doi/10.1145/3548606.3560557">Link</a><br/><a href="https://arxiv.org/pdf/2111.07380">PDF</a> |
|                                | EIFFeL: Ensuring Integrity for Federated Learning            | <a href="https://dl.acm.org/doi/10.1145/3548606.3560611">Link</a><br/><a href="https://arxiv.org/pdf/2112.12727">PDF</a> |
|                                | pMPL: A Robust Multi-Party Learning Framework with a Privileged Party | <a href="https://dl.acm.org/doi/10.1145/3548606.3560697">Link</a><br/><a href="https://arxiv.org/pdf/2210.00486">PDF</a> |
|                                | Private and Reliable Neural Network Inference                | <a href="https://dl.acm.org/doi/10.1145/3548606.3560709">Link</a><br/><a href="https://arxiv.org/pdf/2210.15614">PDF</a> |
| Federated Analytics & Learning | Distributed, Private, Sparse Histograms in the Two-Server Model | <a href="https://dl.acm.org/doi/10.1145/3548606.3559383">Link</a><br/><a href="https://eprint.iacr.org/2022/920.pdf">PDF</a> |
|                                | Selective MPC: Distributed Computation of Differentially Private Key-Value Statistics | <a href="https://dl.acm.org/doi/10.1145/3548606.3560559">Link</a><br/><a href="https://arxiv.org/pdf/2107.12407">PDF</a> |
|                                | STAR: Secret Sharing for Private Threshold Aggregation Reporting | <a href="https://dl.acm.org/doi/10.1145/3548606.3560631">Link</a><br/><a href="https://arxiv.org/pdf/2109.10074">PDF</a> |
|                                | Federated Boosted Decision Trees with Differential Privacy   | <a href="https://dl.acm.org/doi/10.1145/3548606.3560687">Link</a><br/><a href="https://arxiv.org/pdf/2210.02910">PDF</a> |
| Differential Privacy           | Shifted Inverse: A General Mechanism for Monotonic Functions under User Differential Privacy | <a href="https://dl.acm.org/doi/10.1145/3548606.3560567">Link</a><br/><a href="https://www.cse.ust.hk/~yike/ShiftedInverse.pdf">PDF</a> |
|                                | Frequency Estimation in the Shuffle Model with Almost a Single Message | <a href="https://dl.acm.org/doi/10.1145/3548606.3560608">Link<br/><a href="https://arxiv.org/pdf/2111.06833">PDF</a></a> |
|                                | Differentially Private Triangle and 4-Cycle Counting in the Shuffle Model | <a href="https://dl.acm.org/doi/10.1145/3548606.3560659">Link</a><br/><a href="https://arxiv.org/pdf/2205.01429">PDF</a> |
|                                | Widespread Underestimation of Sensitivity in Differentially Private Libraries and How to Fix It | <a href="https://dl.acm.org/doi/10.1145/3548606.3560708">Link</a><br/><a href="https://arxiv.org/pdf/2207.10635">PDF</a> |
| ML for Network Security        | CERBERUS: Federated Prediction of Security Events            | <a href="https://dl.acm.org/doi/10.1145/3548606.3560595">Link</a><br/><a href="https://arxiv.org/pdf/2209.03050">PDF</a> |
|                                | Exposing the Rat in the Tunnel: Using Traffic Analysis for Tor-based Malware Detection | <a href="https://dl.acm.org/doi/10.1145/3548606.3560604">Link</a><br/><a href="https://alrawi.io/static/papers/tor-malware_ccs22.pdf">PDF</a> |
|                                | AI/ML for Network Security: The Emperor has no Clothes       | <a href="https://dl.acm.org/doi/10.1145/3548606.3560609">Link<br/><a href="http://www.inf.ufrgs.br/~granville/wp-content/papercite-data/pdf/ccsjacobs2022.pdf">PDF</a></a> |
|                                | Phishing URL Detection: A Network-based Approach Robust to Evasion | <a href="https://dl.acm.org/doi/10.1145/3548606.3560615">Link</a><br/><a href="https://arxiv.org/pdf/2209.01454">PDF</a> |

#### SEC 2022


|                             | USENIX Security 2022 [<a href="https://www.usenix.org/conference/usenixsecurity22">Homepage</a>] [<a href="https://www.usenix.org/conference/usenixsecurity22/technical-sessions">Accepted</a>] |                                                              |
| --------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| Machine Learning            | PatchCleanser: Certifiably Robust Defense against Adversarial Patches for Any Image Classifier | <a href="https://www.usenix.org/conference/usenixsecurity22/presentation/xiang">Link</a><br/><a href="https://www.usenix.org/system/files/sec22fall_xiang.pdf">PDF</a> |
|                             | Transferring Adversarial Robustness Through Robust Representation Matching | <a href="https://www.usenix.org/conference/usenixsecurity22/presentation/vaishnavi">Link</a><br/><a href="https://www.usenix.org/system/files/sec22fall_vaishnavi.pdf">PDF</a> |
|                             | How Machine Learning Is Solving the Binary Function Similarity Problem | <a href="https://www.usenix.org/conference/usenixsecurity22/presentation/marcelli">Link</a><br/><a href="https://www.usenix.org/system/files/sec22fall_marcelli.pdf">PDF</a> |
|                             | Blacklight: Scalable Defense for Neural Networks against Query-Based Black-Box Attacks | <a href="https://www.usenix.org/conference/usenixsecurity22/presentation/li-huiying">Link</a><br/><a href="https://www.usenix.org/system/files/sec22-li-huiying.pdf">PDF</a> |
|                             | DnD: A Cross-Architecture Deep Neural Network Decompiler     | <a href="https://www.usenix.org/conference/usenixsecurity22/presentation/wu-ruoyu">Link</a><br/><a href="https://www.usenix.org/system/files/sec22-wu-ruoyu.pdf">PDF</a> |
|                             | Towards More Robust Keyword Spotting for Voice Assistants    | <a href="https://www.usenix.org/conference/usenixsecurity22/presentation/ahmed">Link</a><br/><a href="https://www.usenix.org/system/files/sec22summer_ahmed.pdf">PDF</a> |
|                             | Seeing is Living? Rethinking the Security of Facial Liveness Verification in the Deepfake Era | <a href="https://www.usenix.org/conference/usenixsecurity22/presentation/li-changjiang">Link</a><br/><a href="https://www.usenix.org/system/files/sec22fall_li-changjiang.pdf">PDF</a> |
|                             | Who Are You (I Really Wanna Know)? Detecting Audio DeepFakes Through Vocal Tract Reconstruction | <a href="https://www.usenix.org/conference/usenixsecurity22/presentation/blue">Link</a><br/><a href="https://www.usenix.org/system/files/sec22fall_blue.pdf">PDF</a> |
|                             | DeepDi: Learning a Relational Graph Convolutional Network Model on Instructions for Fast and Accurate Disassembly | <a href="https://www.usenix.org/conference/usenixsecurity22/presentation/yu-sheng">Link</a><br/><a href="https://www.usenix.org/system/files/sec22summer_yu-sheng.pdf">PDF</a> |
| Federated Learning          | SIMC: ML Inference Secure Against Malicious Clients at Semi-Honest Cost | <a href="https://www.usenix.org/conference/usenixsecurity22/presentation/chandran">Link</a><br/><a href="https://www.usenix.org/system/files/sec22summer_chandran.pdf">PDF</a> |
|                             | Efficient Differentially Private Secure Aggregation for Federated Learning via Hardness of Learning with Errors | <a href="https://www.usenix.org/conference/usenixsecurity22/presentation/stevens">Link</a><br/><a href="https://www.usenix.org/system/files/sec22fall_stevens.pdf">PDF</a> |
|                             | Label Inference Attacks Against Vertical Federated Learning  | <a href="https://www.usenix.org/conference/usenixsecurity22/presentation/fu-chong">Link</a><br/><a href="https://www.usenix.org/system/files/sec22summer_fu.pdf">PDF</a> |
|                             | FLAME: Taming Backdoors in Federated Learning                | <a href="https://www.usenix.org/conference/usenixsecurity22/presentation/nguyen">Link</a><br/><a href="https://www.usenix.org/system/files/sec22fall_nguyen.pdf">PDF</a> |
| ML Inference                | ML-Doctor: Holistic Risk Assessment of Inference Attacks Against Machine Learning Models | <a href="https://www.usenix.org/conference/usenixsecurity22/presentation/liu-yugeng">Link</a><br/><a href="https://www.usenix.org/system/files/sec22summer_liu-yugeng.pdf">PDF</a> |
|                             | Inference Attacks Against Graph Neural Networks              | <a href="https://www.usenix.org/conference/usenixsecurity22/presentation/zhang-zhikun">Link</a><br/><a href="https://www.usenix.org/system/files/sec22summer_zhang-zhikun.pdf">PDF</a> |
|                             | Membership Inference Attacks and Defenses in Neural Network Pruning | <a href="https://www.usenix.org/conference/usenixsecurity22/presentation/yuan-xiaoyong">Link</a><br/><a href="https://www.usenix.org/system/files/sec22fall_yuan-xiaoyong.pdf">PDF</a> |
|                             | Are Your Sensitive Attributes Private? Novel Model Inversion Attribute Inference Attacks on Classification Models | <a href="https://www.usenix.org/conference/usenixsecurity22/presentation/mehnaz">Link</a><br/><a href="https://www.usenix.org/system/files/sec22fall_mehnaz.pdf">PDF</a> |
| ML Attacks                  | AutoDA: Automated Decision-based Iterative Adversarial Attacks | <a href="https://www.usenix.org/conference/usenixsecurity22/presentation/fu-qi">Link</a><br/><a href="https://www.usenix.org/system/files/sec22-fu-qi.pdf">PDF</a> |
|                             | Poison Forensics: Traceback of Data Poisoning Attacks in Neural Networks | <a href="https://www.usenix.org/conference/usenixsecurity22/presentation/shan">Link</a><br/><a href="https://www.usenix.org/system/files/sec22-shan.pdf">PDF</a> |
|                             | Teacher Model Fingerprinting Attacks Against Transfer Learning | <a href="https://www.usenix.org/conference/usenixsecurity22/presentation/chen-yufei">Link</a><br/><a href="https://www.usenix.org/system/files/sec22-chen-yufei.pdf">PDF</a> |
|                             | Hidden Trigger Backdoor Attack on NLP Models via Linguistic Style Manipulation | <a href="https://www.usenix.org/conference/usenixsecurity22/presentation/pan-hidden">Link</a><br/><a href="https://www.usenix.org/system/files/sec22-pan-hidden.pdf">PDF</a> |
|                             | PoisonedEncoder: Poisoning the Unlabeled Pre-training Data in Contrastive Learning | <a href="https://www.usenix.org/conference/usenixsecurity22/presentation/liu-hongbin">Link</a><br/><a href="https://www.usenix.org/system/files/sec22-liu-hongbin.pdf">PDF</a> |
| Principles & Best Practices | On the Security Risks of AutoML                              | <a href="https://www.usenix.org/conference/usenixsecurity22/presentation/pang-ren">Link</a><br/><a href="https://www.usenix.org/system/files/sec22summer_pang-ren.pdf">PDF</a> |
|                             | Dos and Don'ts of Machine Learning in Computer Security      | <a href="https://www.usenix.org/conference/usenixsecurity22/presentation/arp">Link</a><br/><a href="https://www.usenix.org/system/files/sec22summer_arp.pdf">PDF</a> |
|                             | Exploring the Security Boundary of Data Reconstruction via Neuron Exclusivity Analysis | <a href="https://www.usenix.org/conference/usenixsecurity22/presentation/pan-exploring">Link</a><br/><a href="https://www.usenix.org/system/files/sec22fall_pan-exploring.pdf">PDF</a> |
|                             | On the Necessity of Auditable Algorithmic Definitions for Machine Unlearning | <a href="https://www.usenix.org/conference/usenixsecurity22/presentation/thudi">Link</a><br/><a href="https://www.usenix.org/system/files/sec22fall_thudi.pdf">PDF</a> |
| Differential Privacy        | Pool Inference Attacks on Local Differential Privacy: Quantifying the Privacy Guarantees of Apple's Count Mean Sketch in Practice | <a href="https://www.usenix.org/conference/usenixsecurity22/presentation/gadotti">Link</a><br/><a href="https://www.usenix.org/system/files/sec22-gadotti_1.pdf">PDF</a> |
|                             | Poisoning Attacks to Local Differential Privacy Protocols for Key-Value Data | <a href="https://www.usenix.org/conference/usenixsecurity22/presentation/wu-yongji">Link</a><br/><a href="https://www.usenix.org/system/files/sec22fall_wu-yongji.pdf">PDF</a> |
|                             | Communication-Efficient Triangle Counting under Local Differential Privacy | <a href="https://www.usenix.org/conference/usenixsecurity22/presentation/imola">Link</a><br/><a href="https://www.usenix.org/system/files/sec22fall_imola.pdf">PDF</a> |
|                             | Twilight: A Differentially Private Payment Channel Network   | <a href="https://www.usenix.org/conference/usenixsecurity22/presentation/dotan">Link<br/><a href="https://www.usenix.org/system/files/sec22-dotan.pdf">PDF</a></a> |
| Deanonymization             | Mitigating Membership Inference Attacks by Self-Distillation Through a Novel Ensemble Architecture | <a href="https://www.usenix.org/conference/usenixsecurity22/presentation/tang">Link</a><br/><a href="https://www.usenix.org/system/files/sec22fall_tang.pdf">PDF</a> |
|                             | Synthetic Data – Anonymisation Groundhog Day                 | <a href="https://www.usenix.org/conference/usenixsecurity22/presentation/stadler">Link</a><br/><a href="https://www.usenix.org/system/files/sec22summer_stadler.pdf">PDF</a> |
|                             | Attacks on Deidentification's Defenses                       | <a href="https://www.usenix.org/conference/usenixsecurity22/presentation/cohen">Link</a><br/><a href="https://www.usenix.org/system/files/sec22fall_cohen.pdf">PDF</a> |
|                             | Birds of a Feather Flock Together: How Set Bias Helps to Deanonymize You via Revealed Intersection Sizes | <a href="https://www.usenix.org/conference/usenixsecurity22/presentation/guo">Link</a><br/><a href="https://www.usenix.org/system/files/sec22-guo.pdf">PDF</a> |
|                             | Targeted Deanonymization via the Cache Side Channel: Attacks and Defenses | <a href="https://www.usenix.org/conference/usenixsecurity22/presentation/zaheri">Link</a><br/><a href="https://www.usenix.org/system/files/sec22-zaheri.pdf">PDF</a> |
| Performance Improvement     | Cheetah: Lean and Fast Secure Two-Party Deep Neural Network Inference | <a href="https://www.usenix.org/conference/usenixsecurity22/presentation/huang-zhicong">Link</a><br/><a href="https://www.usenix.org/system/files/sec22fall_huang-zhicong.pdf">PDF</a> |
|                             | Piranha: A GPU Platform for Secure Computation               | <a href="https://www.usenix.org/conference/usenixsecurity22/presentation/watson">Link</a><br/><a href="https://www.usenix.org/system/files/sec22-watson.pdf">PDF</a> |
| Other Interesting           | Can one hear the shape of a neural network?: Snooping the GPU via Magnetic Side Channel | <a href="https://www.usenix.org/conference/usenixsecurity22/presentation/maia">Link</a><br/><a href="https://www.usenix.org/system/files/sec22summer_maia.pdf">PDF</a> |
|                             | Rolling Colors: Adversarial Laser Exploits against Traffic Light Recognition | <a href="https://www.usenix.org/conference/usenixsecurity22/presentation/yan">Link</a><br/><a href="https://www.usenix.org/system/files/sec22summer_yan.pdf">PDF</a> |
|                             | Adversarial Detection Avoidance Attacks: Evaluating the robustness of perceptual hashing-based client-side scanning | <a href="https://www.usenix.org/conference/usenixsecurity22/presentation/jain">Link</a><br/><a href="https://www.usenix.org/system/files/sec22summer_jain.pdf">PDF</a> |

#### NDSS 2022

|                  | NDSS 2022 [<a href="https://www.ndss-symposium.org/ndss2022/">Homepage</a>] [<a href="https://www.ndss-symposium.org/ndss2022/accepted-papers/">Accepted</a>] |                                                              |
| ---------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| ML & AI          | Tetrad: Actively Secure 4PC for Secure Training and Inference | <a href="https://www.ndss-symposium.org/ndss-paper/auto-draft-202/">Link</a><br><a href="https://www.ndss-symposium.org/wp-content/uploads/2022-58-paper.pdf">PDF</a> |
|                  | MIRROR: Model Inversion for Deep LearningNetwork with High Fidelity | <a href="https://www.ndss-symposium.org/ndss-paper/auto-draft-203/">Link</a><br/><a href="https://www.ndss-symposium.org/wp-content/uploads/2022-335-paper.pdf">PDF</a> |
|                  | Local and Central Differential Privacy for Robustness and Privacy in Federated Learning | <a href="https://www.ndss-symposium.org/ndss-paper/auto-draft-204/">Link</a><br/><a href="https://www.ndss-symposium.org/wp-content/uploads/2022-54-paper.pdf">PDF</a> |
|                  | DeepSight: Mitigating Backdoor Attacks in Federated Learning Through Deep Model Inspection | <a href="https://www.ndss-symposium.org/ndss-paper/auto-draft-205/">Link</a><br/><a href="https://www.ndss-symposium.org/wp-content/uploads/2022-156-paper.pdf">PDF</a> |
|                  | What You See is Not What the Network Infers: Detecting Adversarial Examples Based on Semantic Contradiction | <a href="https://www.ndss-symposium.org/ndss-paper/auto-draft-226/">Link</a><br/><a href="https://www.ndss-symposium.org/wp-content/uploads/2022-1-paper.pdf">PDF</a> |
|                  | Euler: Detecting Network Lateral Movement via Scalable Temporal Graph Link Prediction | <a href="https://www.ndss-symposium.org/ndss-paper/auto-draft-227/">Link</a><br/><a href="https://www.ndss-symposium.org/wp-content/uploads/2022-107A-paper.pdf">PDF</a> |
|                  | Fooling the Eyes of Autonomous Vehicles: Robust Physical Adversarial Examples Against Traffic Sign Recognition Systems | <a href="https://www.ndss-symposium.org/ndss-paper/auto-draft-228/">Link</a><br/><a href="https://www.ndss-symposium.org/wp-content/uploads/2022-130-paper.pdf">PDF</a> |
|                  | FedCRI: Federated Mobile Cyber-Risk Intelligence             | <a href="https://www.ndss-symposium.org/ndss-paper/auto-draft-229/">Link</a><br/><a href="https://www.ndss-symposium.org/wp-content/uploads/2022-153-paper.pdf">PDF</a> |
| Attacks on ML/AI | ATTEQ-NN: Attention-based QoE-aware Evasive Backdoor Attacks | <a href="https://www.ndss-symposium.org/ndss-paper/auto-draft-238/">Link</a><br/><a href="https://www.ndss-symposium.org/wp-content/uploads/2022-12-paper.pdf">PDF</a> |
|                  | RamBoAttack: A Robust and Query Efficient Deep Neural Network Decision Exploit | <a href="https://www.ndss-symposium.org/ndss-paper/auto-draft-239/">Link</a><br/><a href="https://www.ndss-symposium.org/wp-content/uploads/2022-200-paper.pdf">PDF</a> |
|                  | Property Inference Attacks Against GANs                      | <a href="https://www.ndss-symposium.org/ndss-paper/auto-draft-240/">Link</a><br/><a href="https://www.ndss-symposium.org/wp-content/uploads/2022-19-paper.pdf">PDF</a> |
|                  | Get a Model! Model Hijacking Attack Against Machine Learning Models | <a href="https://www.ndss-symposium.org/ndss-paper/auto-draft-241/">Link</a><br/><a href="https://www.ndss-symposium.org/wp-content/uploads/2022-64-paper.pdf">PDF</a> |



### 2021

#### S&P 2021

|                             | S&P 2021 [<a href="https://www.ieee-security.org/TC/SP2021/index.html">Homepage</a>] [<a href="https://www.ieee-security.org/TC/SP2021/program-papers.html">Accepted</a>] |                                            |
| --------------------------- | ------------------------------------------------------------ | ------------------------------------------ |
| Adversarial ML & Unlearning | Detecting AI Trojans Using Meta Neural Analysis              | <a href="https://ieeexplore.ieee.org/document/9519467/">Link</a><br/><a href="https://arxiv.org/pdf/1910.03137.pdf">PDF</a> |
|                             | Adversarial Watermarking Transformer: Towards Tracing Text Provenance with Data Hiding | <a href="https://ieeexplore.ieee.org/document/9519400/">Link</a><br/><a href="https://arxiv.org/pdf/2009.03015.pdf">PDF</a> |
|                             | Machine Unlearning                                           | <a href="https://ieeexplore.ieee.org/document/9519428/">Link</a><br/><a href="https://arxiv.org/pdf/1912.03817">PDF</a> |
| Privacy                     | Defensive Technology Use by Political Activists During the Sudanese Revolution | <a href="https://ieeexplore.ieee.org/document/9519493/">Link</a><br/><a href="https://par.nsf.gov/servlets/purl/10231786">PDF</a> |
|                             | DP-Sniper: Black-Box Discovery of Differential Privacy Violations using Classifiers | <a href="https://ieeexplore.ieee.org/document/9519405/">Link</a><br/><a href="https://files.sri.inf.ethz.ch/website/papers/sp21-dpsniper.pdf">PDF</a> |
|                             | Is Private Learning Possible with Instance Encoding?         | <a href="https://ieeexplore.ieee.org/document/9519450/">Link</a><br/><a href="https://arxiv.org/pdf/2011.05315">PDF</a> |
| Differential Privacy        | Learning Differentially Private Mechanisms                   | <a href="https://ieeexplore.ieee.org/document/9519410/">Link</a><br/><a href="https://arxiv.org/pdf/2101.00961.pdf">PDF</a> |
|                             | Adversary Instantiation: Lower bounds for differentially private machine learning | <a href="https://ieeexplore.ieee.org/document/9519424/">Link</a><br/><a href="https://arxiv.org/pdf/2101.04535">PDF</a> |
|                             | Manipulation Attacks in Local Differential Privacy           | <a href="https://ieeexplore.ieee.org/document/9519418/">Link</a><br/><a href="https://arxiv.org/pdf/1909.09630">PDF</a> |
| ML Security & Privacy       | SIRNN: A Math Library for Secure RNN Inference               | <a href="https://ieeexplore.ieee.org/document/9519413/">Link</a><br/><a href="https://arxiv.org/pdf/2105.04236">PDF</a> |
|                             | CryptGPU: Fast Privacy-Preserving Machine Learning on the GPU | <a href="https://ieeexplore.ieee.org/document/9519386/">Link</a><br/><a href="https://arxiv.org/pdf/2104.10949">PDF</a> |
|                             | Proof-of-Learning: Definitions and Practice                  | <a href="https://ieeexplore.ieee.org/document/9519402/">Link</a><br/><a href="https://arxiv.org/pdf/2103.05633">PDF</a> |
| Speech Systems              | Who is Real Bob? Adversarial Attacks on Speaker Recognition Systems | <a href="https://ieeexplore.ieee.org/document/9519486/">Link</a><br/><a href="https://arxiv.org/pdf/1911.01840">PDF</a> |
|                             | Hear "No Evil", See "Kenansville": Efficient and Transferable Black-Box Attacks on Speech Recognition and Voice Identification Systems | <a href="https://ieeexplore.ieee.org/document/9519472/">Link</a><br/><a href="https://arxiv.org/pdf/1910.05262">PDF</a> |
|                             | SoK: The Faults in our ASRs: An Overview of Attacks against Automatic Speech Recognition and Speaker Identification Systems | <a href="https://ieeexplore.ieee.org/document/9519395/">Link</a><br/><a href="https://arxiv.org/pdf/2007.06622">PDF</a> |
| Autonomous Vehicles         | Poltergeist: Acoustic Adversarial Machine Learning against Cameras and Computer Vision | <a href="https://ieeexplore.ieee.org/document/9519394/">Link</a><br/><a href="https://spqrlab1.github.io/papers/ji-poltergeist-oakland21.pdf">PDF</a> |
|                             | Invisible for both Camera and LiDAR: Security of Multi-Sensor Fusion based Perception in Autonomous Driving Under Physical-World Attacks | <a href="https://ieeexplore.ieee.org/document/9519442/">Link</a><br/><a href="https://arxiv.org/pdf/2106.09249">PDF</a> |

#### CCS 2021

|                                    | CCS 2021 [<a href="https://www.sigsac.org/ccs/CCS2021/index.html">Homepage</a>] [<a href="https://www.sigsac.org/ccs/CCS2021/accepted-papers.html">Accepted</a>] |                                                              |
| ---------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| Attacks on Robustness              | Black-box Adversarial Attacks on Commercial Speech Platforms with Minimal Information | <a href="https://dl.acm.org/doi/10.1145/3460120.3485383">Link</a><br/><a href="https://arxiv.org/pdf/2110.09714">PDF</a> |
|                                    | A Hard Label Black-box Adversarial Attack Against Graph Neural Networks | <a href="https://dl.acm.org/doi/10.1145/3460120.3484796">Link</a><br/><a href="https://arxiv.org/pdf/2108.09513">PDF</a> |
|                                    | Robust Adversarial Attacks Against DNN-Based Wireless Communication Systems | <a href="https://dl.acm.org/doi/10.1145/3460120.3484777">Link</a><br/><a href="https://arxiv.org/pdf/2102.00918">PDF</a> |
|                                    | AI-Lancet: Locating Error-inducing Neurons to Optimize Neural Networks | <a href="https://dl.acm.org/doi/10.1145/3460120.3484818">Link</a><br/><a href="https://dl.acm.org/doi/pdf/10.1145/3460120.3484818">PDF</a> |
| Defenses for ML Robustness         | Learning Security Classifiers with Verified Global Robustness Properties | <a href="https://dl.acm.org/doi/10.1145/3460120.3484776">Link</a><br/><a href="https://arxiv.org/pdf/2105.11363">PDF</a> |
|                                    | On the Robustness of Domain Constraints                      | <a href="https://dl.acm.org/doi/10.1145/3460120.3484570">Link</a><br/><a href="https://arxiv.org/pdf/2105.08619">PDF</a> |
|                                    | Cert-RNN: Towards Certifying the Robustness of Recurrent Neural Networks | <a href="https://dl.acm.org/doi/10.1145/3460120.3484538">Link</a><br/><a href="https://nesa.zju.edu.cn/download/dty_pdf_cert_rnn.pdf">PDF</a> |
|                                    | TSS: Transformation-Specific Smoothing for Robustness Certification | <a href="https://dl.acm.org/doi/10.1145/3460120.3485258">Link</a><br/><a href="https://arxiv.org/pdf/2002.12398">PDF</a> |
| Inference Attacks                  | Honest-but-Curious Nets: Sensitive Attributes of Private Inputs Can Be Secretly Coded into the Classifiers' Outputs | <a href="https://dl.acm.org/doi/10.1145/3460120.3484533">Link</a><br/><a href="https://arxiv.org/pdf/2105.12049">PDF</a> |
|                                    | Quantifying and Mitigating Privacy Risks of Contrastive Learning | <a href="https://dl.acm.org/doi/10.1145/3460120.3484571">Link</a><br/><a href="https://arxiv.org/pdf/2102.04140">PDF</a> |
|                                    | Membership Inference Attacks Against Recommender Systems     | <a href="https://dl.acm.org/doi/10.1145/3460120.3484770">Link</a><br/><a href="https://arxiv.org/pdf/2109.08045">PDF</a> |
|                                    | Membership Leakage in Label-Only Exposures                   | <a href="https://dl.acm.org/doi/10.1145/3460120.3484575">Link</a><br/><a href="https://arxiv.org/pdf/2007.15528">PDF</a> |
|                                    | When Machine Unlearning Jeopardizes Privacy                  | <a href="https://dl.acm.org/doi/10.1145/3460120.3484756">Link</a><br/><a href="https://arxiv.org/pdf/2005.02205">PDF</a> |
| Privacy Attacks & Defenses for ML  | EncoderMI: Membership Inference against Pre-trained Encoders in Contrastive Learning | <a href="https://dl.acm.org/doi/10.1145/3460120.3484749">Link</a><br/><a href="https://arxiv.org/pdf/2108.11023">PDF</a> |
|                                    | TableGAN-MCA: Evaluating Membership Collisions of GAN-Synthesized Tabular Data Releasing | <a href="https://dl.acm.org/doi/10.1145/3460120.3485251">Link</a><br/><a href="https://arxiv.org/pdf/2107.13190">PDF</a> |
|                                    | Unleashing the Tiger: Inference Attacks on Split Learning    | <a href="https://dl.acm.org/doi/10.1145/3460120.3485259">Link</a><br/><a href="https://arxiv.org/pdf/2012.02670">PDF</a> |
|                                    | Locally Private Graph Neural Networks                        | <a href="https://dl.acm.org/doi/10.1145/3460120.3484565">Link</a><br/><a href="https://arxiv.org/pdf/2006.05535">PDF</a> |
|                                    | DataLens: Scalable Privacy Preserving Training via Gradient Compression and Aggregation | <a href="https://dl.acm.org/doi/10.1145/3460120.3484579">Link</a><br/><a href="https://arxiv.org/pdf/2103.11109">PDF</a> |
| Privacy for Distributed Data & FL  | LEAP: Leakage-Abuse Attack on Efficiently Deployable, Efficiently Searchable Encryption with Partially Known Dataset | <a href="https://dl.acm.org/doi/10.1145/3460120.3484540">Link</a><br/><a href="https://ink.library.smu.edu.sg/cgi/viewcontent.cgi?article=7743&context=sis_research">PDF</a> |
|                                    | On the Renyi Differential Privacy of the Shuffle Model       | <a href="https://dl.acm.org/doi/10.1145/3460120.3484794">Link</a><br/><a href="https://arxiv.org/pdf/2105.05180">PDF</a> |
|                                    | Private Hierarchical Clustering in Federated Networks        | <a href="https://dl.acm.org/doi/10.1145/3460120.3484822">Link</a><br/><a href="https://arxiv.org/pdf/2105.09057">PDF</a> |
|                                    | Secure Multi-party Computation of Differentially Private Heavy Hitters | <a href="https://dl.acm.org/doi/10.1145/3460120.3484557">Link</a><br/><a href="https://dl.acm.org/doi/pdf/10.1145/3460120.3484557">PDF</a> |
| Differential Privacy               | Differential Privacy for Directional Data                    | <a href="https://dl.acm.org/doi/10.1145/3460120.3484734">Link</a><br/><a href="https://dl.acm.org/doi/pdf/10.1145/3460120.3484734">PDF</a> |
|                                    | Differentially Private Sparse Vectors with Low Error, Optimal Space, and Fast Access | <a href="https://dl.acm.org/doi/10.1145/3460120.3484735">Link</a><br/><a href="https://arxiv.org/pdf/2106.10068">PDF</a> |
|                                    | Continuous Release of Data Streams under both Centralized and Local Differential Privacy | <a href="https://dl.acm.org/doi/10.1145/3460120.3484750">Link</a><br/><a href="https://dl.acm.org/doi/pdf/10.1145/3460120.3484750">PDF</a> |
|                                    | Side-Channel Attacks on Query-Based Data Anonymization       | <a href="https://dl.acm.org/doi/10.1145/3460120.3484751">Link</a><br/><a href="https://people.mpi-sws.org/~francis/side-channel.pdf">PDF</a> |
|                                    | AHEAD: Adaptive Hierarchical Decomposition for Range Query under Local Differential Privacy | <a href="https://dl.acm.org/doi/10.1145/3460120.3485668">Link</a><br/><a href="https://arxiv.org/pdf/2110.07505">PDF</a> |
| Other Privacy                      | Reconstructing with Less: Leakage Abuse Attacks in Two Dimensions | <a href="https://dl.acm.org/doi/10.1145/3460120.3484552">Link</a><br/><a href="https://eprint.iacr.org/2020/1531.pdf">PDF</a> |
|                                    | Epsolute: Efficiently Querying Databases While Providing Differential Privacy | <a href="https://dl.acm.org/doi/10.1145/3460120.3484786">Link</a><br/><a href="https://dl.acm.org/doi/pdf/10.1145/3460120.3484786">PDF</a> |
|                                    | The Invisible Shadow: How Security Cameras Leak Private Activities | <a href="https://dl.acm.org/doi/10.1145/3460120.3484741">Link</a><br/><a href="http://xyzhang.ucsd.edu/papers/Jian.Gong_CCS21_InvisibleShadow.pdf">PDF</a> |
| Data Poisoning & Backdoor Attacks  | Subpopulation Data Poisoning Attacks                         | <a href="https://dl.acm.org/doi/10.1145/3460120.3485368">Link</a><br/><a href="https://arxiv.org/pdf/2006.14026">PDF</a> |
|                                    | Hidden Backdoors in Human-Centric Language Models            | <a href="https://dl.acm.org/doi/10.1145/3460120.3484576">Link</a><br/><a href="https://arxiv.org/pdf/2105.00164">PDF</a> |
|                                    | Backdoor Pre-trained Models Can Transfer to All              | <a href="https://dl.acm.org/doi/10.1145/3460120.3485370">Link</a><br/><a href="https://arxiv.org/pdf/2111.00197">PDF</a> |
|                                    | Feature-Indistinguishable Attack to Circumvent Trapdoor-Enabled Defense | <a href="https://dl.acm.org/doi/10.1145/3460120.3485378">Link</a><br/><a href="https://www.microsoft.com/en-us/research/uploads/prod/2022/07/Feature_Indistinguishable_Attack_to_Circumvent_Trapdoor_enabled_Defense.pdf">PDF</a> |
|                                    | DetectorGuard: Provably Securing Object Detectors against Localized Patch Hiding Attacks | <a href="https://dl.acm.org/doi/10.1145/3460120.3484757">Link</a><br/><a href="https://arxiv.org/pdf/2102.02956">PDF</a> |
| Applications & Privacy of ML       | DeepAID: Interpreting and Improving Deep Learning-based Anomaly Detection in Security Applications | <a href="https://dl.acm.org/doi/10.1145/3460120.3484589">Link</a><br/><a href="https://arxiv.org/pdf/2109.11495">PDF</a> |
|                                    | Structural Attack against Graph Based Android Malware Detection | <a href="https://dl.acm.org/doi/10.1145/3460120.3485387">Link</a><br/><a href="https://www4.comp.polyu.edu.hk/~csxluo/HRAT.pdf">PDF</a> |
|                                    | PalmTree: Learning an Assembly Language Model for Instruction Embedding | <a href="https://dl.acm.org/doi/10.1145/3460120.3484587">Link</a><br/><a href="https://arxiv.org/pdf/2103.03809">PDF</a> |
|                                    | A One-Pass Distributed and Private Sketch for Kernel Sums with Applications to Machine Learning at Scale | <a href="https://dl.acm.org/doi/10.1145/3460120.3485255">Link</a><br/><a href="https://dl.acm.org/doi/pdf/10.1145/3460120.3485255">PDF</a> |
|                                    | COINN: Crypto/ML Codesign for Oblivious Inference via Neural Networks | <a href="https://dl.acm.org/doi/10.1145/3460120.3484797">Link</a><br/><a href="https://dl.acm.org/doi/pdf/10.1145/3460120.3484797">PDF</a> |
| Audio Systems & Autonomous Driving | FakeWake: Understanding and Mitigating Fake Wake-up Words of Voice Assistants | <a href="https://dl.acm.org/doi/10.1145/3460120.3485365">Link</a><br/><a href="https://arxiv.org/pdf/2109.09958">PDF</a> |
|                                    | Robust Detection of Machine-induced Audio Attacks in Intelligent Audio Systems with Microphone Array | <a href="https://dl.acm.org/doi/10.1145/3460120.3484755">Link</a><br/><a href="https://dl.acm.org/doi/pdf/10.1145/3460120.3484755">PDF</a> |
|                                    | I Can See the Light: Attacks on Autonomous Vehicles Using Invisible Lights | <a href="https://dl.acm.org/doi/10.1145/3460120.3484766">Link</a><br/><a href="https://dl.acm.org/doi/pdf/10.1145/3460120.3484766">PDF</a> |
|                                    | Can We Use Arbitrary Objects to Attack LiDAR Perception in Autonomous Driving? | <a href="https://dl.acm.org/doi/10.1145/3460120.3485377">Link</a><br/><a href="https://www.acsu.buffalo.edu/~yzhu39/Yi_Zhu_homepage_files/papers/CCS21_434.pdf">PDF</a> |

#### SEC 2021


|                                         | USENIX Security 2021 [<a href="https://www.usenix.org/conference/usenixsecurity21">Homepage</a>] [<a href="https://www.usenix.org/conference/usenixsecurity21/technical-sessions">Accepted</a>] |                                            |
| --------------------------------------- | ------------------------------------------------------------ | ------------------------------------------ |
| Backdoor & Poisoning                    | Explanation-Guided Backdoor Poisoning Attacks Against Malware Classifiers | <a href="https://www.usenix.org/conference/usenixsecurity21/presentation/severi">Link</a><br/><a href="https://www.usenix.org/system/files/sec21fall-severi.pdf">PDF</a> |
|                                         | Blind Backdoors in Deep Learning Models                      | <a href="https://www.usenix.org/conference/usenixsecurity21/presentation/bagdasaryan">Link</a><br/><a href="https://www.usenix.org/system/files/sec21-bagdasaryan.pdf">PDF</a> |
|                                         | Graph Backdoor                                               | <a href="https://www.usenix.org/conference/usenixsecurity21/presentation/xi">Link</a><br/><a href="https://www.usenix.org/system/files/sec21fall-xi.pdf">PDF</a> |
|                                         | Demon in the Variant: Statistical Analysis of DNNs for Robust Backdoor Contamination Detection | <a href="https://www.usenix.org/conference/usenixsecurity21/presentation/tang-di">Link</a><br/><a href="https://www.usenix.org/system/files/sec21summer_tang-di.pdf">PDF</a> |
|                                         | You Autocomplete Me: Poisoning Vulnerabilities in Neural Code Completion | <a href="https://www.usenix.org/conference/usenixsecurity21/presentation/schuster">Link</a><br/><a href="https://www.usenix.org/system/files/sec21summer_schuster.pdf">PDF</a> |
|                                         | Poisoning the Unlabeled Dataset of Semi-Supervised Learning  | <a href="https://www.usenix.org/conference/usenixsecurity21/presentation/carlini-poisoning">Link</a><br/><a href="https://www.usenix.org/system/files/sec21-carlini-poisoning.pdf">PDF</a> |
|                                         | Double-Cross Attacks: Subverting Active Learning Systems     | <a href="https://www.usenix.org/conference/usenixsecurity21/presentation/vicarte">Link</a><br/><a href="https://www.usenix.org/system/files/sec21-vicarte.pdf">PDF</a> |
| Adversarial Examples & Model Extraction | SLAP: Improving Physical Adversarial Examples with Short-Lived Adversarial Perturbations | <a href="https://www.usenix.org/conference/usenixsecurity21/presentation/lovisotto">Link</a><br/><a href="https://www.usenix.org/system/files/sec21fall-lovisotto.pdf">PDF</a> |
|                                         | Adversarial Policy Training against Deep Reinforcement Learning | <a href="https://www.usenix.org/conference/usenixsecurity21/presentation/wu-xian">Link</a><br/><a href="https://www.usenix.org/system/files/sec21summer_wu-xian.pdf">PDF</a> |
|                                         | DRMI: A Dataset Reduction Technology based on Mutual Information for Black-box Attacks | <a href="https://www.usenix.org/conference/usenixsecurity21/presentation/he-yingzhe">Link</a><br/><a href="https://www.usenix.org/system/files/sec21fall-he-yingzhe.pdf">PDF</a> |
|                                         | Deep-Dup: An Adversarial Weight Duplication Attack Framework to Crush Deep Neural Network in Multi-Tenant FPGA | <a href="https://www.usenix.org/conference/usenixsecurity21/presentation/rakin">Link</a><br/><a href="https://www.usenix.org/system/files/sec21-rakin.pdf">PDF</a> |
|                                         | Entangled Watermarks as a Defense against Model Extraction   | <a href="https://www.usenix.org/conference/usenixsecurity21/presentation/jia">Link</a><br/><a href="https://www.usenix.org/system/files/sec21fall-jia.pdf">PDF</a> |
|                                         | Mind Your Weight(s): A Large-scale Study on Insufficient Machine Learning Model Protection in Mobile Apps | <a href="https://www.usenix.org/conference/usenixsecurity21/presentation/sun-zhichuang">Link</a><br/><a href="https://www.usenix.org/system/files/sec21fall-sun-zhichuang.pdf">PDF</a> |
|                                         | Hermes Attack: Steal DNN Models with Lossless Inference Accuracy | <a href="https://www.usenix.org/conference/usenixsecurity21/presentation/zhu">Link</a><br/><a href="https://www.usenix.org/system/files/sec21summer_zhu.pdf">PDF</a> |
| Adversarial ML Defenses                 | PatchGuard: A Provably Robust Defense against Adversarial Patches via Small Receptive Fields and Masking | <a href="https://www.usenix.org/conference/usenixsecurity21/presentation/xiang">Link</a><br/><a href="https://www.usenix.org/system/files/sec21fall-xiang.pdf">PDF</a> |
|                                         | T-Miner: A Generative Approach to Defend Against Trojan Attacks on DNN-based Text Classification | <a href="https://www.usenix.org/conference/usenixsecurity21/presentation/azizi">Link</a><br/><a href="https://www.usenix.org/system/files/sec21fall-azizi.pdf">PDF</a> |
|                                         | WaveGuard: Understanding and Mitigating Audio Adversarial Examples | <a href="https://www.usenix.org/conference/usenixsecurity21/presentation/hussain">Link</a><br/><a href="https://www.usenix.org/system/files/sec21fall-hussain.pdf">PDF</a> |
|                                         | Cost-Aware Robust Tree Ensembles for Security Applications   | <a href="https://www.usenix.org/conference/usenixsecurity21/presentation/chen-yizheng">Link</a><br/><a href="https://www.usenix.org/system/files/sec21fall-chen-yizheng.pdf">PDF</a> |
|                                         | Dompteur: Taming Audio Adversarial Examples                  | <a href="https://usenix.org/conference/usenixsecurity21/presentation/eisenhofer">Link</a><br/><a href="https://www.usenix.org/system/files/sec21-eisenhofer.pdf">PDF</a> |
|                                         | CADE: Detecting and Explaining Concept Drift Samples for Security Applications | <a href="https://www.usenix.org/conference/usenixsecurity21/presentation/yang-limin">Link</a><br/><a href="https://www.usenix.org/system/files/sec21summer_yang.pdf">PDF</a> |
|                                         | SIGL: Securing Software Installations Through Deep Graph Learning | <a href="https://www.usenix.org/conference/usenixsecurity21/presentation/han-xueyuan">Link</a><br/><a href="https://www.usenix.org/system/files/sec21summer_han-xueyuan.pdf">PDF</a> |
| ML Privacy Issues                       | Systematic Evaluation of Privacy Risks of Machine Learning Models | <a href="https://www.usenix.org/conference/usenixsecurity21/presentation/song">Link</a><br/><a href="https://www.usenix.org/system/files/sec21fall-song.pdf">PDF</a> |
|                                         | Extracting Training Data from Large Language Models          | <a href="https://www.usenix.org/conference/usenixsecurity21/presentation/carlini-extracting">Link</a><br/><a href="https://www.usenix.org/system/files/sec21-carlini-extracting.pdf">PDF</a> |
|                                         | SWIFT: Super-fast and Robust Privacy-Preserving Machine Learning | <a href="https://www.usenix.org/conference/usenixsecurity21/presentation/koti">Link</a><br/><a href="https://www.usenix.org/system/files/sec21fall-koti.pdf">PDF</a> |
|                                         | Stealing Links from Graph Neural Networks                    | <a href="https://www.usenix.org/conference/usenixsecurity21/presentation/he-xinlei">Link</a><br/><a href="https://www.usenix.org/system/files/sec21summer_he.pdf">PDF</a> |
|                                         | Leakage of Dataset Properties in Multi-Party Machine Learning | <a href="https://www.usenix.org/conference/usenixsecurity21/presentation/zhang-wanrong">Link</a><br/><a href="https://www.usenix.org/system/files/sec21-zhang-wanrong.pdf">PDF</a> |
|                                         | Defeating DNN-Based Traffic Analysis Systems in Real-Time With Blind Adversarial Perturbations | <a href="https://www.usenix.org/conference/usenixsecurity21/presentation/nasr">Link</a><br/><a href="https://www.usenix.org/system/files/sec21fall-nasr.pdf">PDF</a> |
|                                         | Cerebro: A Platform for Multi-Party Cryptographic Collaborative Learning | <a href="https://www.usenix.org/conference/usenixsecurity21/presentation/zheng">Link</a><br/><a href="https://www.usenix.org/system/files/sec21fall-zheng.pdf">PDF</a> |
| Differential Privacy                    | PrivSyn: Differentially Private Data Synthesis               | <a href="https://www.usenix.org/conference/usenixsecurity21/presentation/zhang-zhikun">Link</a><br/><a href="https://www.usenix.org/system/files/sec21fall-zhang-zhikun.pdf">PDF</a> |
|                                         | Data Poisoning Attacks to Local Differential Privacy Protocols | <a href="https://www.usenix.org/conference/usenixsecurity21/presentation/cao-xiaoyu">Link</a><br/><a href="https://www.usenix.org/system/files/sec21fall-cao.pdf">PDF</a> |
|                                         | How to Make Private Distributed Cardinality Estimation Practical, and Get Differential Privacy for Free | <a href="https://www.usenix.org/conference/usenixsecurity21/presentation/hu-changhui">Link</a><br/><a href="https://www.usenix.org/system/files/sec21fall-hu-changhui.pdf">PDF</a> |
|                                         | Locally Differentially Private Analysis of Graph Statistics  | <a href="https://www.usenix.org/conference/usenixsecurity21/presentation/imola">Link</a><br/><a href="https://www.usenix.org/system/files/sec21fall-imola.pdf">PDF</a> |
| Secure Multiparty Computation           | GForce: GPU-Friendly Oblivious and Rapid Neural Network Inference | <a href="https://www.usenix.org/conference/usenixsecurity21/presentation/ng">Link</a><br/><a href="https://www.usenix.org/system/files/sec21fall-ng.pdf">PDF</a> |
|                                         | Fantastic Four: Honest-Majority Four-Party Secure Computation With Malicious Security | <a href="https://www.usenix.org/conference/usenixsecurity21/presentation/dalskov">Link</a><br/><a href="https://www.usenix.org/system/files/sec21fall-dalskov.pdf">PDF</a> |
|                                         | Muse: Secure Inference Resilient to Malicious Clients        | <a href="https://www.usenix.org/conference/usenixsecurity21/presentation/lehmkuhl">Link</a><br/><a href="https://www.usenix.org/system/files/sec21fall-lehmkuhl.pdf">PDF</a> |
| Other Attacks                           | Too Good to Be Safe: Tricking Lane Detection in Autonomous Driving with Crafted Perturbations | <a href="https://www.usenix.org/conference/usenixsecurity21/presentation/jing">Link</a><br/><a href="https://www.usenix.org/system/files/sec21fall-jing.pdf">PDF</a> |
|                                         | Research on the Security of Visual Reasoning CAPTCHA         | <a href="https://www.usenix.org/conference/usenixsecurity21/presentation/gao">Link</a><br/><a href="https://www.usenix.org/system/files/sec21fall-gao.pdf">PDF</a> |
|                                         | Dirty Road Can Attack: Security of Deep Learning based Automated Lane Centering under Physical-World Attack | <a href="https://www.usenix.org/conference/usenixsecurity21/presentation/sato">Link</a><br/><a href="https://www.usenix.org/system/files/sec21-sato.pdf">PDF</a> |
|                                         | mID: Tracing Screen Photos via Moiré Patterns                | <a href="https://www.usenix.org/conference/usenixsecurity21/presentation/cheng-yushi">Link</a><br/><a href="https://www.usenix.org/system/files/sec21fall-cheng-yushi.pdf">PDF</a> |

#### NDSS 2021

|                        | NDSS 2021 [<a href="https://www.ndss-symposium.org/ndss2021/">Homepage</a>] [<a href="https://www.ndss-symposium.org/ndss2021/accepted-papers/">Accepted</a>] |                                            |
| ---------------------- | ------------------------------------------------------------ | ------------------------------------------ |
| Machine Learning       | Let’s Stride Blindfolded in a Forest: Sublinear Multi-Client Decision Trees Evaluation | <a href="https://www.ndss-symposium.org/ndss-paper/lets-stride-blindfolded-in-a-forest-sublinear-multi-client-decision-trees-evaluation/">Link</a><br/><a href="https://www.ndss-symposium.org/wp-content/uploads/ndss2021_5C-1_23166_paper.pdf">PDF</a> |
|                        | Practical Blind Membership Inference Attack via Differential Comparisons | <a href="https://www.ndss-symposium.org/ndss-paper/practical-blind-membership-inference-attack-via-differential-comparisons/">Link</a><br/><a href="https://www.ndss-symposium.org/wp-content/uploads/ndss2021_5C-2_24293_paper.pdf">PDF</a> |
|                        | GALA: Greedy ComputAtion for Linear Algebra in Privacy-Preserved Neural Networks | <a href="https://www.ndss-symposium.org/ndss-paper/gala-greedy-computation-for-linear-algebra-in-privacy-preserved-neural-networks/">Link</a><br/><a href="https://www.ndss-symposium.org/wp-content/uploads/ndss2021_5C-3_24351_paper.pdf">PDF</a> |
|                        | FARE: Enabling Fine-grained Attack Categorization under Low-quality Labeled Data | <a href="https://www.ndss-symposium.org/ndss-paper/fare-enabling-fine-grained-attack-categorization-under-low-quality-labeled-data/">Link</a><br/><a href="https://www.ndss-symposium.org/wp-content/uploads/ndss2021_5C-4_24403_paper.pdf">PDF</a> |
| FL & Poisoning Attacks | POSEIDON: Privacy-Preserving Federated Neural Network Learning | <a href="https://www.ndss-symposium.org/ndss-paper/poseidon-privacy-preserving-federated-neural-network-learning/">Link</a><br/><a href="https://www.ndss-symposium.org/wp-content/uploads/ndss2021_6C-1_24119_paper.pdf">PDF</a> |
|                        | FLTrust: Byzantine-robust Federated Learning via Trust Bootstrapping | <a href="https://www.ndss-symposium.org/ndss-paper/fltrust-byzantine-robust-federated-learning-via-trust-bootstrapping/">Link</a><br/><a href="https://www.ndss-symposium.org/wp-content/uploads/ndss2021_6C-2_24434_paper.pdf">PDF</a> |
|                        | Manipulating the Byzantine: Optimizing Model Poisoning Attacks and Defenses for Federated Learning | <a href="https://www.ndss-symposium.org/ndss-paper/manipulating-the-byzantine-optimizing-model-poisoning-attacks-and-defenses-for-federated-learning/">Link</a><br/><a href="https://www.ndss-symposium.org/wp-content/uploads/ndss2021_6C-3_24498_paper.pdf">PDF</a> |
|                        | Data Poisoning Attacks to Deep Learning Based Recommender Systems | <a href="https://www.ndss-symposium.org/ndss-paper/data-poisoning-attacks-to-deep-learning-based-recommender-systems/">Link</a><br/><a href="https://www.ndss-symposium.org/wp-content/uploads/ndss2021_6C-4_24525_paper.pdf">PDF</a> |



### 2020

#### S&P 2020

|                      | S&P 2020 [<a href="https://www.ieee-security.org/TC/SP2020/index.html">Homepage</a>] [<a href="https://www.ieee-security.org/TC/SP2020/program-papers.html">Accepted</a>] |                                            |
| -------------------- | ------------------------------------------------------------ | ------------------------------------------ |
| ML & Privacy         | The Value of Collaboration in Convex Machine Learning with Differential Privacy | <a href="https://ieeexplore.ieee.org/document/9152691/">Link</a><br/><a href="https://arxiv.org/pdf/1906.09679">PDF</a> |
|                      | Automatically Detecting Bystanders in Photos to Reduce Privacy Risks | <a href="https://ieeexplore.ieee.org/document/9152778/">Link</a><br/><a href="https://rakib062.github.io/files/bystander-oakland-2020.pdf">PDF</a> |
|                      | CrypTFlow: Secure TensorFlow Inference                       | <a href="https://ieeexplore.ieee.org/document/9152660/">Link</a><br/><a href="https://arxiv.org/pdf/1909.07814">PDF</a> |
| Adversarial ML       | HopSkipJumpAttack: A Query-Efficient Decision-Based Attack   | <a href="https://ieeexplore.ieee.org/document/9152788/">Link</a><br/><a href="https://arxiv.org/pdf/1904.02144">PDF</a> |
|                      | Humpty Dumpty: Controlling Word Meanings via Corpus Poisoning | <a href="https://ieeexplore.ieee.org/document/9152608/">Link</a><br/><a href="https://arxiv.org/pdf/2001.04935">PDF</a> |
|                      | Privacy Risks of General-Purpose Language Models             | <a href="https://ieeexplore.ieee.org/document/9152761/">Link</a><br/><a href="https://nesa.zju.edu.cn/download/Privacy%20Risks%20of%20General-Purpose%20Language%20Models.pdf">PDF</a> |
|                      | Intriguing Properties of Adversarial ML Attacks in the Problem Space | <a href="https://ieeexplore.ieee.org/document/9152781/">Link</a><br/><a href="https://arxiv.org/pdf/1911.02142">PDF</a> |
| Differential Privacy | SoK: Differential Privacy as a Causal Property               | <a href="https://ieeexplore.ieee.org/document/9152780/">Link</a><br/><a href="https://arxiv.org/pdf/1710.05899">PDF</a> |
|                      | Private Resource Allocators and Their Applications           | <a href="https://ieeexplore.ieee.org/document/9152764/">Link</a><br/><a href="https://eprint.iacr.org/2020/287.pdf">PDF</a> |
|                      | Towards Effective Differential Privacy Communication for Users' Data Sharing Decision and Comprehension | <a href="https://ieeexplore.ieee.org/document/9152658/">Link</a><br/><a href="https://arxiv.org/pdf/2003.13922">PDF</a> |
|                      | A Programming Framework for Differential Privacy with Accuracy Concentration Bounds | <a href="https://ieeexplore.ieee.org/document/9152641/">Link</a><br/><a href="https://arxiv.org/pdf/1909.07918">PDF</a> |
| Attacks & Forensics  | Throwing Darts in the Dark? Detecting Bots with Limited Data using Neural Data Augmentation | <a href="https://ieeexplore.ieee.org/document/9152805/">Link</a><br/><a href="https://gangw.web.illinois.edu/sp20-odds.pdf">PDF</a> |

#### CCS 2020

|                          | CCS 2020 [<a href="https://www.sigsac.org/ccs/CCS2020/index.html">Homepage</a>] [<a href="https://www.sigsac.org/ccs/CCS2020/accepted-papers.html">Accepted</a>] |                                            |
| ------------------------ | ------------------------------------------------------------ | ------------------------------------------ |
| Attacking & Defending ML | Gotta Catch'Em All: Using Honeypots to Catch Adversarial Attacks on Neural Networks | <a href="https://dl.acm.org/doi/10.1145/3372297.3417231">Link</a><br/><a href="https://dl.acm.org/doi/pdf/10.1145/3372297.3417231">PDF</a> |
|                          | A Tale of Evil Twins: Adversarial Inputs versus Poisoned Models | <a href="https://dl.acm.org/doi/10.1145/3372297.3417253">Link</a><br/><a href="https://arxiv.org/pdf/1911.01559">PDF</a> |
|                          | DeepDyve: Dynamic Verification for Deep Neural Networks      | <a href="https://dl.acm.org/doi/10.1145/3372297.3423338">Link</a><br/><a href="https://arxiv.org/pdf/2009.09663">PDF</a> |
|                          | Composite Backdoor Attack for Deep Neural Network by Mixing Existing Benign Features | <a href="https://dl.acm.org/doi/10.1145/3372297.3423362">Link</a><br/><a href="https://dl.acm.org/doi/pdf/10.1145/3372297.3423362">PDF</a> |
| ML & Information Leakage | CrypTFlow2: Practical 2-Party Secure Inference               | <a href="https://dl.acm.org/doi/10.1145/3372297.3417274">Link</a><br/><a href="https://arxiv.org/pdf/2010.06457">PDF</a> |
|                          | GAN-Leaks: A Taxonomy of Membership Inference Attacks against Generative Models | <a href="https://dl.acm.org/doi/10.1145/3372297.3417238">Link</a><br/><a href="https://arxiv.org/pdf/1909.03935">PDF</a> |
|                          | Analyzing Information Leakage of Updates to Natural Language Models | <a href="https://dl.acm.org/doi/10.1145/3372297.3417880">Link</a><br/><a href="https://arxiv.org/pdf/1912.07942">PDF</a> |
|                          | Information Leakage in Embedding Models                      | <a href="https://dl.acm.org/doi/10.1145/3372297.3417270">Link</a><br/><a href="https://arxiv.org/pdf/2004.00053">PDF</a> |
| Privacy                  | Private Summation in the Multi-Message Shuffle Model         | <a href="https://dl.acm.org/doi/10.1145/3372297.3417242">Link</a><br/><a href="https://arxiv.org/pdf/2002.00817">PDF</a> |
|                          | R^2DP: A Universal and Automated Approach to Optimizing the Randomization Mechanisms of Differential Privacy for Utility Metrics with No Known Optimal Distributions | <a href="https://dl.acm.org/doi/10.1145/3372297.3417259">Link</a><br/><a href="https://arxiv.org/pdf/2009.09451">PDF</a> |
|                          | Estimating g-Leakage via Machine Learning                    | <a href="https://dl.acm.org/doi/10.1145/3372297.3423363">Link</a><br/><a href="https://arxiv.org/pdf/2005.04399">PDF</a> |
|                          | Implementing the Exponential Mechanism with Base-2 Differential Privacy | <a href="https://dl.acm.org/doi/10.1145/3372297.3417269">Link</a><br/><a href="https://arxiv.org/pdf/1912.04222">PDF</a> |

#### SEC 2020


|                     | USENIX Security 2020 [<a href="https://www.usenix.org/conference/usenixsecurity20">Homepage</a>] [<a href="https://www.usenix.org/conference/usenixsecurity20/technical-sessions">Accepted</a>] |                                            |
| ------------------- | ------------------------------------------------------------ | ------------------------------------------ |
| Machine Learning    | Updates-Leak: Data Set Inference and Reconstruction Attacks in Online Learning | <a href="https://www.usenix.org/conference/usenixsecurity20/presentation/salem">Link</a><br/><a href="https://www.usenix.org/system/files/sec20summer_salem_prepub.pdf">PDF</a> |
|                     | Exploring Connections Between Active Learning and Model Extraction | <a href="https://www.usenix.org/conference/usenixsecurity20/presentation/chandrasekaran">Link</a><br/><a href="https://www.usenix.org/system/files/sec20summer_chandrasekaran_prepub.pdf">PDF</a> |
|                     | Hybrid Batch Attacks: Finding Black-box Adversarial Examples with Limited Queries | <a href="https://www.usenix.org/conference/usenixsecurity20/presentation/suya">Link</a><br/><a href="https://www.usenix.org/system/files/sec20summer_suya_prepub.pdf">PDF</a> |
|                     | High Accuracy and High Fidelity Extraction of Neural Networks | <a href="https://www.usenix.org/conference/usenixsecurity20/presentation/jagielski">Link</a><br/><a href="https://www.usenix.org/system/files/sec20fall_jagielski_prepub.pdf">PDF</a> |
|                     | Adversarial Preprocessing: Understanding and Preventing Image-Scaling Attacks in Machine Learning | <a href="https://www.usenix.org/conference/usenixsecurity20/presentation/quiring">Link</a><br/><a href="https://www.usenix.org/system/files/sec20fall_quiring_prepub.pdf">PDF</a> |
|                     | TextShield: Robust Text Classification Based on Multimodal Embedding and Neural Machine Translation | <a href="https://www.usenix.org/conference/usenixsecurity20/presentation/li-jinfeng">Link</a><br/><a href="https://www.usenix.org/system/files/sec20fall_li-jinfeng_prepub.pdf">PDF</a> |
|                     | Fawkes: Protecting Privacy against Unauthorized Deep Learning Models | <a href="https://www.usenix.org/conference/usenixsecurity20/presentation/shan">Link</a><br/><a href="https://www.usenix.org/system/files/sec20-shan.pdf">PDF</a> |
|                     | Stolen Memories: Leveraging Model Memorization for Calibrated White-Box Membership Inference | <a href="https://www.usenix.org/conference/usenixsecurity20/presentation/leino">Link</a><br/><a href="https://www.usenix.org/system/files/sec20-leino.pdf">PDF</a> |
|                     | Local Model Poisoning Attacks to Byzantine-Robust Federated Learning | <a href="https://www.usenix.org/conference/usenixsecurity20/presentation/fang">Link</a><br/><a href="https://www.usenix.org/system/files/sec20summer_fang_prepub.pdf">PDF</a> |
|                     | Justinian's GAAvernor: Robust Distributed Learning with Gradient Aggregation Agent | <a href="https://www.usenix.org/conference/usenixsecurity20/presentation/pan">Link</a><br/><a href="https://www.usenix.org/system/files/sec20-pan.pdf">PDF</a> |
|                     | Interpretable Deep Learning under Fire                       | <a href="https://www.usenix.org/conference/usenixsecurity20/presentation/zhang-xinyang">Link</a><br/><a href="https://www.usenix.org/system/files/sec20spring_zhang_prepub.pdf">PDF</a> |
| Privacy Enhancement | PCKV: Locally Differentially Private Correlated Key-Value Data Collection with Optimized Utility | <a href="https://www.usenix.org/conference/usenixsecurity20/presentation/gu">Link</a><br/><a href="https://www.usenix.org/system/files/sec20summer_gu_prepub.pdf">PDF</a> |
|                     | Actions Speak Louder than Words: Entity-Sensitive Privacy Policy and Data Flow Analysis with PoliCheck | <a href="https://www.usenix.org/conference/usenixsecurity20/presentation/andow">Link</a><br/><a href="https://www.usenix.org/system/files/sec20summer_andow_prepub.pdf">PDF</a> |
|                     | Walking Onions: Scaling Anonymity Networks while Protecting Users | <a href="https://www.usenix.org/conference/usenixsecurity20/presentation/komlo">Link</a><br/><a href="https://www.usenix.org/system/files/sec20-komlo.pdf">PDF</a> |
|                     | Differentially-Private Control-Flow Node Coverage for Software Usage Analysis | <a href="https://www.usenix.org/conference/usenixsecurity20/presentation/zhang-hailong">Link</a><br/><a href="https://par.nsf.gov/servlets/purl/10191952">PDF</a> |
|                     | Visor: Privacy-Preserving Video Analytics as a Cloud Service | <a href="https://www.usenix.org/conference/usenixsecurity20/presentation/poddar">Link</a><br/><a href="https://www.usenix.org/system/files/sec20-poddar.pdf">PDF</a> |
|                     | DELF: Safeguarding deletion correctness in Online Social Networks | <a href="https://www.usenix.org/conference/usenixsecurity20/presentation/cohn-gordon">Link</a><br/><a href="https://www.usenix.org/system/files/sec20-cohn-gordon.pdf">PDF</a> |
| Private Computing   | Secure Multi-party Computation of Differentially Private Median | <a href="https://www.usenix.org/conference/usenixsecurity20/presentation/boehler">Link</a><br/><a href="https://www.usenix.org/system/files/sec20-bohler.pdf">PDF</a> |
|                     | Delphi: A Cryptographic Inference Service for Neural Networks | <a href="https://www.usenix.org/conference/usenixsecurity20/presentation/mishra">Link</a><br/><a href="https://www.usenix.org/system/files/sec20spring_mishra_prepub.pdf">PDF</a> |
| Speech Attack       | Devil’s Whisper: A General Approach for Physical Adversarial Attacks against Commercial Black-box Speech Recognition Devices | <a href="https://www.usenix.org/conference/usenixsecurity20/presentation/chen-yuxuan">Link</a><br/><a href="https://www.usenix.org/system/files/sec20summer_chen-yuxuan_prepub.pdf">PDF</a> |

#### NDSS 2020

|                                | NDSS 2020 [<a href="https://www.ndss-symposium.org/ndss2020/">Homepage</a>] [<a href="https://www.ndss-symposium.org/ndss2020/accepted-papers/">Accepted</a>] |                                            |
| ------------------------------ | ------------------------------------------------------------ | ------------------------------------------ |
| Private Computation & Learning | Trident: Efficient 4PC Framework for Privacy Preserving Machine Learning | <a href="https://www.ndss-symposium.org/ndss-paper/trident-efficient-4pc-framework-for-privacy-preserving-machine-learning/">Link</a><br/><a href="https://www.ndss-symposium.org/wp-content/uploads/2020/02/23005-paper.pdf">PDF</a> |
|                                | Secure Sublinear Time Differentially Private Median Computation | <a href="https://www.ndss-symposium.org/ndss-paper/secure-sublinear-time-differentially-private-median-computation/">Link</a><br/><a href="https://www.ndss-symposium.org/wp-content/uploads/2020/02/24150-paper.pdf">PDF</a> |
|                                | CloudLeak: Large-Scale Deep Learning Models Stealing Through Adversarial Examples | <a href="https://www.ndss-symposium.org/ndss-paper/cloudleak-large-scale-deep-learning-models-stealing-through-adversarial-examples/">Link</a><br/><a href="https://www.ndss-symposium.org/wp-content/uploads/2020/02/24178-paper.pdf">PDF</a> |
|                                | BLAZE: Blazing Fast Privacy-Preserving Machine Learning      | <a href="https://www.ndss-symposium.org/ndss-paper/blaze-blazing-fast-privacy-preserving-machine-learning/">Link</a><br/><a href="https://www.ndss-symposium.org/wp-content/uploads/2020/02/24202-paper.pdf">PDF</a> |
| Privacy                        | Towards Plausible Graph Anonymization                        | <a href="https://www.ndss-symposium.org/ndss-paper/towards-plausible-graph-anonymization/">Link</a><br/><a href="https://www.ndss-symposium.org/wp-content/uploads/2020/02/23032-paper.pdf">PDF</a> |
|                                | Adversarial Classification Under Differential Privacy        | <a href="https://www.ndss-symposium.org/ndss-paper/adversarial-classification-under-differential-privacy/">Link</a><br/><a href="https://www.ndss-symposium.org/wp-content/uploads/2020/02/23047-paper.pdf">PDF</a> |
|                                | Locally Differentially Private Frequency Estimation with Consistency | <a href="https://www.ndss-symposium.org/ndss-paper/locally-differentially-private-frequency-estimation-with-consistency/">Link</a><br/><a href="https://www.ndss-symposium.org/wp-content/uploads/2020/02/24157-paper.pdf">PDF</a> |
|                                | DESENSITIZATION: Privacy-Aware and Attack-Preserving Crash Report | <a href="https://www.ndss-symposium.org/ndss-paper/desensitization-privacy-aware-and-attack-preserving-crash-report/">Link</a><br/><a href="https://www.ndss-symposium.org/wp-content/uploads/2020/02/24428-paper.pdf">PDF</a> |





## HTML File

### 2023

#### S&P 2023

<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  overflow:hidden;padding:10px 5px;word-break:normal;}
.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-cly1{text-align:left;vertical-align:middle}
.tg .tg-axmw{font-size:18px;font-weight:bold;text-align:center;vertical-align:middle}
.tg .tg-wa1i{font-weight:bold;text-align:center;vertical-align:middle}
</style>
<table class="tg">
<thead>
  <tr>
    <th class="tg-axmw" colspan="3" align="center">S&amp;P 2023 &nbsp; [<a href="https://www.ieee-security.org/TC/SP2023/index.html">Homepage</a>] [<a href="https://www.ieee-security.org/TC/SP2023/program-papers.html">Accepted</a>] </th>
  </tr>
</thead>
<tbody>
  <tr>
    <td class="tg-wa1i" rowspan="8">First Cycle</td>
    <td class="tg-cly1">FedRecover: Recovering from Poisoning Attacks in Federated Learning using Historical Information</td>
    <td class="tg-cly1"><a href="https://arxiv.org/pdf/2210.10936">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Private, Efficient, and Accurate: Protecting Models Trained by Multi-party Learning with Differential Privacy</td>
    <td class="tg-cly1"><a href="https://arxiv.org/pdf/2208.08662">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">D-DAE: Defense-Penetrating Model Extraction Attacks</td>
    <td class="tg-cly1"></td>
  </tr>
  <tr>
    <td class="tg-cly1">Examining Zero-Shot Vulnerability Repair with Large Language Models</td>
    <td class="tg-cly1"><a href="https://arxiv.org/pdf/2112.02125">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">SoK: Certified Robustness for Deep Neural Networks</td>
    <td class="tg-cly1"><a href="https://arxiv.org/pdf/2009.04131">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">RAB: Provable Robustness Against Backdoor Attacks</td>
    <td class="tg-cly1"><a href="https://arxiv.org/pdf/2003.08904">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">SoK: Anti-Facial Recognition Technology</td>
    <td class="tg-cly1"><a href="https://arxiv.org/pdf/2112.04558">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Deepfake Text Detection: Limitations and Opportunities</td>
    <td class="tg-cly1"><a href="https://arxiv.org/pdf/2210.09421">PDF</a></td>
  </tr>
</tbody>
</table>
<br/>

#### SEC 2023

<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  overflow:hidden;padding:10px 5px;word-break:normal;}
.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-cly1{text-align:left;vertical-align:middle}
.tg .tg-axmw{font-size:18px;font-weight:bold;text-align:center;vertical-align:middle}
.tg .tg-yla0{font-weight:bold;text-align:left;vertical-align:middle}
</style>
<table class="tg">
<thead>
  <tr>
    <th class="tg-axmw" colspan="3" align="center">USENIX Security 2023 &nbsp; [<a href="https://www.usenix.org/conference/usenixsecurity23">Homepage</a>] [<a href="https://www.usenix.org/conference/usenixsecurity23/summer-accepted-papers">Accepted</a>] </th>
  </tr>
</thead>
<tbody>
  <tr>
    <td class="tg-yla0" rowspan="16">Summer Cycle</td>
    <td class="tg-cly1">V-Cloak: Intelligibility-, Naturalness- &amp; Timbre-Preserving Real-Time Voice Anonymization</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity23/presentation/deng">Link</a><br><a href="https://arxiv.org/pdf/2210.15140">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">TPatch: A Triggered Physical Adversarial Patch</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity23/presentation/zhu">Link</a><br><a href="https://www.usenix.org/system/files/sec23summer_123-zhu-prepub.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">UnGANable: Defending Against GAN-based Face Manipulation</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity23/presentation/lizheng">Link</a><br><a href="https://www.usenix.org/system/files/sec23summer_136-li_zheng-prepub.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Squint Hard Enough: Attacking Perceptual Hashing with Adversarial Machine Learning</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity23/presentation/prokos">Link</a><br><a href="https://www.usenix.org/system/files/sec23summer_146-prokos-prepub.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">PrivTrace: Differentially Private Trajectory Synthesis by Adaptive Markov Models</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity23/presentation/wanghaiming">Link</a><br><a href="https://www.usenix.org/system/files/sec23summer_179-wang_haiming-prepub.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">The Space of Adversarial Strategies</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity23/presentation/sheatsley">Link</a><br><a href="https://www.usenix.org/system/files/sec23summer_256-sheatsley-prepub.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">That Person Moves Like A Car: Misclassification Attack Detection for Autonomous Systems Using Spatiotemporal Consistency</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity23/presentation/man">Link</a><br><a href="https://www.usenix.org/system/files/sec23summer_278-man-prepub.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">NeuroPots: Realtime Proactive Defense against Bit-Flip Attacks in Neural Networks</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity23/presentation/liuqi">Link</a><br><a href="https://www.usenix.org/system/files/sec23summer_334-liu_qi-prepub.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">URET: Universal Robustness Evaluation Toolkit (for Evasion)</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity23/presentation/eykholt">Link</a><br><a href="https://www.usenix.org/system/files/sec23summer_347-eykholt-prepub.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Towards a General Video-based Keystroke Inference Attack</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity23/presentation/yangzhuolin">Link</a><br><a href="https://www.usenix.org/system/files/sec23summer_338-yang_zhuolin-prepub.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">You Can't See Me: Physical Removal Attacks on LiDAR-based Autonomous Vehicles Driving Frameworks</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity23/presentation/cao">Link</a><br><a href="https://www.usenix.org/system/files/sec23summer_349-cao-prepub.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">SMACK: Semantically Meaningful Adversarial Audio Attack</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity23/presentation/yuzhiyuan">Link</a><br><a href="https://www.usenix.org/system/files/sec23summer_371-yu_zhiyuan-prepub.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Gradient Obfuscation Gives a False Sense of Security in Federated Learning</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity23/presentation/yue">Link</a><br><a href="https://www.usenix.org/system/files/sec23summer_372-yue-prepub.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Fairness Properties of Face Recognition and Obfuscation Systems</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity23/presentation/rosenberg">Link</a><br><a href="https://www.usenix.org/system/files/sec23summer_388-rosenberg-prepub.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Decompiling x86 Deep Neural Network Executables</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity23/presentation/liuzhibo">Link</a><br><a href="https://www.usenix.org/system/files/sec23summer_406-liu_zhibo-prepub.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">PCAT: Functionality and Data Stealing from Split Learning by Pseudo-Client Attack</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity23/presentation/gao">Link</a><br><a href="https://www.usenix.org/system/files/sec23summer_445-gao-prepub.pdf">PDF</a></td>
  </tr>
</tbody>
</table>

<p width="100%" align="right"><a href="#">🔝</a></p>

<br/>

### 2022

#### S&P 2022

<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  overflow:hidden;padding:10px 5px;word-break:normal;}
.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-cly1{text-align:left;vertical-align:middle}
.tg .tg-axmw{font-size:18px;font-weight:bold;text-align:center;vertical-align:middle}
.tg .tg-yla0{font-weight:bold;text-align:left;vertical-align:middle}
</style>
<table class="tg">
<thead>
  <tr>
    <th class="tg-axmw" colspan="3" align="center">S&amp;P 2022 &nbsp; [<a href="https://www.ieee-security.org/TC/SP2022/index.html">Homepage</a>] [<a href="https://www.ieee-security.org/TC/SP2022/program-papers.html">Accepted</a>] </th>
  </tr>
</thead>
<tbody>
  <tr>
    <td class="tg-yla0" rowspan="10">ML Attacks</td>
    <td class="tg-cly1">Back to the Drawing Board: A Critical Evaluation of Poisoning Attacks on Production Federated Learning</td>
    <td class="tg-cly1"><a href="https://ieeexplore.ieee.org/document/9833647/">Link</a><br><a href="https://arxiv.org/pdf/2108.10241">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Model Orthogonalization: Class Distance Hardening in Neural Networks for Better Security</td>
    <td class="tg-cly1"><a href="https://ieeexplore.ieee.org/document/9833688/">Link</a><br><a href="https://www.cs.purdue.edu/homes/zhan3299/res/SP22_Tao.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Universal 3-Dimensional Perturbations for Black-Box Attacks on Video Recognition Systems</td>
    <td class="tg-cly1"><a href="https://ieeexplore.ieee.org/document/9833776/">Link</a><br><a href="https://arxiv.org/pdf/2107.04284">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">"Adversarial Examples" for Proof-of-Learning</td>
    <td class="tg-cly1"><a href="https://ieeexplore.ieee.org/document/9833596/">Link</a><br><a href="https://arxiv.org/pdf/2108.09454">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Transfer Attacks Revisited: A Large-Scale Empirical Study in Real Computer Vision Settings</td>
    <td class="tg-cly1"><a href="https://ieeexplore.ieee.org/document/9833783/">Link</a><br><a href="https://arxiv.org/pdf/2204.04063">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Membership Inference Attacks From First Principles</td>
    <td class="tg-cly1"><a href="https://ieeexplore.ieee.org/document/9833649/">Link</a><br><a href="https://arxiv.org/pdf/2112.03570">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Bad Characters: Imperceptible NLP Attacks</td>
    <td class="tg-cly1"><a href="https://ieeexplore.ieee.org/document/9833641/">Link</a><br><a href="https://arxiv.org/pdf/2106.09898">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">LinkTeller: Recovering Private Edges from Graph Neural Networks via Influence Analysis</td>
    <td class="tg-cly1"><a href="https://ieeexplore.ieee.org/document/9833806/">Link</a><br><a href="https://arxiv.org/pdf/2108.06504">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">PICCOLO: Exposing Complex Backdoors in NLP Transformer Models</td>
    <td class="tg-cly1"><a href="https://ieeexplore.ieee.org/document/9833579/">Link</a><br><a href="https://www.cs.purdue.edu/homes/taog/docs/SP22_Liu.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">BadEncoder: Backdoor Attacks to Pre-trained Encoders in Self-Supervised Learning</td>
    <td class="tg-cly1"><a href="https://ieeexplore.ieee.org/document/9833644/">Link</a><br><a href="https://arxiv.org/pdf/2108.00352">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-yla0" rowspan="4">Poisoning &amp; Model Stealing</td>
    <td class="tg-cly1">Property Inference from Poisoning</td>
    <td class="tg-cly1"><a href="https://ieeexplore.ieee.org/document/9833623/">Link</a><br><a href="https://arxiv.org/pdf/2101.11073">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Reconstructing Training Data with Informed Adversaries</td>
    <td class="tg-cly1"><a href="https://ieeexplore.ieee.org/document/9833677/">Link</a><br><a href="https://arxiv.org/pdf/2201.04845">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">DeepSteal: Advanced Model Extractions Leveraging Efficient Weight Stealing in Memories</td>
    <td class="tg-cly1"><a href="https://ieeexplore.ieee.org/document/9833743/">Link</a><br><a href="https://arxiv.org/pdf/2111.04625">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Model Stealing Attacks Against Inductive Graph Neural Networks</td>
    <td class="tg-cly1"><a href="https://ieeexplore.ieee.org/document/9833607/">Link</a><br><a href="https://arxiv.org/pdf/2112.08331">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-yla0" rowspan="5">Applications of ML</td>
    <td class="tg-cly1">Asleep at the Keyboard? Assessing the Security of GitHub Copilot’s Code Contributions</td>
    <td class="tg-cly1"><a href="https://ieeexplore.ieee.org/document/9833571/">Link</a><br><a href="https://arxiv.org/pdf/2108.09293">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Spinning Language Models: Risks of Propaganda-as-a-Service and Countermeasures</td>
    <td class="tg-cly1"><a href="https://ieeexplore.ieee.org/document/9833572/">Link</a><br><a href="https://arxiv.org/pdf/2112.05224">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">SoK: How Robust is Image Classification Deep Neural Network Watermarking?</td>
    <td class="tg-cly1"><a href="https://ieeexplore.ieee.org/document/9833693/">Link</a><br><a href="https://arxiv.org/pdf/2108.04974">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Transcending TRANSCEND: Revisiting Malware Classification in the Presence of Concept Drift</td>
    <td class="tg-cly1"><a href="https://ieeexplore.ieee.org/document/9833659/">Link</a><br><a href="https://arxiv.org/pdf/2010.03856">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Copy, Right? A Testing Framework for Copyright Protection of Deep Learning Models</td>
    <td class="tg-cly1"><a href="https://ieeexplore.ieee.org/document/9833747/">Link</a><br><a href="https://arxiv.org/pdf/2112.05588">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-yla0" rowspan="5">Differential Privacy</td>
    <td class="tg-cly1">Statistical Quantification of Differential Privacy: A Local Approach</td>
    <td class="tg-cly1"><a href="https://ieeexplore.ieee.org/document/9833689/">Link</a><br><a href="https://arxiv.org/pdf/2108.09528">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Locally Differentially Private Sparse Vector Aggregation</td>
    <td class="tg-cly1"><a href="https://ieeexplore.ieee.org/document/9833635/">Link</a><br><a href="https://arxiv.org/pdf/2112.03449">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Differentially Private Histograms in the Shuffle Model from Fake Users</td>
    <td class="tg-cly1"><a href="https://ieeexplore.ieee.org/document/9833614/">Link</a><br><a href="https://arxiv.org/pdf/2104.02739">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Differential Privacy and Swapping: Examining De-Identification's Impact on Minority Representation and Privacy Preservation in the U.S. Census</td>
    <td class="tg-cly1"><a href="https://ieeexplore.ieee.org/document/9833668/">Link</a><br><a href="https://www.cs.columbia.edu/~mchrist/dp_swap_census.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Are We There Yet? Timing and Floating-Point Attacks on Differential Privacy Systems</td>
    <td class="tg-cly1"><a href="https://ieeexplore.ieee.org/document/9833672/">Link</a><br><a href="https://arxiv.org/pdf/2112.05307">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-yla0">Privacy Applications</td>
    <td class="tg-cly1">Sphinx: Enabling Privacy-Preserving Online Learning over the Cloud</td>
    <td class="tg-cly1"><a href="https://ieeexplore.ieee.org/document/9833648/">Link</a><br><a href="https://cse.hkust.edu.hk/~kaichen/papers/sphinx-sp22.pdf">PDF</a></td>
  </tr>
</tbody>
</table>
<br/>

#### CCS 2022

<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  overflow:hidden;padding:10px 5px;word-break:normal;}
.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-cly1{text-align:left;vertical-align:middle}
.tg .tg-axmw{font-size:18px;font-weight:bold;text-align:center;vertical-align:middle}
.tg .tg-yla0{font-weight:bold;text-align:left;vertical-align:middle}
</style>
<table class="tg">
<thead>
  <tr>
    <th class="tg-axmw" colspan="3" align="center">CCS 2022 &nbsp; [<a href="https://www.sigsac.org/ccs/CCS2022/home.html">Homepage</a>] [<a href="https://www.sigsac.org/ccs/CCS2022/program/accepted-papers.html">Accepted</a>] </th>
  </tr>
</thead>
<tbody>
  <tr>
    <td class="tg-yla0" rowspan="4">Inference Attacks to ML</td>
    <td class="tg-cly1">Group Property Inference Attacks Against Graph Neural Networks</td>
    <td class="tg-cly1"><a href="https://dl.acm.org/doi/10.1145/3548606.3560662">Link</a><br><a href="https://arxiv.org/pdf/2209.01100">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Are Attribute Inference Attacks Just Imputation?</td>
    <td class="tg-cly1"><a href="https://dl.acm.org/doi/10.1145/3548606.3560663">Link</a><br><a href="https://arxiv.org/pdf/2209.01292">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Enhanced Membership Inference Attacks against Machine Learning Models</td>
    <td class="tg-cly1"><a href="https://dl.acm.org/doi/10.1145/3548606.3560675">Link</a><br><a href="https://arxiv.org/pdf/2111.09679">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Membership Inference Attacks and Generalization: A Causal Perspective</td>
    <td class="tg-cly1"><a href="https://dl.acm.org/doi/10.1145/3548606.3560694">Link</a><br><a href="https://arxiv.org/pdf/2209.08615">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-yla0" rowspan="4">Privacy Attacks in ML</td>
    <td class="tg-cly1">SSLGuard: A Watermarking Scheme for Self-supervised Learning Pre-trained Encoders</td>
    <td class="tg-cly1"><a href="https://dl.acm.org/doi/10.1145/3548606.3559355">Link</a><br><a href="https://arxiv.org/pdf/2201.11692">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Auditing Membership Leakages of Multi-Exit Networks</td>
    <td class="tg-cly1"><a href="https://dl.acm.org/doi/10.1145/3548606.3559359">Link</a><br><a href="https://arxiv.org/pdf/2208.11180">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">StolenEncoder: Stealing Pre-trained Encoders</td>
    <td class="tg-cly1"><a href="https://dl.acm.org/doi/10.1145/3548606.3560586">Link</a><br><a href="https://arxiv.org/pdf/2201.05889">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Membership Inference Attacks by Exploiting Loss Trajectory</td>
    <td class="tg-cly1"><a href="https://dl.acm.org/doi/10.1145/3548606.3560684">Link</a><br><a href="https://arxiv.org/pdf/2208.14933">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-yla0" rowspan="2">Privacy in Graphs</td>
    <td class="tg-cly1">Graph Unlearning</td>
    <td class="tg-cly1"><a href="https://dl.acm.org/doi/10.1145/3548606.3559352">Link</a><br><a href="https://arxiv.org/pdf/2103.14991">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Finding MNEMON: Reviving Memories of Node Embeddings</td>
    <td class="tg-cly1"><a href="https://dl.acm.org/doi/10.1145/3548606.3559358">Link</a><br><a href="https://arxiv.org/pdf/2204.06963">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-yla0" rowspan="4">Privacy Preserving ML</td>
    <td class="tg-cly1">DPIS: an Enhanced Mechanism for Differentially Private SGD with Importance Sampling</td>
    <td class="tg-cly1"><a href="https://dl.acm.org/doi/10.1145/3548606.3560562">Link</a><br><a href="https://arxiv.org/pdf/2210.09634">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">NFGen: Automatic Non-linear Function Evaluation Code Generator for General-purpose MPC Platforms</td>
    <td class="tg-cly1"><a href="https://dl.acm.org/doi/10.1145/3548606.3560565">Link</a><br><a href="https://arxiv.org/pdf/2210.09802">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">On the Privacy Risks of Cell-Based NAS Architectures</td>
    <td class="tg-cly1"><a href="https://dl.acm.org/doi/10.1145/3548606.3560619">Link</a><br><a href="https://arxiv.org/pdf/2209.01688">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">LPGNet: Link Private Graph Networks for Node Classification</td>
    <td class="tg-cly1"><a href="https://dl.acm.org/doi/10.1145/3548606.3560705">Link</a><br><a href="https://arxiv.org/pdf/2205.03105">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-yla0" rowspan="4">Poisoning &amp; Backdooring ML</td>
    <td class="tg-cly1">Identifying a Training-Set Attack’s Target Using Renormalized Influence Estimation</td>
    <td class="tg-cly1"><a href="https://dl.acm.org/doi/10.1145/3548606.3559335">Link</a><br><a href="https://arxiv.org/pdf/2201.10055">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">FenceSitter: Black-box, Content-Agnostic, and Synchronization-Free Enrollment-Phase Attacks on Speaker Recognition Systems</td>
    <td class="tg-cly1"><a href="https://dl.acm.org/doi/10.1145/3548606.3559357">Link</a><br><a href="https://www.tablesgenerator.com/html_tables">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Truth Serum: Poisoning Machine Learning Models to Reveal Their Secrets</td>
    <td class="tg-cly1"><a href="https://dl.acm.org/doi/10.1145/3548606.3560554">Link</a><br><a href="https://arxiv.org/pdf/2204.00032">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">LoneNeuron: a Highly-effective Feature-domain Neural Trojan using Invisible and Polymorphic Watermarks</td>
    <td class="tg-cly1"><a href="https://dl.acm.org/doi/10.1145/3548606.3560678">Link</a><br><a href="https://www.ittc.ku.edu/~bluo/download/liu2022ccs.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-yla0" rowspan="4">Adversarial Examples in ML</td>
    <td class="tg-cly1">Perception-Aware Attack: Creating Adversarial Music via Reverse-Engineering Human Perception</td>
    <td class="tg-cly1"><a href="https://dl.acm.org/doi/10.1145/3548606.3559350">Link</a><br><a href="https://arxiv.org/pdf/2207.13192">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">''Is your explanation stable?'': A Robustness Evaluation Framework for Feature Attribution</td>
    <td class="tg-cly1"><a href="https://dl.acm.org/doi/10.1145/3548606.3559392">Link</a><br><a href="https://arxiv.org/pdf/2209.01782">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Post-breach Recovery: Protection against White-box Adversarial Examples for Leaked DNN Models</td>
    <td class="tg-cly1"><a href="https://dl.acm.org/doi/10.1145/3548606.3560561">Link</a><br><a href="https://arxiv.org/pdf/2205.10686">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Harnessing Perceptual Adversarial Patches for Crowd Counting</td>
    <td class="tg-cly1"><a href="https://dl.acm.org/doi/10.1145/3548606.3560566">Link</a><br><a href="https://arxiv.org/pdf/2109.07986">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-yla0" rowspan="4">Attacks to ML</td>
    <td class="tg-cly1">Physical Hijacking Attacks against Object Trackers</td>
    <td class="tg-cly1"><a href="https://dl.acm.org/doi/10.1145/3548606.3559390">Link</a><br><a href="https://dl.acm.org/doi/pdf/10.1145/3548606.3559390">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Feature Inference Attack on Shapley Values</td>
    <td class="tg-cly1"><a href="https://dl.acm.org/doi/10.1145/3548606.3560573">Link</a><br><a href="https://dl.acm.org/doi/pdf/10.1145/3548606.3560573">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">When Evil Calls: Targeted Adversarial Voice over IP-Telephony Network</td>
    <td class="tg-cly1"><a href="https://dl.acm.org/doi/10.1145/3548606.3560671">Link</a><br><a href="https://dl.acm.org/doi/pdf/10.1145/3548606.3560671">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Order-Disorder: Imitation Adversarial Attacks for Black-box Neural Ranking Models</td>
    <td class="tg-cly1"><a href="https://dl.acm.org/doi/10.1145/3548606.3560683">Link</a><br><a href="https://arxiv.org/pdf/2209.06506">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-yla0" rowspan="4">FL Security</td>
    <td class="tg-cly1">Eluding Secure Aggregation in Federated Learning via Model Inconsistency</td>
    <td class="tg-cly1"><a href="https://dl.acm.org/doi/10.1145/3548606.3560557">Link</a><br><a href="https://arxiv.org/pdf/2111.07380">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">EIFFeL: Ensuring Integrity for Federated Learning</td>
    <td class="tg-cly1"><a href="https://dl.acm.org/doi/10.1145/3548606.3560611">Link</a><br><a href="https://arxiv.org/pdf/2112.12727">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">pMPL: A Robust Multi-Party Learning Framework with a Privileged Party</td>
    <td class="tg-cly1"><a href="https://dl.acm.org/doi/10.1145/3548606.3560697">Link</a><br><a href="https://arxiv.org/pdf/2210.00486">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Private and Reliable Neural Network Inference</td>
    <td class="tg-cly1"><a href="https://dl.acm.org/doi/10.1145/3548606.3560709">Link</a><br><a href="https://arxiv.org/pdf/2210.15614">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-yla0" rowspan="4">Federated Analytics &amp; Learning</td>
    <td class="tg-cly1">Distributed, Private, Sparse Histograms in the Two-Server Model</td>
    <td class="tg-cly1"><a href="https://dl.acm.org/doi/10.1145/3548606.3559383">Link</a><br><a href="https://eprint.iacr.org/2022/920.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Selective MPC: Distributed Computation of Differentially Private Key-Value Statistics</td>
    <td class="tg-cly1"><a href="https://dl.acm.org/doi/10.1145/3548606.3560559">Link</a><br><a href="https://arxiv.org/pdf/2107.12407">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">STAR: Secret Sharing for Private Threshold Aggregation Reporting</td>
    <td class="tg-cly1"><a href="https://dl.acm.org/doi/10.1145/3548606.3560631">Link</a><br><a href="https://arxiv.org/pdf/2109.10074">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Federated Boosted Decision Trees with Differential Privacy</td>
    <td class="tg-cly1"><a href="https://dl.acm.org/doi/10.1145/3548606.3560687">Link</a><br><a href="https://arxiv.org/pdf/2210.02910">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-yla0" rowspan="4">Differential Privacy</td>
    <td class="tg-cly1">Shifted Inverse: A General Mechanism for Monotonic Functions under User Differential Privacy</td>
    <td class="tg-cly1"><a href="https://dl.acm.org/doi/10.1145/3548606.3560567">Link</a><br><a href="https://www.cse.ust.hk/~yike/ShiftedInverse.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Frequency Estimation in the Shuffle Model with Almost a Single Message</td>
    <td class="tg-cly1"><a href="https://dl.acm.org/doi/10.1145/3548606.3560608">Link</a><br><a href="https://arxiv.org/pdf/2111.06833">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Differentially Private Triangle and 4-Cycle Counting in the Shuffle Model</td>
    <td class="tg-cly1"><a href="https://dl.acm.org/doi/10.1145/3548606.3560659">Link</a><br><a href="https://arxiv.org/pdf/2205.01429">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Widespread Underestimation of Sensitivity in Differentially Private Libraries and How to Fix It</td>
    <td class="tg-cly1"><a href="https://dl.acm.org/doi/10.1145/3548606.3560708">Link</a><br><a href="https://arxiv.org/pdf/2207.10635">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-yla0" rowspan="4">ML for Network Security</td>
    <td class="tg-cly1">CERBERUS: Federated Prediction of Security Events</td>
    <td class="tg-cly1"><a href="https://dl.acm.org/doi/10.1145/3548606.3560595">Link</a><br><a href="https://arxiv.org/pdf/2209.03050">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Exposing the Rat in the Tunnel: Using Traffic Analysis for Tor-based Malware Detection</td>
    <td class="tg-cly1"><a href="https://dl.acm.org/doi/10.1145/3548606.3560604">Link</a><br><a href="https://alrawi.io/static/papers/tor-malware_ccs22.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">AI/ML for Network Security: The Emperor has no Clothes</td>
    <td class="tg-cly1"><a href="https://dl.acm.org/doi/10.1145/3548606.3560609">Link</a><br><a href="http://www.inf.ufrgs.br/~granville/wp-content/papercite-data/pdf/ccsjacobs2022.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Phishing URL Detection: A Network-based Approach Robust to Evasion</td>
    <td class="tg-cly1"><a href="https://dl.acm.org/doi/10.1145/3548606.3560615">Link</a><br><a href="https://arxiv.org/pdf/2209.01454">PDF</a></td>
  </tr>
</tbody>
</table>
<br/>

#### SEC 2022

<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  overflow:hidden;padding:10px 5px;word-break:normal;}
.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-cly1{text-align:left;vertical-align:middle}
.tg .tg-axmw{font-size:18px;font-weight:bold;text-align:center;vertical-align:middle}
.tg .tg-yla0{font-weight:bold;text-align:left;vertical-align:middle}
</style>
<table class="tg">
<thead>
  <tr>
    <th class="tg-axmw" colspan="3" align="center">USENIX Security 2022 &nbsp; [<a href="https://www.usenix.org/conference/usenixsecurity22">Homepage</a>] [<a href="https://www.usenix.org/conference/usenixsecurity22/technical-sessions">Accepted</a>] </th>
  </tr>
</thead>
<tbody>
  <tr>
    <td class="tg-yla0" rowspan="9">Machine Learning</td>
    <td class="tg-cly1">PatchCleanser: Certifiably Robust Defense against Adversarial Patches for Any Image Classifier</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity22/presentation/xiang">Link</a><br><a href="https://www.usenix.org/system/files/sec22fall_xiang.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Transferring Adversarial Robustness Through Robust Representation Matching</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity22/presentation/vaishnavi">Link</a><br><a href="https://www.usenix.org/system/files/sec22fall_vaishnavi.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">How Machine Learning Is Solving the Binary Function Similarity Problem</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity22/presentation/marcelli">Link</a><br><a href="https://www.usenix.org/system/files/sec22fall_marcelli.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Blacklight: Scalable Defense for Neural Networks against Query-Based Black-Box Attacks</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity22/presentation/li-huiying">Link</a><br><a href="https://www.usenix.org/system/files/sec22-li-huiying.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">DnD: A Cross-Architecture Deep Neural Network Decompiler</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity22/presentation/wu-ruoyu">Link</a><br><a href="https://www.usenix.org/system/files/sec22-wu-ruoyu.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Towards More Robust Keyword Spotting for Voice Assistants</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity22/presentation/ahmed">Link</a><br><a href="https://www.usenix.org/system/files/sec22summer_ahmed.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Seeing is Living? Rethinking the Security of Facial Liveness Verification in the Deepfake Era</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity22/presentation/li-changjiang">Link</a><br><a href="https://www.usenix.org/system/files/sec22fall_li-changjiang.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Who Are You (I Really Wanna Know)? Detecting Audio DeepFakes Through Vocal Tract Reconstruction</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity22/presentation/blue">Link</a><br><a href="https://www.usenix.org/system/files/sec22fall_blue.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">DeepDi: Learning a Relational Graph Convolutional Network Model on Instructions for Fast and Accurate Disassembly</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity22/presentation/yu-sheng">Link</a><br><a href="https://www.usenix.org/system/files/sec22summer_yu-sheng.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-yla0" rowspan="4">Federated Learning</td>
    <td class="tg-cly1">SIMC: ML Inference Secure Against Malicious Clients at Semi-Honest Cost</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity22/presentation/chandran">Link</a><br><a href="https://www.usenix.org/system/files/sec22summer_chandran.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Efficient Differentially Private Secure Aggregation for Federated Learning via Hardness of Learning with Errors</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity22/presentation/stevens">Link</a><br><a href="https://www.usenix.org/system/files/sec22fall_stevens.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Label Inference Attacks Against Vertical Federated Learning</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity22/presentation/fu-chong">Link</a><br><a href="https://www.usenix.org/system/files/sec22summer_fu.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">FLAME: Taming Backdoors in Federated Learning</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity22/presentation/nguyen">Link</a><br><a href="https://www.usenix.org/system/files/sec22fall_nguyen.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-yla0" rowspan="4">ML Inference</td>
    <td class="tg-cly1">ML-Doctor: Holistic Risk Assessment of Inference Attacks Against Machine Learning Models</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity22/presentation/liu-yugeng">Link</a><br><a href="https://www.usenix.org/system/files/sec22summer_liu-yugeng.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Inference Attacks Against Graph Neural Networks</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity22/presentation/zhang-zhikun">Link</a><br><a href="https://www.usenix.org/system/files/sec22summer_zhang-zhikun.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Membership Inference Attacks and Defenses in Neural Network Pruning</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity22/presentation/yuan-xiaoyong">Link</a><br><a href="https://www.usenix.org/system/files/sec22fall_yuan-xiaoyong.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Are Your Sensitive Attributes Private? Novel Model Inversion Attribute Inference Attacks on Classification Models</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity22/presentation/mehnaz">Link</a><br><a href="https://www.usenix.org/system/files/sec22fall_mehnaz.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-yla0" rowspan="5">ML Attacks</td>
    <td class="tg-cly1">AutoDA: Automated Decision-based Iterative Adversarial Attacks</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity22/presentation/fu-qi">Link</a><br><a href="https://www.usenix.org/system/files/sec22-fu-qi.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Poison Forensics: Traceback of Data Poisoning Attacks in Neural Networks</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity22/presentation/shan">Link</a><br><a href="https://www.usenix.org/system/files/sec22-shan.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Teacher Model Fingerprinting Attacks Against Transfer Learning</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity22/presentation/chen-yufei">Link</a><br><a href="https://www.usenix.org/system/files/sec22-chen-yufei.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Hidden Trigger Backdoor Attack on NLP Models via Linguistic Style Manipulation</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity22/presentation/pan-hidden">Link</a><br><a href="https://www.usenix.org/system/files/sec22-pan-hidden.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">PoisonedEncoder: Poisoning the Unlabeled Pre-training Data in Contrastive Learning</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity22/presentation/liu-hongbin">Link</a><br><a href="https://www.usenix.org/system/files/sec22-liu-hongbin.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-yla0" rowspan="4">Principles &amp; Best Practices</td>
    <td class="tg-cly1">On the Security Risks of AutoML</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity22/presentation/pang-ren">Link</a><br><a href="https://www.usenix.org/system/files/sec22summer_pang-ren.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Dos and Don'ts of Machine Learning in Computer Security</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity22/presentation/arp">Link</a><br><a href="https://www.usenix.org/system/files/sec22summer_arp.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Exploring the Security Boundary of Data Reconstruction via Neuron Exclusivity Analysis</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity22/presentation/pan-exploring">Link</a><br><a href="https://www.usenix.org/system/files/sec22fall_pan-exploring.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">On the Necessity of Auditable Algorithmic Definitions for Machine Unlearning</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity22/presentation/thudi">Link</a><br><a href="https://www.usenix.org/system/files/sec22fall_thudi.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-yla0" rowspan="4">Differential Privacy</td>
    <td class="tg-cly1">Pool Inference Attacks on Local Differential Privacy: Quantifying the Privacy Guarantees of Apple's Count Mean Sketch in Practice</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity22/presentation/gadotti">Link</a><br><a href="https://www.usenix.org/system/files/sec22-gadotti_1.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Poisoning Attacks to Local Differential Privacy Protocols for Key-Value Data</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity22/presentation/wu-yongji">Link</a><br><a href="https://www.usenix.org/system/files/sec22fall_wu-yongji.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Communication-Efficient Triangle Counting under Local Differential Privacy</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity22/presentation/imola">Link</a><br><a href="https://www.usenix.org/system/files/sec22fall_imola.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Twilight: A Differentially Private Payment Channel Network</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity22/presentation/dotan">Link</a><br><a href="https://www.usenix.org/system/files/sec22-dotan.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-yla0" rowspan="5">Deanonymization</td>
    <td class="tg-cly1">Mitigating Membership Inference Attacks by Self-Distillation Through a Novel Ensemble Architecture</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity22/presentation/tang">Link</a><br><a href="https://www.usenix.org/system/files/sec22fall_tang.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Synthetic Data – Anonymisation Groundhog Day</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity22/presentation/stadler">Link</a><br><a href="https://www.usenix.org/system/files/sec22summer_stadler.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Attacks on Deidentification's Defenses</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity22/presentation/cohen">Link</a><br><a href="https://www.usenix.org/system/files/sec22fall_cohen.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Birds of a Feather Flock Together: How Set Bias Helps to Deanonymize You via Revealed Intersection Sizes</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity22/presentation/guo">Link</a><br><a href="https://www.usenix.org/system/files/sec22-guo.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Targeted Deanonymization via the Cache Side Channel: Attacks and Defenses</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity22/presentation/zaheri">Link</a><br><a href="https://www.usenix.org/system/files/sec22-zaheri.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-yla0" rowspan="2">Performance Improvement</td>
    <td class="tg-cly1">Cheetah: Lean and Fast Secure Two-Party Deep Neural Network Inference</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity22/presentation/huang-zhicong">Link</a><br><a href="https://www.usenix.org/system/files/sec22fall_huang-zhicong.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Piranha: A GPU Platform for Secure Computation</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity22/presentation/watson">Link</a><br><a href="https://www.usenix.org/system/files/sec22-watson.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-yla0" rowspan="3">Other Interesting</td>
    <td class="tg-cly1">Can one hear the shape of a neural network?: Snooping the GPU via Magnetic Side Channel</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity22/presentation/maia">Link</a><br><a href="https://www.usenix.org/system/files/sec22summer_maia.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Rolling Colors: Adversarial Laser Exploits against Traffic Light Recognition</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity22/presentation/yan">Link</a><br><a href="https://www.usenix.org/system/files/sec22summer_yan.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Adversarial Detection Avoidance Attacks: Evaluating the robustness of perceptual hashing-based client-side scanning</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity22/presentation/jain">Link</a><br><a href="https://www.usenix.org/system/files/sec22summer_jain.pdf">PDF</a></td>
  </tr>
</tbody>
</table>
<br/>

#### NDSS 2022

<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  overflow:hidden;padding:10px 5px;word-break:normal;}
.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-cly1{text-align:left;vertical-align:middle}
.tg .tg-axmw{font-size:18px;font-weight:bold;text-align:center;vertical-align:middle}
.tg .tg-yla0{font-weight:bold;text-align:left;vertical-align:middle}
</style>
<table class="tg">
<thead>
  <tr>
    <th class="tg-axmw" colspan="3" align="center">NDSS 2022 &nbsp; [<a href="https://www.ndss-symposium.org/ndss2022/"><span style="color:inherit">Homepage</span></a>] [<a href="https://www.ndss-symposium.org/ndss2022/accepted-papers/"><span style="color:inherit">Accepted</span></a>] </th>
  </tr>
</thead>
<tbody>
  <tr>
    <td class="tg-yla0" rowspan="8">ML &amp; AI</td>
    <td class="tg-cly1">Tetrad: Actively Secure 4PC for Secure Training and Inference</td>
    <td class="tg-cly1"><a href="https://www.ndss-symposium.org/ndss-paper/auto-draft-202/"><span style="color:inherit">Link</span></a><span style="color:#A7A7A7"> </span><a href="https://www.ndss-symposium.org/wp-content/uploads/2022-58-paper.pdf"><span style="color:inherit">PDF</span></a></td>
  </tr>
  <tr>
    <td class="tg-cly1">MIRROR: Model Inversion for Deep LearningNetwork with High Fidelity</td>
    <td class="tg-cly1"><a href="https://www.ndss-symposium.org/ndss-paper/auto-draft-203/"><span style="color:inherit">Link</span></a><span style="color:#A7A7A7"> </span><a href="https://www.ndss-symposium.org/wp-content/uploads/2022-335-paper.pdf"><span style="color:inherit">PDF</span></a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Local and Central Differential Privacy for Robustness and Privacy in Federated Learning</td>
    <td class="tg-cly1"><a href="https://www.ndss-symposium.org/ndss-paper/auto-draft-204/"><span style="color:inherit">Link</span></a><span style="color:#A7A7A7"> </span><a href="https://www.ndss-symposium.org/wp-content/uploads/2022-54-paper.pdf"><span style="color:inherit">PDF</span></a></td>
  </tr>
  <tr>
    <td class="tg-cly1">DeepSight: Mitigating Backdoor Attacks in Federated Learning Through Deep Model Inspection</td>
    <td class="tg-cly1"><a href="https://www.ndss-symposium.org/ndss-paper/auto-draft-205/"><span style="color:inherit">Link</span></a><span style="color:#A7A7A7"> </span><a href="https://www.ndss-symposium.org/wp-content/uploads/2022-156-paper.pdf"><span style="color:inherit">PDF</span></a></td>
  </tr>
  <tr>
    <td class="tg-cly1">What You See is Not What the Network Infers: Detecting Adversarial Examples Based on Semantic Contradiction</td>
    <td class="tg-cly1"><a href="https://www.ndss-symposium.org/ndss-paper/auto-draft-226/"><span style="color:inherit">Link</span></a><span style="color:#A7A7A7"> </span><a href="https://www.ndss-symposium.org/wp-content/uploads/2022-1-paper.pdf"><span style="color:inherit">PDF</span></a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Euler: Detecting Network Lateral Movement via Scalable Temporal Graph Link Prediction</td>
    <td class="tg-cly1"><a href="https://www.ndss-symposium.org/ndss-paper/auto-draft-227/"><span style="color:inherit">Link</span></a><span style="color:#A7A7A7"> </span><a href="https://www.ndss-symposium.org/wp-content/uploads/2022-107A-paper.pdf"><span style="color:inherit">PDF</span></a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Fooling the Eyes of Autonomous Vehicles: Robust Physical Adversarial Examples Against Traffic Sign Recognition Systems</td>
    <td class="tg-cly1"><a href="https://www.ndss-symposium.org/ndss-paper/auto-draft-228/"><span style="color:inherit">Link</span></a><span style="color:#A7A7A7"> </span><a href="https://www.ndss-symposium.org/wp-content/uploads/2022-130-paper.pdf"><span style="color:inherit">PDF</span></a></td>
  </tr>
  <tr>
    <td class="tg-cly1">FedCRI: Federated Mobile Cyber-Risk Intelligence</td>
    <td class="tg-cly1"><a href="https://www.ndss-symposium.org/ndss-paper/auto-draft-229/"><span style="color:inherit">Link</span></a><span style="color:#A7A7A7"> </span><a href="https://www.ndss-symposium.org/wp-content/uploads/2022-153-paper.pdf"><span style="color:inherit">PDF</span></a></td>
  </tr>
  <tr>
    <td class="tg-yla0" rowspan="4">Attacks on ML/AI</td>
    <td class="tg-cly1">ATTEQ-NN: Attention-based QoE-aware Evasive Backdoor Attacks</td>
    <td class="tg-cly1"><a href="https://www.ndss-symposium.org/ndss-paper/auto-draft-238/"><span style="color:inherit">Link</span></a><span style="color:#A7A7A7"> </span><a href="https://www.ndss-symposium.org/wp-content/uploads/2022-12-paper.pdf"><span style="color:inherit">PDF</span></a></td>
  </tr>
  <tr>
    <td class="tg-cly1">RamBoAttack: A Robust and Query Efficient Deep Neural Network Decision Exploit</td>
    <td class="tg-cly1"><a href="https://www.ndss-symposium.org/ndss-paper/auto-draft-239/"><span style="color:inherit">Link</span></a><span style="color:#A7A7A7"> </span><a href="https://www.ndss-symposium.org/wp-content/uploads/2022-200-paper.pdf"><span style="color:inherit">PDF</span></a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Property Inference Attacks Against GANs</td>
    <td class="tg-cly1"><a href="https://www.ndss-symposium.org/ndss-paper/auto-draft-240/"><span style="color:inherit">Link</span></a><span style="color:#A7A7A7"> </span><a href="https://www.ndss-symposium.org/wp-content/uploads/2022-19-paper.pdf"><span style="color:inherit">PDF</span></a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Get a Model! Model Hijacking Attack Against Machine Learning Models</td>
    <td class="tg-cly1"><a href="https://www.ndss-symposium.org/ndss-paper/auto-draft-241/"><span style="color:inherit">Link</span></a><span style="color:#A7A7A7"> </span><a href="https://www.ndss-symposium.org/wp-content/uploads/2022-64-paper.pdf"><span style="color:inherit">PDF</span></a></td>
  </tr>
</tbody>
</table>

<p width="100%" align="right"><a href="#">🔝</a></p>

<br/>

### 2021

#### S&P 2021

<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  overflow:hidden;padding:10px 5px;word-break:normal;}
.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-cly1{text-align:left;vertical-align:middle}
.tg .tg-axmw{font-size:18px;font-weight:bold;text-align:center;vertical-align:middle}
.tg .tg-yla0{font-weight:bold;text-align:left;vertical-align:middle}
</style>
<table class="tg">
<thead>
  <tr>
    <th class="tg-axmw" colspan="3" align="center">S&amp;P 2021 &nbsp; [<a href="https://www.ieee-security.org/TC/SP2021/index.html">Homepage</a>] [<a href="https://www.ieee-security.org/TC/SP2021/program-papers.html">Accepted</a>] </th>
  </tr>
</thead>
<tbody>
  <tr>
    <td class="tg-yla0" rowspan="3">Adversarial ML &amp; Unlearning</td>
    <td class="tg-cly1">Detecting AI Trojans Using Meta Neural Analysis</td>
    <td class="tg-cly1"><a href="https://ieeexplore.ieee.org/document/9519467/">Link</a><br><a href="https://arxiv.org/pdf/1910.03137.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Adversarial Watermarking Transformer: Towards Tracing Text Provenance with Data Hiding</td>
    <td class="tg-cly1"><a href="https://ieeexplore.ieee.org/document/9519400/">Link</a><br><a href="https://arxiv.org/pdf/2009.03015.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Machine Unlearning</td>
    <td class="tg-cly1"><a href="https://ieeexplore.ieee.org/document/9519428/">Link</a><br><a href="https://arxiv.org/pdf/1912.03817">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-yla0" rowspan="3">Privacy</td>
    <td class="tg-cly1">Defensive Technology Use by Political Activists During the Sudanese Revolution</td>
    <td class="tg-cly1"><a href="https://ieeexplore.ieee.org/document/9519493/">Link</a><br><a href="https://par.nsf.gov/servlets/purl/10231786">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">DP-Sniper: Black-Box Discovery of Differential Privacy Violations using Classifiers</td>
    <td class="tg-cly1"><a href="https://ieeexplore.ieee.org/document/9519405/">Link</a><br><a href="https://files.sri.inf.ethz.ch/website/papers/sp21-dpsniper.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Is Private Learning Possible with Instance Encoding?</td>
    <td class="tg-cly1"><a href="https://ieeexplore.ieee.org/document/9519450/">Link</a><br><a href="https://arxiv.org/pdf/2011.05315">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-yla0" rowspan="3">Differential Privacy</td>
    <td class="tg-cly1">Learning Differentially Private Mechanisms</td>
    <td class="tg-cly1"><a href="https://ieeexplore.ieee.org/document/9519410/">Link</a><br><a href="https://arxiv.org/pdf/2101.00961.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Adversary Instantiation: Lower bounds for differentially private machine learning</td>
    <td class="tg-cly1"><a href="https://ieeexplore.ieee.org/document/9519424/">Link</a><br><a href="https://arxiv.org/pdf/2101.04535">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Manipulation Attacks in Local Differential Privacy</td>
    <td class="tg-cly1"><a href="https://ieeexplore.ieee.org/document/9519418/">Link</a><br><a href="https://arxiv.org/pdf/1909.09630">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-yla0" rowspan="3">ML Security &amp; Privacy</td>
    <td class="tg-cly1">SIRNN: A Math Library for Secure RNN Inference</td>
    <td class="tg-cly1"><a href="https://ieeexplore.ieee.org/document/9519413/">Link</a><br><a href="https://arxiv.org/pdf/2105.04236">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">CryptGPU: Fast Privacy-Preserving Machine Learning on the GPU</td>
    <td class="tg-cly1"><a href="https://ieeexplore.ieee.org/document/9519386/">Link</a><br><a href="https://arxiv.org/pdf/2104.10949">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Proof-of-Learning: Definitions and Practice</td>
    <td class="tg-cly1"><a href="https://ieeexplore.ieee.org/document/9519402/">Link</a><br><a href="https://arxiv.org/pdf/2103.05633">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-yla0" rowspan="3">Speech Systems</td>
    <td class="tg-cly1">Who is Real Bob? Adversarial Attacks on Speaker Recognition Systems</td>
    <td class="tg-cly1"><a href="https://ieeexplore.ieee.org/document/9519486/">Link</a><br><a href="https://arxiv.org/pdf/1911.01840">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Hear "No Evil", See "Kenansville": Efficient and Transferable Black-Box Attacks on Speech Recognition and Voice Identification Systems</td>
    <td class="tg-cly1"><a href="https://ieeexplore.ieee.org/document/9519472/">Link</a><br><a href="https://arxiv.org/pdf/1910.05262">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">SoK: The Faults in our ASRs: An Overview of Attacks against Automatic Speech Recognition and Speaker Identification Systems</td>
    <td class="tg-cly1"><a href="https://ieeexplore.ieee.org/document/9519395/">Link</a><br><a href="https://arxiv.org/pdf/2007.06622">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-yla0" rowspan="2">Autonomous Vehicles</td>
    <td class="tg-cly1">Poltergeist: Acoustic Adversarial Machine Learning against Cameras and Computer Vision</td>
    <td class="tg-cly1"><a href="https://ieeexplore.ieee.org/document/9519394/">Link</a><br><a href="https://spqrlab1.github.io/papers/ji-poltergeist-oakland21.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Invisible for both Camera and LiDAR: Security of Multi-Sensor Fusion based Perception in Autonomous Driving Under Physical-World Attacks</td>
    <td class="tg-cly1"><a href="https://ieeexplore.ieee.org/document/9519442/">Link</a><br><a href="https://arxiv.org/pdf/2106.09249">PDF</a></td>
  </tr>
</tbody>
</table>

<br/>

#### CCS 2021

<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  overflow:hidden;padding:10px 5px;word-break:normal;}
.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-cly1{text-align:left;vertical-align:middle}
.tg .tg-axmw{font-size:18px;font-weight:bold;text-align:center;vertical-align:middle}
.tg .tg-yla0{font-weight:bold;text-align:left;vertical-align:middle}
</style>
<table class="tg">
<thead>
  <tr>
    <th class="tg-axmw" colspan="3" align="center">CCS 2021 &nbsp;  [<a href="https://www.sigsac.org/ccs/CCS2021/index.html">Homepage</a>] [<a href="https://www.sigsac.org/ccs/CCS2021/accepted-papers.html">Accepted</a>] </th>
  </tr>
</thead>
<tbody>
  <tr>
    <td class="tg-yla0" rowspan="4">Attacks on Robustness</td>
    <td class="tg-cly1">Black-box Adversarial Attacks on Commercial Speech Platforms with Minimal Information</td>
    <td class="tg-cly1"><a href="https://dl.acm.org/doi/10.1145/3460120.3485383">Link</a><br><a href="https://arxiv.org/pdf/2110.09714">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">A Hard Label Black-box Adversarial Attack Against Graph Neural Networks</td>
    <td class="tg-cly1"><a href="https://dl.acm.org/doi/10.1145/3460120.3484796">Link</a><br><a href="https://arxiv.org/pdf/2108.09513">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Robust Adversarial Attacks Against DNN-Based Wireless Communication Systems</td>
    <td class="tg-cly1"><a href="https://dl.acm.org/doi/10.1145/3460120.3484777">Link</a><br><a href="https://arxiv.org/pdf/2102.00918">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">AI-Lancet: Locating Error-inducing Neurons to Optimize Neural Networks</td>
    <td class="tg-cly1"><a href="https://dl.acm.org/doi/10.1145/3460120.3484818">Link</a><br><a href="https://dl.acm.org/doi/pdf/10.1145/3460120.3484818">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-yla0" rowspan="4">Defenses for ML Robustness</td>
    <td class="tg-cly1">Learning Security Classifiers with Verified Global Robustness Properties</td>
    <td class="tg-cly1"><a href="https://dl.acm.org/doi/10.1145/3460120.3484776">Link</a><br><a href="https://arxiv.org/pdf/2105.11363">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">On the Robustness of Domain Constraints</td>
    <td class="tg-cly1"><a href="https://dl.acm.org/doi/10.1145/3460120.3484570">Link</a><br><a href="https://arxiv.org/pdf/2105.08619">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Cert-RNN: Towards Certifying the Robustness of Recurrent Neural Networks</td>
    <td class="tg-cly1"><a href="https://dl.acm.org/doi/10.1145/3460120.3484538">Link</a><br><a href="https://nesa.zju.edu.cn/download/dty_pdf_cert_rnn.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">TSS: Transformation-Specific Smoothing for Robustness Certification</td>
    <td class="tg-cly1"><a href="https://dl.acm.org/doi/10.1145/3460120.3485258">Link</a><br><a href="https://arxiv.org/pdf/2002.12398">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-yla0" rowspan="5">Inference Attacks</td>
    <td class="tg-cly1">Honest-but-Curious Nets: Sensitive Attributes of Private Inputs Can Be Secretly Coded into the Classifiers' Outputs</td>
    <td class="tg-cly1"><a href="https://dl.acm.org/doi/10.1145/3460120.3484533">Link</a><br><a href="https://arxiv.org/pdf/2105.12049">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Quantifying and Mitigating Privacy Risks of Contrastive Learning</td>
    <td class="tg-cly1"><a href="https://dl.acm.org/doi/10.1145/3460120.3484571">Link</a><br><a href="https://arxiv.org/pdf/2102.04140">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Membership Inference Attacks Against Recommender Systems</td>
    <td class="tg-cly1"><a href="https://dl.acm.org/doi/10.1145/3460120.3484770">Link</a><br><a href="https://arxiv.org/pdf/2109.08045">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Membership Leakage in Label-Only Exposures</td>
    <td class="tg-cly1"><a href="https://dl.acm.org/doi/10.1145/3460120.3484575">Link</a><br><a href="https://arxiv.org/pdf/2007.15528">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">When Machine Unlearning Jeopardizes Privacy</td>
    <td class="tg-cly1"><a href="https://dl.acm.org/doi/10.1145/3460120.3484756">Link</a><br><a href="https://arxiv.org/pdf/2005.02205">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-yla0" rowspan="5">Privacy Attacks &amp; Defenses for ML</td>
    <td class="tg-cly1">EncoderMI: Membership Inference against Pre-trained Encoders in Contrastive Learning</td>
    <td class="tg-cly1"><a href="https://dl.acm.org/doi/10.1145/3460120.3484749">Link</a><br><a href="https://arxiv.org/pdf/2108.11023">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">TableGAN-MCA: Evaluating Membership Collisions of GAN-Synthesized Tabular Data Releasing</td>
    <td class="tg-cly1"><a href="https://dl.acm.org/doi/10.1145/3460120.3485251">Link</a><br><a href="https://arxiv.org/pdf/2107.13190">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Unleashing the Tiger: Inference Attacks on Split Learning</td>
    <td class="tg-cly1"><a href="https://dl.acm.org/doi/10.1145/3460120.3485259">Link</a><br><a href="https://arxiv.org/pdf/2012.02670">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Locally Private Graph Neural Networks</td>
    <td class="tg-cly1"><a href="https://dl.acm.org/doi/10.1145/3460120.3484565">Link</a><br><a href="https://arxiv.org/pdf/2006.05535">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">DataLens: Scalable Privacy Preserving Training via Gradient Compression and Aggregation</td>
    <td class="tg-cly1"><a href="https://dl.acm.org/doi/10.1145/3460120.3484579">Link</a><br><a href="https://arxiv.org/pdf/2103.11109">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-yla0" rowspan="4">Privacy for Distributed Data &amp; FL</td>
    <td class="tg-cly1">LEAP: Leakage-Abuse Attack on Efficiently Deployable, Efficiently Searchable Encryption with Partially Known Dataset</td>
    <td class="tg-cly1"><a href="https://dl.acm.org/doi/10.1145/3460120.3484540">Link</a><br><a href="https://ink.library.smu.edu.sg/cgi/viewcontent.cgi?article=7743&context=sis_research">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">On the Renyi Differential Privacy of the Shuffle Model</td>
    <td class="tg-cly1"><a href="https://dl.acm.org/doi/10.1145/3460120.3484794">Link</a><br><a href="https://arxiv.org/pdf/2105.05180">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Private Hierarchical Clustering in Federated Networks</td>
    <td class="tg-cly1"><a href="https://dl.acm.org/doi/10.1145/3460120.3484822">Link</a><br><a href="https://arxiv.org/pdf/2105.09057">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Secure Multi-party Computation of Differentially Private Heavy Hitters</td>
    <td class="tg-cly1"><a href="https://dl.acm.org/doi/10.1145/3460120.3484557">Link</a><br><a href="https://dl.acm.org/doi/pdf/10.1145/3460120.3484557">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-yla0" rowspan="5">Differential Privacy</td>
    <td class="tg-cly1">Differential Privacy for Directional Data</td>
    <td class="tg-cly1"><a href="https://dl.acm.org/doi/10.1145/3460120.3484734">Link</a><br><a href="https://dl.acm.org/doi/pdf/10.1145/3460120.3484734">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Differentially Private Sparse Vectors with Low Error, Optimal Space, and Fast Access</td>
    <td class="tg-cly1"><a href="https://dl.acm.org/doi/10.1145/3460120.3484735">Link</a><br><a href="https://arxiv.org/pdf/2106.10068">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Continuous Release of Data Streams under both Centralized and Local Differential Privacy</td>
    <td class="tg-cly1"><a href="https://dl.acm.org/doi/10.1145/3460120.3484750">Link</a><br><a href="https://dl.acm.org/doi/pdf/10.1145/3460120.3484750">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Side-Channel Attacks on Query-Based Data Anonymization</td>
    <td class="tg-cly1"><a href="https://dl.acm.org/doi/10.1145/3460120.3484751">Link</a><br><a href="https://people.mpi-sws.org/~francis/side-channel.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">AHEAD: Adaptive Hierarchical Decomposition for Range Query under Local Differential Privacy</td>
    <td class="tg-cly1"><a href="https://dl.acm.org/doi/10.1145/3460120.3485668">Link</a><br><a href="https://arxiv.org/pdf/2110.07505">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-yla0" rowspan="3">Other Privacy</td>
    <td class="tg-cly1">Reconstructing with Less: Leakage Abuse Attacks in Two Dimensions</td>
    <td class="tg-cly1"><a href="https://dl.acm.org/doi/10.1145/3460120.3484552">Link</a><br><a href="https://eprint.iacr.org/2020/1531.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Epsolute: Efficiently Querying Databases While Providing Differential Privacy</td>
    <td class="tg-cly1"><a href="https://dl.acm.org/doi/10.1145/3460120.3484786">Link</a><br><a href="https://dl.acm.org/doi/pdf/10.1145/3460120.3484786">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">The Invisible Shadow: How Security Cameras Leak Private Activities</td>
    <td class="tg-cly1"><a href="https://dl.acm.org/doi/10.1145/3460120.3484741">Link</a><br><a href="http://xyzhang.ucsd.edu/papers/Jian.Gong_CCS21_InvisibleShadow.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-yla0" rowspan="5">Data Poisoning &amp; Backdoor Attacks</td>
    <td class="tg-cly1">Subpopulation Data Poisoning Attacks</td>
    <td class="tg-cly1"><a href="https://dl.acm.org/doi/10.1145/3460120.3485368">Link</a><br><a href="https://arxiv.org/pdf/2006.14026">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Hidden Backdoors in Human-Centric Language Models</td>
    <td class="tg-cly1"><a href="https://dl.acm.org/doi/10.1145/3460120.3484576">Link</a><br><a href="https://arxiv.org/pdf/2105.00164">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Backdoor Pre-trained Models Can Transfer to All</td>
    <td class="tg-cly1"><a href="https://dl.acm.org/doi/10.1145/3460120.3485370">Link</a><br><a href="https://arxiv.org/pdf/2111.00197">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Feature-Indistinguishable Attack to Circumvent Trapdoor-Enabled Defense</td>
    <td class="tg-cly1"><a href="https://dl.acm.org/doi/10.1145/3460120.3485378">Link</a><br><a href="https://www.microsoft.com/en-us/research/uploads/prod/2022/07/Feature_Indistinguishable_Attack_to_Circumvent_Trapdoor_enabled_Defense.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">DetectorGuard: Provably Securing Object Detectors against Localized Patch Hiding Attacks</td>
    <td class="tg-cly1"><a href="https://dl.acm.org/doi/10.1145/3460120.3484757">Link</a><br><a href="https://arxiv.org/pdf/2102.02956">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-yla0" rowspan="5">Applications &amp; Privacy of ML</td>
    <td class="tg-cly1">DeepAID: Interpreting and Improving Deep Learning-based Anomaly Detection in Security Applications</td>
    <td class="tg-cly1"><a href="https://dl.acm.org/doi/10.1145/3460120.3484589">Link</a><br><a href="https://arxiv.org/pdf/2109.11495">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Structural Attack against Graph Based Android Malware Detection</td>
    <td class="tg-cly1"><a href="https://dl.acm.org/doi/10.1145/3460120.3485387">Link</a><br><a href="https://www4.comp.polyu.edu.hk/~csxluo/HRAT.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">PalmTree: Learning an Assembly Language Model for Instruction Embedding</td>
    <td class="tg-cly1"><a href="https://dl.acm.org/doi/10.1145/3460120.3484587">Link</a><br><a href="https://arxiv.org/pdf/2103.03809">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">A One-Pass Distributed and Private Sketch for Kernel Sums with Applications to Machine Learning at Scale</td>
    <td class="tg-cly1"><a href="https://dl.acm.org/doi/10.1145/3460120.3485255">Link</a><br><a href="https://dl.acm.org/doi/pdf/10.1145/3460120.3485255">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">COINN: Crypto/ML Codesign for Oblivious Inference via Neural Networks</td>
    <td class="tg-cly1"><a href="https://dl.acm.org/doi/10.1145/3460120.3484797">Link</a><br><a href="https://dl.acm.org/doi/pdf/10.1145/3460120.3484797">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-yla0" rowspan="4">Audio Systems &amp; Autonomous Driving</td>
    <td class="tg-cly1">FakeWake: Understanding and Mitigating Fake Wake-up Words of Voice Assistants</td>
    <td class="tg-cly1"><a href="https://dl.acm.org/doi/10.1145/3460120.3485365">Link</a><br><a href="https://arxiv.org/pdf/2109.09958">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Robust Detection of Machine-induced Audio Attacks in Intelligent Audio Systems with Microphone Array</td>
    <td class="tg-cly1"><a href="https://dl.acm.org/doi/10.1145/3460120.3484755">Link</a><br><a href="https://dl.acm.org/doi/pdf/10.1145/3460120.3484755">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">I Can See the Light: Attacks on Autonomous Vehicles Using Invisible Lights</td>
    <td class="tg-cly1"><a href="https://dl.acm.org/doi/10.1145/3460120.3484766">Link</a><br><a href="https://dl.acm.org/doi/pdf/10.1145/3460120.3484766">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Can We Use Arbitrary Objects to Attack LiDAR Perception in Autonomous Driving?</td>
    <td class="tg-cly1"><a href="https://dl.acm.org/doi/10.1145/3460120.3485377">Link</a><br><a href="https://www.acsu.buffalo.edu/~yzhu39/Yi_Zhu_homepage_files/papers/CCS21_434.pdf">PDF</a></td>
  </tr>
</tbody>
</table>

<br/>

#### SEC 2021

<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  overflow:hidden;padding:10px 5px;word-break:normal;}
.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-cly1{text-align:left;vertical-align:middle}
.tg .tg-axmw{font-size:18px;font-weight:bold;text-align:center;vertical-align:middle}
.tg .tg-yla0{font-weight:bold;text-align:left;vertical-align:middle}
</style>
<table class="tg">
<thead>
  <tr>
    <th class="tg-axmw" colspan="3" align="center">USENIX Security 2021 &nbsp; [<a href="https://www.usenix.org/conference/usenixsecurity21">Homepage</a>] [<a href="https://www.usenix.org/conference/usenixsecurity21/technical-sessions">Accepted</a>] </th>
  </tr>
</thead>
<tbody>
  <tr>
    <td class="tg-yla0" rowspan="7">Backdoor &amp; Poisoning</td>
    <td class="tg-cly1">Explanation-Guided Backdoor Poisoning Attacks Against Malware Classifiers</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity21/presentation/severi">Link</a><br><a href="https://www.usenix.org/system/files/sec21fall-severi.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Blind Backdoors in Deep Learning Models</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity21/presentation/bagdasaryan">Link</a><br><a href="https://www.usenix.org/system/files/sec21-bagdasaryan.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Graph Backdoor</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity21/presentation/xi">Link</a><br><a href="https://www.usenix.org/system/files/sec21fall-xi.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Demon in the Variant: Statistical Analysis of DNNs for Robust Backdoor Contamination Detection</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity21/presentation/tang-di">Link</a><br><a href="https://www.usenix.org/system/files/sec21summer_tang-di.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">You Autocomplete Me: Poisoning Vulnerabilities in Neural Code Completion</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity21/presentation/schuster">Link</a><br><a href="https://www.usenix.org/system/files/sec21summer_schuster.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Poisoning the Unlabeled Dataset of Semi-Supervised Learning</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity21/presentation/carlini-poisoning">Link</a><br><a href="https://www.usenix.org/system/files/sec21-carlini-poisoning.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Double-Cross Attacks: Subverting Active Learning Systems</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity21/presentation/vicarte">Link</a><br><a href="https://www.usenix.org/system/files/sec21-vicarte.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-yla0" rowspan="7">Adversarial Examples &amp; Model Extraction</td>
    <td class="tg-cly1">SLAP: Improving Physical Adversarial Examples with Short-Lived Adversarial Perturbations</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity21/presentation/lovisotto">Link</a><br><a href="https://www.usenix.org/system/files/sec21fall-lovisotto.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Adversarial Policy Training against Deep Reinforcement Learning</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity21/presentation/wu-xian">Link</a><br><a href="https://www.usenix.org/system/files/sec21summer_wu-xian.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">DRMI: A Dataset Reduction Technology based on Mutual Information for Black-box Attacks</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity21/presentation/he-yingzhe">Link</a><br><a href="https://www.usenix.org/system/files/sec21fall-he-yingzhe.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Deep-Dup: An Adversarial Weight Duplication Attack Framework to Crush Deep Neural Network in Multi-Tenant FPGA</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity21/presentation/rakin">Link</a><br><a href="https://www.usenix.org/system/files/sec21-rakin.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Entangled Watermarks as a Defense against Model Extraction</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity21/presentation/jia">Link</a><br><a href="https://www.usenix.org/system/files/sec21fall-jia.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Mind Your Weight(s): A Large-scale Study on Insufficient Machine Learning Model Protection in Mobile Apps</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity21/presentation/sun-zhichuang">Link</a><br><a href="https://www.usenix.org/system/files/sec21fall-sun-zhichuang.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Hermes Attack: Steal DNN Models with Lossless Inference Accuracy</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity21/presentation/zhu">Link</a><br><a href="https://www.usenix.org/system/files/sec21summer_zhu.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-yla0" rowspan="7">Adversarial ML Defenses</td>
    <td class="tg-cly1">PatchGuard: A Provably Robust Defense against Adversarial Patches via Small Receptive Fields and Masking</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity21/presentation/xiang">Link</a><br><a href="https://www.usenix.org/system/files/sec21fall-xiang.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">T-Miner: A Generative Approach to Defend Against Trojan Attacks on DNN-based Text Classification</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity21/presentation/azizi">Link</a><br><a href="https://www.usenix.org/system/files/sec21fall-azizi.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">WaveGuard: Understanding and Mitigating Audio Adversarial Examples</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity21/presentation/hussain">Link</a><br><a href="https://www.usenix.org/system/files/sec21fall-hussain.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Cost-Aware Robust Tree Ensembles for Security Applications</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity21/presentation/chen-yizheng">Link</a><br><a href="https://www.usenix.org/system/files/sec21fall-chen-yizheng.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Dompteur: Taming Audio Adversarial Examples</td>
    <td class="tg-cly1"><a href="https://usenix.org/conference/usenixsecurity21/presentation/eisenhofer">Link</a><br><a href="https://www.usenix.org/system/files/sec21-eisenhofer.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">CADE: Detecting and Explaining Concept Drift Samples for Security Applications</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity21/presentation/yang-limin">Link</a><br><a href="https://www.usenix.org/system/files/sec21summer_yang.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">SIGL: Securing Software Installations Through Deep Graph Learning</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity21/presentation/han-xueyuan">Link</a><br><a href="https://www.usenix.org/system/files/sec21summer_han-xueyuan.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-yla0" rowspan="7">ML Privacy Issues</td>
    <td class="tg-cly1">Systematic Evaluation of Privacy Risks of Machine Learning Models</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity21/presentation/song">Link</a><br><a href="https://www.usenix.org/system/files/sec21fall-song.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Extracting Training Data from Large Language Models</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity21/presentation/carlini-extracting">Link</a><br><a href="https://www.usenix.org/system/files/sec21-carlini-extracting.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">SWIFT: Super-fast and Robust Privacy-Preserving Machine Learning</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity21/presentation/koti">Link</a><br><a href="https://www.usenix.org/system/files/sec21fall-koti.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Stealing Links from Graph Neural Networks</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity21/presentation/he-xinlei">Link</a><br><a href="https://www.usenix.org/system/files/sec21summer_he.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Leakage of Dataset Properties in Multi-Party Machine Learning</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity21/presentation/zhang-wanrong">Link</a><br><a href="https://www.usenix.org/system/files/sec21-zhang-wanrong.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Defeating DNN-Based Traffic Analysis Systems in Real-Time With Blind Adversarial Perturbations</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity21/presentation/nasr">Link</a><br><a href="https://www.usenix.org/system/files/sec21fall-nasr.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Cerebro: A Platform for Multi-Party Cryptographic Collaborative Learning</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity21/presentation/zheng">Link</a><br><a href="https://www.usenix.org/system/files/sec21fall-zheng.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-yla0" rowspan="4">Differential Privacy</td>
    <td class="tg-cly1">PrivSyn: Differentially Private Data Synthesis</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity21/presentation/zhang-zhikun">Link</a><br><a href="https://www.usenix.org/system/files/sec21fall-zhang-zhikun.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Data Poisoning Attacks to Local Differential Privacy Protocols</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity21/presentation/cao-xiaoyu">Link</a><br><a href="https://www.usenix.org/system/files/sec21fall-cao.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">How to Make Private Distributed Cardinality Estimation Practical, and Get Differential Privacy for Free</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity21/presentation/hu-changhui">Link</a><br><a href="https://www.usenix.org/system/files/sec21fall-hu-changhui.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Locally Differentially Private Analysis of Graph Statistics</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity21/presentation/imola">Link</a><br><a href="https://www.usenix.org/system/files/sec21fall-imola.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-yla0" rowspan="3">Secure Multiparty Computation</td>
    <td class="tg-cly1">GForce: GPU-Friendly Oblivious and Rapid Neural Network Inference</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity21/presentation/ng">Link</a><br><a href="https://www.usenix.org/system/files/sec21fall-ng.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Fantastic Four: Honest-Majority Four-Party Secure Computation With Malicious Security</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity21/presentation/dalskov">Link</a><br><a href="https://www.usenix.org/system/files/sec21fall-dalskov.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Muse: Secure Inference Resilient to Malicious Clients</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity21/presentation/lehmkuhl">Link</a><br><a href="https://www.usenix.org/system/files/sec21fall-lehmkuhl.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-yla0" rowspan="4">Other Attacks</td>
    <td class="tg-cly1">Too Good to Be Safe: Tricking Lane Detection in Autonomous Driving with Crafted Perturbations</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity21/presentation/jing">Link</a><br><a href="https://www.usenix.org/system/files/sec21fall-jing.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Research on the Security of Visual Reasoning CAPTCHA</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity21/presentation/gao">Link</a><br><a href="https://www.usenix.org/system/files/sec21fall-gao.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Dirty Road Can Attack: Security of Deep Learning based Automated Lane Centering under Physical-World Attack</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity21/presentation/sato">Link</a><br><a href="https://www.usenix.org/system/files/sec21-sato.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">mID: Tracing Screen Photos via Moiré Patterns</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity21/presentation/cheng-yushi">Link</a><br><a href="https://www.usenix.org/system/files/sec21fall-cheng-yushi.pdf">PDF</a></td>
  </tr>
</tbody>
</table>

<br/>

#### NDSS 2021

<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  overflow:hidden;padding:10px 5px;word-break:normal;}
.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-cly1{text-align:left;vertical-align:middle}
.tg .tg-axmw{font-size:18px;font-weight:bold;text-align:center;vertical-align:middle}
.tg .tg-yla0{font-weight:bold;text-align:left;vertical-align:middle}
</style>
<table class="tg">
<thead>
  <tr>
    <th class="tg-axmw" colspan="3" align="center">NDSS 2021 &nbsp;  [<a href="https://www.ndss-symposium.org/ndss2021/">Homepage</a>] [<a href="https://www.ndss-symposium.org/ndss2021/accepted-papers/">Accepted</a>] </th>
  </tr>
</thead>
<tbody>
  <tr>
    <td class="tg-yla0" rowspan="4">Machine Learning</td>
    <td class="tg-cly1">Let’s Stride Blindfolded in a Forest: Sublinear Multi-Client Decision Trees Evaluation</td>
    <td class="tg-cly1"><a href="https://www.ndss-symposium.org/ndss-paper/lets-stride-blindfolded-in-a-forest-sublinear-multi-client-decision-trees-evaluation/">Link</a><br><a href="https://www.ndss-symposium.org/wp-content/uploads/ndss2021_5C-1_23166_paper.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Practical Blind Membership Inference Attack via Differential Comparisons</td>
    <td class="tg-cly1"><a href="https://www.ndss-symposium.org/ndss-paper/practical-blind-membership-inference-attack-via-differential-comparisons/">Link</a><br><a href="https://www.ndss-symposium.org/wp-content/uploads/ndss2021_5C-2_24293_paper.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">GALA: Greedy ComputAtion for Linear Algebra in Privacy-Preserved Neural Networks</td>
    <td class="tg-cly1"><a href="https://www.ndss-symposium.org/ndss-paper/gala-greedy-computation-for-linear-algebra-in-privacy-preserved-neural-networks/">Link</a><br><a href="https://www.ndss-symposium.org/wp-content/uploads/ndss2021_5C-3_24351_paper.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">FARE: Enabling Fine-grained Attack Categorization under Low-quality Labeled Data</td>
    <td class="tg-cly1"><a href="https://www.ndss-symposium.org/ndss-paper/fare-enabling-fine-grained-attack-categorization-under-low-quality-labeled-data/">Link</a><br><a href="https://www.ndss-symposium.org/wp-content/uploads/ndss2021_5C-4_24403_paper.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-yla0" rowspan="4">FL &amp; Poisoning Attacks</td>
    <td class="tg-cly1">POSEIDON: Privacy-Preserving Federated Neural Network Learning</td>
    <td class="tg-cly1"><a href="https://www.ndss-symposium.org/ndss-paper/poseidon-privacy-preserving-federated-neural-network-learning/">Link</a><br><a href="https://www.ndss-symposium.org/wp-content/uploads/ndss2021_6C-1_24119_paper.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">FLTrust: Byzantine-robust Federated Learning via Trust Bootstrapping</td>
    <td class="tg-cly1"><a href="https://www.ndss-symposium.org/ndss-paper/fltrust-byzantine-robust-federated-learning-via-trust-bootstrapping/">Link</a><br><a href="https://www.ndss-symposium.org/wp-content/uploads/ndss2021_6C-2_24434_paper.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Manipulating the Byzantine: Optimizing Model Poisoning Attacks and Defenses for Federated Learning</td>
    <td class="tg-cly1"><a href="https://www.ndss-symposium.org/ndss-paper/manipulating-the-byzantine-optimizing-model-poisoning-attacks-and-defenses-for-federated-learning/">Link</a><br><a href="https://www.ndss-symposium.org/wp-content/uploads/ndss2021_6C-3_24498_paper.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Data Poisoning Attacks to Deep Learning Based Recommender Systems</td>
    <td class="tg-cly1"><a href="https://www.ndss-symposium.org/ndss-paper/data-poisoning-attacks-to-deep-learning-based-recommender-systems/">Link</a><br><a href="https://www.ndss-symposium.org/wp-content/uploads/ndss2021_6C-4_24525_paper.pdf">PDF</a></td>
  </tr>
</tbody>
</table>

<p width="100%" align="right"><a href="#">🔝</a></p>

<br/>

### 2020

#### S&P 2020

<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  overflow:hidden;padding:10px 5px;word-break:normal;}
.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-cly1{text-align:left;vertical-align:middle}
.tg .tg-axmw{font-size:18px;font-weight:bold;text-align:center;vertical-align:middle}
.tg .tg-yla0{font-weight:bold;text-align:left;vertical-align:middle}
</style>
<table class="tg">
<thead>
  <tr>
    <th class="tg-axmw" colspan="3" align="center">S&amp;P 2020 &nbsp; [<a href="https://www.ieee-security.org/TC/SP2020/index.html">Homepage</a>] [<a href="https://www.ieee-security.org/TC/SP2020/program-papers.html">Accepted</a>] </th>
  </tr>
</thead>
<tbody>
  <tr>
    <td class="tg-yla0" rowspan="3">ML &amp; Privacy</td>
    <td class="tg-cly1">The Value of Collaboration in Convex Machine Learning with Differential Privacy</td>
    <td class="tg-cly1"><a href="https://ieeexplore.ieee.org/document/9152691/">Link</a><br><a href="https://arxiv.org/pdf/1906.09679">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Automatically Detecting Bystanders in Photos to Reduce Privacy Risks</td>
    <td class="tg-cly1"><a href="https://ieeexplore.ieee.org/document/9152778/">Link</a><br><a href="https://rakib062.github.io/files/bystander-oakland-2020.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">CrypTFlow: Secure TensorFlow Inference</td>
    <td class="tg-cly1"><a href="https://ieeexplore.ieee.org/document/9152660/">Link</a><br><a href="https://arxiv.org/pdf/1909.07814">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-yla0" rowspan="4">Adversarial ML</td>
    <td class="tg-cly1">HopSkipJumpAttack: A Query-Efficient Decision-Based Attack</td>
    <td class="tg-cly1"><a href="https://ieeexplore.ieee.org/document/9152788/">Link</a><br><a href="https://arxiv.org/pdf/1904.02144">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Humpty Dumpty: Controlling Word Meanings via Corpus Poisoning</td>
    <td class="tg-cly1"><a href="https://ieeexplore.ieee.org/document/9152608/">Link</a><br><a href="https://arxiv.org/pdf/2001.04935">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Privacy Risks of General-Purpose Language Models</td>
    <td class="tg-cly1"><a href="https://ieeexplore.ieee.org/document/9152761/">Link</a><br><a href="https://nesa.zju.edu.cn/download/Privacy%20Risks%20of%20General-Purpose%20Language%20Models.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Intriguing Properties of Adversarial ML Attacks in the Problem Space</td>
    <td class="tg-cly1"><a href="https://ieeexplore.ieee.org/document/9152781/">Link</a><br><a href="https://arxiv.org/pdf/1911.02142">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-yla0" rowspan="4">Differential Privacy</td>
    <td class="tg-cly1">SoK: Differential Privacy as a Causal Property</td>
    <td class="tg-cly1"><a href="https://ieeexplore.ieee.org/document/9152780/">Link</a><br><a href="https://arxiv.org/pdf/1710.05899">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Private Resource Allocators and Their Applications</td>
    <td class="tg-cly1"><a href="https://ieeexplore.ieee.org/document/9152764/">Link</a><br><a href="https://eprint.iacr.org/2020/287.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Towards Effective Differential Privacy Communication for Users' Data Sharing Decision and Comprehension</td>
    <td class="tg-cly1"><a href="https://ieeexplore.ieee.org/document/9152658/">Link</a><br><a href="https://arxiv.org/pdf/2003.13922">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">A Programming Framework for Differential Privacy with Accuracy Concentration Bounds</td>
    <td class="tg-cly1"><a href="https://ieeexplore.ieee.org/document/9152641/">Link</a><br><a href="https://arxiv.org/pdf/1909.07918">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-yla0">Attacks &amp; Forensics</td>
    <td class="tg-cly1">Throwing Darts in the Dark? Detecting Bots with Limited Data using Neural Data Augmentation</td>
    <td class="tg-cly1"><a href="https://ieeexplore.ieee.org/document/9152805/">Link</a><br><a href="https://gangw.web.illinois.edu/sp20-odds.pdf">PDF</a></td>
  </tr>
</tbody>
</table>

<br/>

#### CCS 2020

<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  overflow:hidden;padding:10px 5px;word-break:normal;}
.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-cly1{text-align:left;vertical-align:middle}
.tg .tg-axmw{font-size:18px;font-weight:bold;text-align:center;vertical-align:middle}
.tg .tg-yla0{font-weight:bold;text-align:left;vertical-align:middle}
</style>
<table class="tg">
<thead>
  <tr>
    <th class="tg-axmw" colspan="3" align="center">CCS 2020 &nbsp; [<a href="https://www.sigsac.org/ccs/CCS2020/index.html">Homepage</a>] [<a href="https://www.sigsac.org/ccs/CCS2020/accepted-papers.html">Accepted</a>] </th>
  </tr>
</thead>
<tbody>
  <tr>
    <td class="tg-yla0" rowspan="4">Attacking &amp; Defending ML</td>
    <td class="tg-cly1">Gotta Catch'Em All: Using Honeypots to Catch Adversarial Attacks on Neural Networks</td>
    <td class="tg-cly1"><a href="https://dl.acm.org/doi/10.1145/3372297.3417231">Link</a><br><a href="https://dl.acm.org/doi/pdf/10.1145/3372297.3417231">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">A Tale of Evil Twins: Adversarial Inputs versus Poisoned Models</td>
    <td class="tg-cly1"><a href="https://dl.acm.org/doi/10.1145/3372297.3417253">Link</a><br><a href="https://arxiv.org/pdf/1911.01559">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">DeepDyve: Dynamic Verification for Deep Neural Networks</td>
    <td class="tg-cly1"><a href="https://dl.acm.org/doi/10.1145/3372297.3423338">Link</a><br><a href="https://arxiv.org/pdf/2009.09663">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Composite Backdoor Attack for Deep Neural Network by Mixing Existing Benign Features</td>
    <td class="tg-cly1"><a href="https://dl.acm.org/doi/10.1145/3372297.3423362">Link</a><br><a href="https://dl.acm.org/doi/pdf/10.1145/3372297.3423362">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-yla0" rowspan="4">ML &amp; Information Leakage</td>
    <td class="tg-cly1">CrypTFlow2: Practical 2-Party Secure Inference</td>
    <td class="tg-cly1"><a href="https://dl.acm.org/doi/10.1145/3372297.3417274">Link</a><br><a href="https://arxiv.org/pdf/2010.06457">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">GAN-Leaks: A Taxonomy of Membership Inference Attacks against Generative Models</td>
    <td class="tg-cly1"><a href="https://dl.acm.org/doi/10.1145/3372297.3417238">Link</a><br><a href="https://arxiv.org/pdf/1909.03935">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Analyzing Information Leakage of Updates to Natural Language Models</td>
    <td class="tg-cly1"><a href="https://dl.acm.org/doi/10.1145/3372297.3417880">Link</a><br><a href="https://arxiv.org/pdf/1912.07942">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Information Leakage in Embedding Models</td>
    <td class="tg-cly1"><a href="https://dl.acm.org/doi/10.1145/3372297.3417270">Link</a><br><a href="https://arxiv.org/pdf/2004.00053">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-yla0" rowspan="4">Privacy</td>
    <td class="tg-cly1">Private Summation in the Multi-Message Shuffle Model</td>
    <td class="tg-cly1"><a href="https://dl.acm.org/doi/10.1145/3372297.3417242">Link</a><br><a href="https://arxiv.org/pdf/2002.00817">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">R^2DP: A Universal and Automated Approach to Optimizing the Randomization Mechanisms of Differential Privacy for Utility Metrics with No Known Optimal Distributions</td>
    <td class="tg-cly1"><a href="https://dl.acm.org/doi/10.1145/3372297.3417259">Link</a><br><a href="https://arxiv.org/pdf/2009.09451">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Estimating g-Leakage via Machine Learning</td>
    <td class="tg-cly1"><a href="https://dl.acm.org/doi/10.1145/3372297.3423363">Link</a><br><a href="https://arxiv.org/pdf/2005.04399">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Implementing the Exponential Mechanism with Base-2 Differential Privacy</td>
    <td class="tg-cly1"><a href="https://dl.acm.org/doi/10.1145/3372297.3417269">Link</a><br><a href="https://arxiv.org/pdf/1912.04222">PDF</a></td>
  </tr>
</tbody>
</table>

<br/>

#### SEC 2020

<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  overflow:hidden;padding:10px 5px;word-break:normal;}
.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-cly1{text-align:left;vertical-align:middle}
.tg .tg-axmw{font-size:18px;font-weight:bold;text-align:center;vertical-align:middle}
.tg .tg-yla0{font-weight:bold;text-align:left;vertical-align:middle}
</style>
<table class="tg">
<thead>
  <tr>
    <th class="tg-axmw" colspan="3" align="center">USENIX Security 2020 &nbsp; [<a href="https://www.usenix.org/conference/usenixsecurity20">Homepage</a>] [<a href="https://www.usenix.org/conference/usenixsecurity20/technical-sessions">Accepted</a>] </th>
  </tr>
</thead>
<tbody>
  <tr>
    <td class="tg-yla0" rowspan="11">Machine Learning</td>
    <td class="tg-cly1">Updates-Leak: Data Set Inference and Reconstruction Attacks in Online Learning</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity20/presentation/salem">Link</a><br><a href="https://www.usenix.org/system/files/sec20summer_salem_prepub.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Exploring Connections Between Active Learning and Model Extraction</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity20/presentation/chandrasekaran">Link</a><br><a href="https://www.usenix.org/system/files/sec20summer_chandrasekaran_prepub.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Hybrid Batch Attacks: Finding Black-box Adversarial Examples with Limited Queries</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity20/presentation/suya">Link</a><br><a href="https://www.usenix.org/system/files/sec20summer_suya_prepub.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">High Accuracy and High Fidelity Extraction of Neural Networks</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity20/presentation/jagielski">Link</a><br><a href="https://www.usenix.org/system/files/sec20fall_jagielski_prepub.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Adversarial Preprocessing: Understanding and Preventing Image-Scaling Attacks in Machine Learning</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity20/presentation/quiring">Link</a><br><a href="https://www.usenix.org/system/files/sec20fall_quiring_prepub.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">TextShield: Robust Text Classification Based on Multimodal Embedding and Neural Machine Translation</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity20/presentation/li-jinfeng">Link</a><br><a href="https://www.usenix.org/system/files/sec20fall_li-jinfeng_prepub.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Fawkes: Protecting Privacy against Unauthorized Deep Learning Models</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity20/presentation/shan">Link</a><br><a href="https://www.usenix.org/system/files/sec20-shan.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Stolen Memories: Leveraging Model Memorization for Calibrated White-Box Membership Inference</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity20/presentation/leino">Link</a><br><a href="https://www.usenix.org/system/files/sec20-leino.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Local Model Poisoning Attacks to Byzantine-Robust Federated Learning</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity20/presentation/fang">Link</a><br><a href="https://www.usenix.org/system/files/sec20summer_fang_prepub.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Justinian's GAAvernor: Robust Distributed Learning with Gradient Aggregation Agent</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity20/presentation/pan">Link</a><br><a href="https://www.usenix.org/system/files/sec20-pan.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Interpretable Deep Learning under Fire</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity20/presentation/zhang-xinyang">Link</a><br><a href="https://www.usenix.org/system/files/sec20spring_zhang_prepub.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-yla0" rowspan="6">Privacy Enhancement</td>
    <td class="tg-cly1">PCKV: Locally Differentially Private Correlated Key-Value Data Collection with Optimized Utility</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity20/presentation/gu">Link</a><br><a href="https://www.usenix.org/system/files/sec20summer_gu_prepub.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Actions Speak Louder than Words: Entity-Sensitive Privacy Policy and Data Flow Analysis with PoliCheck</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity20/presentation/andow">Link</a><br><a href="https://www.usenix.org/system/files/sec20summer_andow_prepub.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Walking Onions: Scaling Anonymity Networks while Protecting Users</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity20/presentation/komlo">Link</a><br><a href="https://www.usenix.org/system/files/sec20-komlo.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Differentially-Private Control-Flow Node Coverage for Software Usage Analysis</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity20/presentation/zhang-hailong">Link</a><br><a href="https://par.nsf.gov/servlets/purl/10191952">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Visor: Privacy-Preserving Video Analytics as a Cloud Service</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity20/presentation/poddar">Link</a><br><a href="https://www.usenix.org/system/files/sec20-poddar.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">DELF: Safeguarding deletion correctness in Online Social Networks</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity20/presentation/cohn-gordon">Link</a><br><a href="https://www.usenix.org/system/files/sec20-cohn-gordon.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-yla0" rowspan="2">Private Computing</td>
    <td class="tg-cly1">Secure Multi-party Computation of Differentially Private Median</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity20/presentation/boehler">Link</a><br><a href="https://www.usenix.org/system/files/sec20-bohler.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Delphi: A Cryptographic Inference Service for Neural Networks</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity20/presentation/mishra">Link</a><br><a href="https://www.usenix.org/system/files/sec20spring_mishra_prepub.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-yla0">Speech Attack</td>
    <td class="tg-cly1">Devil’s Whisper: A General Approach for Physical Adversarial Attacks against Commercial Black-box Speech Recognition Devices</td>
    <td class="tg-cly1"><a href="https://www.usenix.org/conference/usenixsecurity20/presentation/chen-yuxuan">Link</a><br><a href="https://www.usenix.org/system/files/sec20summer_chen-yuxuan_prepub.pdf">PDF</a></td>
  </tr>
</tbody>
</table>

<br/>

#### NDSS 2020

<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  overflow:hidden;padding:10px 5px;word-break:normal;}
.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-cly1{text-align:left;vertical-align:middle}
.tg .tg-axmw{font-size:18px;font-weight:bold;text-align:center;vertical-align:middle}
.tg .tg-yla0{font-weight:bold;text-align:left;vertical-align:middle}
</style>
<table class="tg">
<thead>
  <tr>
    <th class="tg-axmw" colspan="3" align="center">NDSS 2020 &nbsp; [<a href="https://www.ndss-symposium.org/ndss2020/">Homepage</a>] [<a href="https://www.ndss-symposium.org/ndss2020/accepted-papers/">Accepted</a>] </th>
  </tr>
</thead>
<tbody>
  <tr>
    <td class="tg-yla0" rowspan="4">Private <br>Computation &amp; Learning</td>
    <td class="tg-cly1">Trident: Efficient 4PC Framework for Privacy Preserving Machine Learning</td>
    <td class="tg-cly1"><a href="https://www.ndss-symposium.org/ndss-paper/trident-efficient-4pc-framework-for-privacy-preserving-machine-learning/">Link</a><br><a href="https://www.ndss-symposium.org/wp-content/uploads/2020/02/23005-paper.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Secure Sublinear Time Differentially Private Median Computation</td>
    <td class="tg-cly1"><a href="https://www.ndss-symposium.org/ndss-paper/secure-sublinear-time-differentially-private-median-computation/">Link</a><br><a href="https://www.ndss-symposium.org/wp-content/uploads/2020/02/24150-paper.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">CloudLeak: Large-Scale Deep Learning Models Stealing Through Adversarial Examples</td>
    <td class="tg-cly1"><a href="https://www.ndss-symposium.org/ndss-paper/cloudleak-large-scale-deep-learning-models-stealing-through-adversarial-examples/">Link</a><br><a href="https://www.ndss-symposium.org/wp-content/uploads/2020/02/24178-paper.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">BLAZE: Blazing Fast Privacy-Preserving Machine Learning</td>
    <td class="tg-cly1"><a href="https://www.ndss-symposium.org/ndss-paper/blaze-blazing-fast-privacy-preserving-machine-learning/">Link</a><br><a href="https://www.ndss-symposium.org/wp-content/uploads/2020/02/24202-paper.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-yla0" rowspan="4">Privacy</td>
    <td class="tg-cly1">Towards Plausible Graph Anonymization</td>
    <td class="tg-cly1"><a href="https://www.ndss-symposium.org/ndss-paper/towards-plausible-graph-anonymization/">Link</a><br><a href="https://www.ndss-symposium.org/wp-content/uploads/2020/02/23032-paper.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Adversarial Classification Under Differential Privacy</td>
    <td class="tg-cly1"><a href="https://www.ndss-symposium.org/ndss-paper/adversarial-classification-under-differential-privacy/">Link</a><br><a href="https://www.ndss-symposium.org/wp-content/uploads/2020/02/23047-paper.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Locally Differentially Private Frequency Estimation with Consistency</td>
    <td class="tg-cly1"><a href="https://www.ndss-symposium.org/ndss-paper/locally-differentially-private-frequency-estimation-with-consistency/">Link</a><br><a href="https://www.ndss-symposium.org/wp-content/uploads/2020/02/24157-paper.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">DESENSITIZATION: Privacy-Aware and Attack-Preserving Crash Report</td>
    <td class="tg-cly1"><a href="https://www.ndss-symposium.org/ndss-paper/desensitization-privacy-aware-and-attack-preserving-crash-report/">Link</a><br><a href="https://www.ndss-symposium.org/wp-content/uploads/2020/02/24428-paper.pdf">PDF</a></td>
  </tr>
</tbody>
</table>

<p width="100%" align="right"><a href="#">🔝</a></p>



<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  overflow:hidden;padding:10px 5px;word-break:normal;}
.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-cly1{text-align:left;vertical-align:middle}
.tg .tg-axmw{font-size:18px;font-weight:bold;text-align:center;vertical-align:middle}
.tg .tg-yla0{font-weight:bold;text-align:left;vertical-align:middle}
</style>
<table class="tg">
    <col width="17%" style='mso-width-source:userset;mso-width-alt:6848'>
	<col width="75%" style='mso-width-source:userset;mso-width-alt:26080'>
	<col width="18%" style='mso-width-source:userset;mso-width-alt:4032'>
<thead>
  <tr>
    <th class="tg-axmw" colspan="3" align="center">NDSS 2020 &nbsp; [<a href="https://www.ndss-symposium.org/ndss2020/">Homepage</a>] [<a href="https://www.ndss-symposium.org/ndss2020/accepted-papers/">Accepted</a>] </th>
  </tr>
</thead>
<tbody>
  <tr>
    <td class="tg-yla0" rowspan="4">Private <br>Computation &amp; Learning</td>
    <td class="tg-cly1">Trident: Efficient 4PC Framework for Privacy Preserving Machine Learning</td>
    <td class="tg-cly1"><a href="https://www.ndss-symposium.org/ndss-paper/trident-efficient-4pc-framework-for-privacy-preserving-machine-learning/">Link</a><br><a href="https://www.ndss-symposium.org/wp-content/uploads/2020/02/23005-paper.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Secure Sublinear Time Differentially Private Median Computation</td>
    <td class="tg-cly1"><a href="https://www.ndss-symposium.org/ndss-paper/secure-sublinear-time-differentially-private-median-computation/">Link</a><br><a href="https://www.ndss-symposium.org/wp-content/uploads/2020/02/24150-paper.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">CloudLeak: Large-Scale Deep Learning Models Stealing Through Adversarial Examples</td>
    <td class="tg-cly1"><a href="https://www.ndss-symposium.org/ndss-paper/cloudleak-large-scale-deep-learning-models-stealing-through-adversarial-examples/">Link</a><br><a href="https://www.ndss-symposium.org/wp-content/uploads/2020/02/24178-paper.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">BLAZE: Blazing Fast Privacy-Preserving Machine Learning</td>
    <td class="tg-cly1"><a href="https://www.ndss-symposium.org/ndss-paper/blaze-blazing-fast-privacy-preserving-machine-learning/">Link</a><br><a href="https://www.ndss-symposium.org/wp-content/uploads/2020/02/24202-paper.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-yla0" rowspan="4">Privacy</td>
    <td class="tg-cly1">Towards Plausible Graph Anonymization</td>
    <td class="tg-cly1"><a href="https://www.ndss-symposium.org/ndss-paper/towards-plausible-graph-anonymization/">Link</a><br><a href="https://www.ndss-symposium.org/wp-content/uploads/2020/02/23032-paper.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Adversarial Classification Under Differential Privacy</td>
    <td class="tg-cly1"><a href="https://www.ndss-symposium.org/ndss-paper/adversarial-classification-under-differential-privacy/">Link</a><br><a href="https://www.ndss-symposium.org/wp-content/uploads/2020/02/23047-paper.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">Locally Differentially Private Frequency Estimation with Consistency</td>
    <td class="tg-cly1"><a href="https://www.ndss-symposium.org/ndss-paper/locally-differentially-private-frequency-estimation-with-consistency/">Link</a><br><a href="https://www.ndss-symposium.org/wp-content/uploads/2020/02/24157-paper.pdf">PDF</a></td>
  </tr>
  <tr>
    <td class="tg-cly1">DESENSITIZATION: Privacy-Aware and Attack-Preserving Crash Report</td>
    <td class="tg-cly1"><a href="https://www.ndss-symposium.org/ndss-paper/desensitization-privacy-aware-and-attack-preserving-crash-report/">Link</a><br><a href="https://www.ndss-symposium.org/wp-content/uploads/2020/02/24428-paper.pdf">PDF</a></td>
  </tr>
</tbody>
</table>

