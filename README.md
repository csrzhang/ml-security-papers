# ML Security & Privacy Papers

[![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

> This repository contains a list of *ML Security* (poisoning, backdoor), *Robustness* (adversarial examples), *Privacy* (inference, recovery) and *Privacy & Anonymization* papers of **Top4 ** from <u>2020</u> to <u>2022</u>.



## Selected Papers in 2022

Here, we only list some representative papers of popular Machine Learning Attacks in 2022. For the full list, please refer to the following links.

### S&P 2022

| Topic                                  | Title                                                        |
| -------------------------------------- | ------------------------------------------------------------ |
| **Poisoning & Model Stealing Attacks** | [Property Inference from Poisoning](https://ieeexplore.ieee.org/document/9833623/) |
|                                        | [Reconstructing Training Data with Informed Adversaries](https://ieeexplore.ieee.org/document/9833677/) |
|                                        | [DeepSteal: Advanced Model Extractions Leveraging Efficient Weight Stealing in Memories](https://ieeexplore.ieee.org/document/9833743/) |
|                                        | [Model Stealing Attacks Against Inductive Graph Neural Networks](https://ieeexplore.ieee.org/document/9833607/) |
| **ML Attacks II**                      | [Bad Characters: Imperceptible NLP Attacks](https://ieeexplore.ieee.org/document/9833641/) |
|                                        | [LinkTeller: Recovering Private Edges from Graph Neural Networks via Influence Analysis](https://ieeexplore.ieee.org/document/9833806/) |
|                                        | [PICCOLO: Exposing Complex Backdoors in NLP Transformer Models](https://ieeexplore.ieee.org/document/9833579/) |
|                                        | [BadEncoder: Backdoor Attacks to Pre-trained Encoders in Self-Supervised Learning](https://ieeexplore.ieee.org/document/9833644/) |
| **ML Attacks II**                      | [Bad Characters: Imperceptible NLP Attacks](https://ieeexplore.ieee.org/document/9833641/) |
|                                        | [LinkTeller: Recovering Private Edges from Graph Neural Networks via Influence Analysis](https://ieeexplore.ieee.org/document/9833806/) |
|                                        | [PICCOLO: Exposing Complex Backdoors in NLP Transformer Models](https://ieeexplore.ieee.org/document/9833579/) |
|                                        | [BadEncoder: Backdoor Attacks to Pre-trained Encoders in Self-Supervised Learning](https://ieeexplore.ieee.org/document/9833644/) |



### CCS 2022

| Topic                       | Title                                                        |
| --------------------------- | ------------------------------------------------------------ |
| **Privacy Attacks in ML**   | SSLGuard: A Watermarking Scheme for Self-supervised Learning Pre-trained Encoders |
|                             | Auditing Membership Leakages of Multi-Exit Networks          |
|                             | StolenEncoder: Stealing Pre-trained Encoders                 |
|                             | Membership Inference Attacks by Exploiting Loss Trajectory   |
| **Inference Attacks to ML** | Group Property Inference Attacks Against Graph Neural Networks |
|                             | Are Attribute Inference Attacks Just Imputation?             |
|                             | Enhanced Membership Inference Attacks against Machine Learning Models |
|                             | Membership Inference Attacks and Generalization: A Causal Perspective |
| **FL Security**             | Eluding Secure Aggregation in Federated Learning via Model Inconsistency |
|                             | EIFFeL: Ensuring Integrity for Federated Learning            |
|                             | pMPL: A Robust Multi-Party Learning Framework with a Privileged Party |
|                             | Private and Reliable Neural Network Inference                |



### SEC 2022

| Topic                  | Title                                                        |
| ---------------------- | ------------------------------------------------------------ |
| **ML Attacks**         | [AutoDA: Automated Decision-based Iterative Adversarial Attacks](https://www.usenix.org/conference/usenixsecurity22/presentation/fu-qi) |
|                        | [Poison Forensics: Traceback of Data Poisoning Attacks in Neural Networks](https://www.usenix.org/conference/usenixsecurity22/presentation/shan) |
|                        | [Teacher Model Fingerprinting Attacks Against Transfer Learning](https://www.usenix.org/conference/usenixsecurity22/presentation/chen-yufei) |
|                        | [Hidden Trigger Backdoor Attack on NLP Models via Linguistic Style Manipulation](https://www.usenix.org/conference/usenixsecurity22/presentation/pan-hidden) |
|                        | [PoisonedEncoder: Poisoning the Unlabeled Pre-training Data in Contrastive Learning](https://www.usenix.org/conference/usenixsecurity22/presentation/liu-hongbin) |
| **ML Inference**       | [ML-Doctor: Holistic Risk Assessment of Inference Attacks Against Machine Learning Models](https://www.usenix.org/conference/usenixsecurity22/presentation/liu-yugeng) |
|                        | [Inference Attacks Against Graph Neural Networks](https://www.usenix.org/conference/usenixsecurity22/presentation/zhang-zhikun) |
|                        | [Membership Inference Attacks and Defenses in Neural Network Pruning](https://www.usenix.org/conference/usenixsecurity22/presentation/yuan-xiaoyong) |
|                        | [Are Your Sensitive Attributes Private? Novel Model Inversion Attribute Inference Attacks on Classification Models](https://www.usenix.org/conference/usenixsecurity22/presentation/mehnaz) |
| **Federated Learning** | [SIMC: ML Inference Secure Against Malicious Clients at Semi-Honest Cost](https://www.usenix.org/conference/usenixsecurity22/presentation/chandran) |
|                        | [Efficient Differentially Private Secure Aggregation for Federated Learning via Hardness of Learning with Errors](https://www.usenix.org/conference/usenixsecurity22/presentation/stevens) |
|                        | [Label Inference Attacks Against Vertical Federated Learning](https://www.usenix.org/conference/usenixsecurity22/presentation/fu-chong) |
|                        | [FLAME: Taming Backdoors in Federated Learning](https://www.usenix.org/conference/usenixsecurity22/presentation/nguyen) |



### NDSS 2022

| Topic                | Title                                                        |
| -------------------- | ------------------------------------------------------------ |
| **Attacks on ML/AI** | [ATTEQ-NN: Attention-based QoE-aware Evasive Backdoor Attacks](https://www.ndss-symposium.org/ndss-paper/auto-draft-238/) |
|                      | [RamBoAttack: A Robust and Query Efficient Deep Neural Network Decision Exploit](https://www.ndss-symposium.org/ndss-paper/auto-draft-239/) |
|                      | [Property Inference Attacks Against GANs](https://www.ndss-symposium.org/ndss-paper/auto-draft-240/) |
|                      | [Get a Model! Model Hijacking Attack Against Machine Learning Models](https://www.ndss-symposium.org/ndss-paper/auto-draft-241/) |
| **ML & AI 1**        | [Tetrad: Actively Secure 4PC for Secure Training and Inference](https://www.ndss-symposium.org/ndss-paper/auto-draft-202/) |
|                      | [MIRROR: Model Inversion for Deep LearningNetwork with High Fidelity](https://www.ndss-symposium.org/ndss-paper/auto-draft-203/) |
|                      | [Local and Central Differential Privacy for Robustness and Privacy in Federated Learning](https://www.ndss-symposium.org/ndss-paper/auto-draft-204/) |
|                      | [DeepSight: Mitigating Backdoor Attacks in Federated Learning Through Deep Model Inspection](https://www.ndss-symposium.org/ndss-paper/auto-draft-205/) |
| **ML & AI 2**        | [What You See is Not What the Network Infers: Detecting Adversarial Examples Based on Semantic Contradiction](https://www.ndss-symposium.org/ndss-paper/auto-draft-226/) |
|                      | [Euler: Detecting Network Lateral Movement via Scalable Temporal Graph Link Prediction](https://www.ndss-symposium.org/ndss-paper/auto-draft-227/) |
|                      | [Fooling the Eyes of Autonomous Vehicles: Robust Physical Adversarial Examples Against Traffic Sign Recognition Systems](https://www.ndss-symposium.org/ndss-paper/auto-draft-228/) |
|                      | [FedCRI: Federated Mobile Cyber-Risk Intelligence](https://www.ndss-symposium.org/ndss-paper/auto-draft-229/) |



**Longer List of Papers (2020-2022):**

-  [IEEE S&P Papers](IEEE_SP.md)
-  [ACM CCS Papers](ACM_CCS.md)
-  [USENIX Security Papers](USENIX_Security.md)
-  [ISOC NDSS Papers](ISOC_NDSS.md)



**Other Useful Resources:**

- [DBLP IEEE S&P](https://dblp.org/db/conf/sp/index.html)
- [DBLP ACM CCS](https://dblp.org/db/conf/ccs/index.html)
- [DBLP USENIX Security](https://dblp.org/db/conf/uss/index.html)
- [DBLP ISOC NDSS](https://dblp.org/db/conf/ndss/index.html)

