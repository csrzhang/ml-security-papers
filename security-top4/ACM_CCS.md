# ACM CCS Papers

### CCS 2022

**[Accepted Papers](https://www.sigsac.org/ccs/CCS2022/program/accepted-papers.html)**

| Topic                              | Title                                                        |
| ---------------------------------- | ------------------------------------------------------------ |
| **Poisoning & Backdoor ML**        | Identifying a Training-Set Attackâ€™s Target Using Renormalized Influence Estimation |
|                                    | FenceSitter: Black-box, Content-Agnostic, and Synchronization-Free Enrollment-Phase Attacks on Speaker Recognition Systems |
|                                    | Truth Serum: Poisoning Machine Learning Models to Reveal Their Secrets |
|                                    | LoneNeuron: a Highly-effective Feature-domain Neural Trojan using Invisible and Polymorphic Watermarks |
| **ML for Network Security**        | CERBERUS: Federated Prediction of Security Events            |
|                                    | Tor-based Malware Detection using Traffic Analysis           |
|                                    | AI/ML for Network Security: The Emperor has no Clothes       |
|                                    | Phishing URL Detection: A Network-based Approach Robust to Evasion |
| **Inference Attacks to ML**        | Group Property Inference Attacks Against Graph Neural Networks |
|                                    | Are Attribute Inference Attacks Just Imputation?             |
|                                    | Enhanced Membership Inference Attacks against Machine Learning Models |
|                                    | Membership Inference Attacks and Generalization: A Causal Perspective |
| **FL Security**                    | Eluding Secure Aggregation in Federated Learning via Model Inconsistency |
|                                    | EIFFeL: Ensuring Integrity for Federated Learning            |
|                                    | pMPL: A Robust Multi-Party Learning Framework with a Privileged Party |
|                                    | Private and Reliable Neural Network Inference                |
| **Attacks to ML**                  | Physical Hijacking Attacks against Object Trackers           |
|                                    | Feature Inference Attack on Shapley Values                   |
|                                    | When Evil Calls : Targeted Adversarial Voice over IP-Telephony Network |
|                                    | Order-Disorder: Imitation Adversarial Attacks for Black-box Neural Ranking Models |
| **Adversarial Examples in ML**     | Perception-Aware Attack: Creating Adversarial Music via Reverse-Engineering Human Perception |
|                                    | ''Is your explanation stable?'': A Robustness Evaluation Framework for Feature Attribution |
|                                    | Post-breach Recovery: Protection against White-box Adversarial Examples for Leaked DNN Models |
|                                    | Harnessing Perceptual Adversarial Patches for Crowd Counting |
| **Privacy Attacks in ML**          | SSLGuard: A Watermarking Scheme for Self-supervised Learning Pre-trained Encoders |
|                                    | Auditing Membership Leakages of Multi-Exit Networks          |
|                                    | StolenEncoder: Stealing Pre-trained Encoders                 |
|                                    | Membership Inference Attacks by Exploiting Loss Trajectory   |
| **Privacy Preserving ML**          | DPIS: an Enhanced Mechanism for Differentially Private SGD with Importance Sampling |
|                                    | NFGen: Automatic Non-linear Function Evaluation Code Generator for General-purpose MPC Platforms |
|                                    | On the Privacy Risks of Cell-Based NAS Architectures         |
|                                    | LPGNet: Link Private Graph Networks for Node Classification  |
| **Federated Analytics & Learning** | Distributed, Private, Sparse Histograms in the Two-Server Model |
|                                    | Selective MPC: Distributed Computation of Differentially Private Key-Value Statistics |
|                                    | STAR: Secret Sharing for Private Threshold Aggregation Reporting |
|                                    | Federated Boosted Decision Trees with Differential Privacy   |
| **Differential Privacy**           | Shifted Inverse: A General Mechanism for Monotonic Functions under User Differential Privacy |
|                                    | Frequency Estimation in the Shuffle Model with Almost a Single Message |
|                                    | Differentially Private Triangle and 4-Cycle Counting in the Shuffle Model |
|                                    | Widespread Underestimation of Sensitivity in Differentially Private Libraries and How to Fix It |
| **Privacy in Graphs**              | Graph Unlearning                                             |
|                                    | Finding MNEMON: Reviving Memories of Node Embeddings         |



### CCS 2021

**[Accepted Papers](https://www.sigsac.org/ccs/CCS2021/accepted-papers.html)**

| Topic                                  | Title                                                        |
| -------------------------------------- | ------------------------------------------------------------ |
| **Attacks and Robustness**             | [Black-box Adversarial Attacks on Commercial Speech Platforms with Minimal Information](https://dl.acm.org/doi/10.1145/3460120.3485383) |
|                                        | [A Hard Label Black-box Adversarial Attack Against Graph Neural Networks](https://dl.acm.org/doi/10.1145/3460120.3484796) |
|                                        | [Robust Adversarial Attacks Against DNN-Based Wireless Communication Systems](https://dl.acm.org/doi/10.1145/3460120.3484777) |
|                                        | [AI-Lancet: Locating Error-inducing Neurons to Optimize Neural Networks](https://dl.acm.org/doi/10.1145/3460120.3484818) |
| **Defenses for ML Robustness**         | [Learning Security Classifiers with Verified Global Robustness Properties](https://dl.acm.org/doi/10.1145/3460120.3484776) |
|                                        | [On the Robustness of Domain Constraints](https://dl.acm.org/doi/10.1145/3460120.3484570) |
|                                        | [Cert-RNN: Towards Certifying the Robustness of Recurrent Neural Networks](https://dl.acm.org/doi/10.1145/3460120.3484538) |
|                                        | [TSS: Transformation-Specific Smoothing for Robustness Certification](https://dl.acm.org/doi/10.1145/3460120.3485258) |
| **Privacy Attacks & Defenses for ML**  | [EncoderMI: Membership Inference against Pre-trained Encoders in Contrastive Learning](https://dl.acm.org/doi/10.1145/3460120.3484749) |
|                                        | [TableGAN-MCA: Evaluating Membership Collisions of GAN-Synthesized Tabular Data Releasing](https://dl.acm.org/doi/10.1145/3460120.3485251) |
|                                        | [Unleashing the Tiger: Inference Attacks on Split Learning](https://dl.acm.org/doi/10.1145/3460120.3485259) |
|                                        | [Locally Private Graph Neural Networks](https://dl.acm.org/doi/10.1145/3460120.3484565) |
|                                        | [DataLens: Scalable Privacy Preserving Training via Gradient Compression and Aggregation](https://dl.acm.org/doi/10.1145/3460120.3484579) |
| **Data Poisoning & Backdoors in ML**   | [Subpopulation Data Poisoning Attacks](https://dl.acm.org/doi/10.1145/3460120.3485368) |
|                                        | [Hidden Backdoors in Human-Centric Language Models](https://dl.acm.org/doi/10.1145/3460120.3484576) |
|                                        | [Backdoor Pre-trained Models Can Transfer to All](https://dl.acm.org/doi/10.1145/3460120.3485370) |
|                                        | [Feature-Indistinguishable Attack to Circumvent Trapdoor-Enabled Defense](https://dl.acm.org/doi/10.1145/3460120.3485378) |
|                                        | [DetectorGuard: Provably Securing Object Detectors against Localized Patch Hiding Attacks](https://dl.acm.org/doi/10.1145/3460120.3484757) |
| **Applications & Privacy of ML**       | [DeepAID: Interpreting and Improving Deep Learning-based Anomaly Detection in Security Applications](https://dl.acm.org/doi/10.1145/3460120.3484589) |
|                                        | [Structural Attack against Graph Based Android Malware Detection](https://dl.acm.org/doi/10.1145/3460120.3485387) |
|                                        | [PalmTree: Learning an Assembly Language Model for Instruction Embedding](https://dl.acm.org/doi/10.1145/3460120.3484587) |
|                                        | [A One-Pass Distributed and Private Sketch for Kernel Sums with Applications to Machine Learning at Scale](https://dl.acm.org/doi/10.1145/3460120.3485255) |
|                                        | [COINN: Crypto/ML Codesign for Oblivious Inference via Neural Networks](https://dl.acm.org/doi/10.1145/3460120.3484797) |
| **Inference Attacks**                  | [Honest-but-Curious Nets: Sensitive Attributes of Private Inputs Can Be Secretly Coded into the Classifiers' Outputs](https://dl.acm.org/doi/10.1145/3460120.3484533) |
|                                        | [Quantifying and Mitigating Privacy Risks of Contrastive Learning](https://dl.acm.org/doi/10.1145/3460120.3484571) |
|                                        | [Membership Inference Attacks Against Recommender Systems](https://dl.acm.org/doi/10.1145/3460120.3484770) |
|                                        | [Membership Leakage in Label-Only Exposures](https://dl.acm.org/doi/10.1145/3460120.3484575) |
|                                        | [When Machine Unlearning Jeopardizes Privacy](https://dl.acm.org/doi/10.1145/3460120.3484756) |
| **Differential Privacy**               | [Differential Privacy for Directional Data](https://dl.acm.org/doi/10.1145/3460120.3484734) |
|                                        | [Differentially Private Sparse Vectors with Low Error, Optimal Space, and Fast Access](https://dl.acm.org/doi/10.1145/3460120.3484735) |
|                                        | [Continuous Release of Data Streams under both Centralized and Local Differential Privacy](https://dl.acm.org/doi/10.1145/3460120.3484750) |
|                                        | [Side-Channel Attacks on Query-Based Data Anonymization](https://dl.acm.org/doi/10.1145/3460120.3484751) |
|                                        | [AHEAD: Adaptive Hierarchical Decomposition for Range Query under Local Differential Privacy](https://dl.acm.org/doi/10.1145/3460120.3485668) |
| **Privacy for Distributed Data & FL**  | [LEAP: Leakage-Abuse Attack on Efficiently Deployable, Efficiently Searchable Encryption with Partially Known Dataset](https://dl.acm.org/doi/10.1145/3460120.3484540) |
|                                        | [On the RÃ©nyi Differential Privacy of the Shuffle Model](https://dl.acm.org/doi/10.1145/3460120.3484794) |
|                                        | [Private Hierarchical Clustering in Federated Networks](https://dl.acm.org/doi/10.1145/3460120.3484822) |
|                                        | [Secure Multi-party Computation of Differentially Private Heavy Hitters](https://dl.acm.org/doi/10.1145/3460120.3484557) |
| **Database & Privacy**                 | [Reconstructing with Less: Leakage Abuse Attacks in Two Dimensions](https://dl.acm.org/doi/10.1145/3460120.3484552) |
|                                        | [Îµpsolute: Efficiently Querying Databases While Providing Differential Privacy](https://dl.acm.org/doi/10.1145/3460120.3484786) |
|                                        | [Compressed Oblivious Encoding for Homomorphically Encrypted Search](https://dl.acm.org/doi/10.1145/3460120.3484792) |
|                                        | [OnionPIR: Response Efficient Single-Server PIR](https://dl.acm.org/doi/10.1145/3460120.3485381) |
| **Audio Systems & Autonomous Driving** | [FakeWake: Understanding and Mitigating Fake Wake-up Words of Voice Assistants](https://dl.acm.org/doi/10.1145/3460120.3485365) |
|                                        | [Robust Detection of Machine-induced Audio Attacks in Intelligent Audio Systems with Microphone Array](https://dl.acm.org/doi/10.1145/3460120.3484755) |
|                                        | [Glowworm Attack: Optical TEMPEST Sound Recovery via a Device's Power Indicator LED](https://dl.acm.org/doi/10.1145/3460120.3484775) |
|                                        | [CapSpeaker: Injecting Voices to Microphones via Capacitors](https://dl.acm.org/doi/10.1145/3460120.3485389) |
|                                        | [I Can See the Light: Attacks on Autonomous Vehicles Using Invisible Lights](https://dl.acm.org/doi/10.1145/3460120.3484766) |
|                                        | [Can We Use Arbitrary Objects to Attack LiDAR Perception in Autonomous Driving?](https://dl.acm.org/doi/10.1145/3460120.3485377) |



### CCS 2020

**[Accepted Papers](https://www.sigsac.org/ccs/CCS2020/accepted-papers.html)**

| Topic                        | Title                                                        |
| ---------------------------- | ------------------------------------------------------------ |
| **Attacking & Defending ML** | [Gotta Catch'Em All: Using Honeypots to Catch Adversarial Attacks on Neural Networks](https://dl.acm.org/doi/10.1145/3372297.3417231) |
|                              | [A Tale of Evil Twins: Adversarial Inputs versus Poisoned Models](https://dl.acm.org/doi/10.1145/3372297.3417253) |
|                              | [DeepDyve: Dynamic Verification for Deep Neural Networks](https://dl.acm.org/doi/10.1145/3372297.3423338) |
|                              | [Composite Backdoor Attack for Deep Neural Network by Mixing Existing Benign Features](https://dl.acm.org/doi/10.1145/3372297.3423362) |
| **ML & Information Leakage** | [CrypTFlow2: Practical 2-Party Secure Inference](https://dl.acm.org/doi/10.1145/3372297.3417274) |
|                              | [GAN-Leaks: A Taxonomy of Membership Inference Attacks against Generative Models](https://dl.acm.org/doi/10.1145/3372297.3417238) |
|                              | [Analyzing Information Leakage of Updates to Natural Language Models](https://dl.acm.org/doi/10.1145/3372297.3417880) |
|                              | [Information Leakage in Embedding Models](https://dl.acm.org/doi/10.1145/3372297.3417270) |
| **Privacy**                  | [Private Summation in the Multi-Message Shuffle Model](https://dl.acm.org/doi/10.1145/3372297.3417242) |
|                              | [R2DP: A Universal and Automated Approach to Optimizing the Randomization Mechanisms of Differential Privacy for Utility Metrics with No Known Optimal Distributions](https://dl.acm.org/doi/10.1145/3372297.3417259) |
|                              | [Estimating g-Leakage via Machine Learning](https://dl.acm.org/doi/10.1145/3372297.3423363) |
|                              | [Implementing the Exponential Mechanism with Base-2 Differential Privacy](https://dl.acm.org/doi/10.1145/3372297.3417269) |



---



### CCS Workshops 2021

**[AISec'21: Artificial Intelligence and Security](https://dl.acm.org/doi/proceedings/10.1145/3474369)**

| Topic                     | Title                                                        |
| ------------------------- | ------------------------------------------------------------ |
| **Adversarial ML**        | [Unicode Evil: Evading NLP Systems Using Visual Similarities of Text Characters](https://dl.acm.org/doi/10.1145/3474369.3486871) |
|                           | [Adversarial Transfer Attacks With Unknown Data and Class Overlap](https://dl.acm.org/doi/10.1145/3474369.3486862) |
|                           | [SAT: Improving Adversarial Training via Curriculum-Based Loss Smoothing](https://dl.acm.org/doi/10.1145/3474369.3486878) |
|                           | [SEAT: Similarity Encoder by Adversarial Training for Detecting Model Extraction Attack Queries](https://dl.acm.org/doi/10.1145/3474369.3486863) |
|                           | [NNoculation: Catching BadNets in the Wild](https://dl.acm.org/doi/10.1145/3474369.3486874) |
| **Privacy-Preserving ML** | [FedV: Privacy-Preserving Federated Learning over Vertically Partitioned Data](https://dl.acm.org/doi/10.1145/3474369.3486872) |
|                           | [Differential Privacy Defenses and Sampling Attacks for Membership Inference](https://dl.acm.org/doi/10.1145/3474369.3486876) |



### CCS Workshops 2020

**[AISec'20: Artificial Intelligence and Security](https://dl.acm.org/doi/proceedings/10.1145/3411508)**

| Topic                  | Title                                                        |
| ---------------------- | ------------------------------------------------------------ |
| **Adversarial ML**     | [Where Does the Robustness Come from?: A Study of the Transformation-based Ensemble Defence](https://dl.acm.org/doi/10.1145/3411508.3421380) |
|                        | [Towards Certifiable Adversarial Sample Detection](https://dl.acm.org/doi/10.1145/3411508.3421381) |
|                        | [E-ABS: Extending the Analysis-By-Synthesis Robust Classification Model to More Complex Image Domains](https://dl.acm.org/doi/10.1145/3411508.3421382) |
|                        | [SCRAP: Synthetically Composed Replay Attacks vs. Adversarial Machine Learning Attacks against Mouse-based Biometric Authentication](https://dl.acm.org/doi/10.1145/3411508.3421378) |
| **Security & Privacy** | [eNNclave: Offline Inference with Model Confidentiality](https://dl.acm.org/doi/10.1145/3411508.3421376) |
|                        | [Risk-based Authentication Based on Network Latency Profiling](https://dl.acm.org/doi/10.1145/3411508.3421377) |
|                        | [Disabling Backdoor and Identifying Poison Data by using Knowledge Distillation in Backdoor Attacks on Deep Neural Networks](https://dl.acm.org/doi/10.1145/3411508.3421375) |



**[PPMLP'20: Privacy-Preserving Machine Learning in Practice](https://dl.acm.org/doi/proceedings/10.1145/3411501)**

| Type          | Title                                                        |
| ------------- | ------------------------------------------------------------ |
| **Full**      | [CryptoSPN: Expanding PPML beyond Neural Networks](https://dl.acm.org/doi/10.1145/3411501.3419417) |
|               | [Neither Private Nor Fair: Impact of Data Imbalance on Utility and Fairness in Differential Privacy](https://dl.acm.org/doi/10.1145/3411501.3419419) |
|               | [Secure Collaborative Training and Inference for XGBoost](https://dl.acm.org/doi/10.1145/3411501.3419420) |
|               | [Delphi: A Cryptographic Inference System for Neural Networks](https://dl.acm.org/doi/10.1145/3411501.3419418) |
|               | [Information Leakage by Model Weights on Federated Learning](https://dl.acm.org/doi/10.1145/3411501.3419423) |
|               | [Adversarial Detection on Graph Structured Data](https://dl.acm.org/doi/10.1145/3411501.3419424) |
| **Spotlight** | [MP2ML: A Mixed-Protocol Machine Learning Framework for Private Inference](https://dl.acm.org/doi/10.1145/3411501.3419425) |
|               | [Faster Secure Multiparty Computation of Adaptive Gradient Descent](https://dl.acm.org/doi/10.1145/3411501.3419427) |
|               | [SVM Learning for Default Prediction of Credit Card under Differential Privacy](https://dl.acm.org/doi/10.1145/3411501.3419431) |
|               | [A Systematic Comparison of Encrypted Machine Learning Solutions for Image Classification](https://dl.acm.org/doi/10.1145/3411501.3419432) |
|               | [Privacy-Preserving in Defending against Membership Inference Attacks](https://dl.acm.org/doi/10.1145/3411501.3419428) |
|               | [TinyGarble2: Smart, Efficient, and Scalable Yao's Garble Circuit](https://dl.acm.org/doi/10.1145/3411501.3419433) |
