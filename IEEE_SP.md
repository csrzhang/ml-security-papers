# IEEE S&P Papers

### S&P 2023

**[Accepted Papers (First Cycle)](https://www.ieee-security.org/TC/SP2023/program-papers.html)**

| Title                                                        |
| ------------------------------------------------------------ |
| FedRecover: Recovering from Poisoning Attacks in Federated Learning using Historical Information |
| Private, Efficient, and Accurate: Protecting Models Trained by Multi-party Learning with Differential Privacy |
| D-DAE: Defense-Penetrating Model Extraction Attacks          |
| Examining Zero-Shot Vulnerability Repair with Large Language Models |
| SoK: Certified Robustness for Deep Neural Networks           |
| RAB: Provable Robustness Against Backdoor Attacks            |
| SoK: Anti-Facial Recognition Technology                      |
| Deepfake Text Detection: Limitations and Opportunities       |



### S&P 2022

**[Accepted Papers](https://www.ieee-security.org/TC/SP2022/program-papers.html)**

| Topic                                  | Title                                                        |
| -------------------------------------- | ------------------------------------------------------------ |
| **Differential Privacy**               | [Statistical Quantification of Differential Privacy: A Local Approach](https://ieeexplore.ieee.org/document/9833689/) |
|                                        | [Locally Differentially Private Sparse Vector Aggregation](https://ieeexplore.ieee.org/document/9833635/) |
|                                        | [Differentially Private Histograms in the Shuffle Model from Fake Users](https://ieeexplore.ieee.org/document/9833614/) |
|                                        | [Differential Privacy and Swapping: Examining De-Identification’s Impact on Minority Representation and Privacy Preservation in the U.S. Census](https://ieeexplore.ieee.org/document/9833668/) |
|                                        | [Are We There Yet? Timing and Floating-Point Attacks on Differential Privacy Systems](https://ieeexplore.ieee.org/document/9833672/) |
| **Applications of ML**                 | [Asleep at the Keyboard? Assessing the Security of GitHub Copilot’s Code Contributions](https://ieeexplore.ieee.org/document/9833571/) |
|                                        | [Spinning Language Models: Risks of Propaganda-as-a-Service and Countermeasures](https://ieeexplore.ieee.org/document/9833572/) |
|                                        | [SoK: How Robust is Image Classification Deep Neural Network Watermarking?](https://ieeexplore.ieee.org/document/9833693/) |
|                                        | [Transcending TRANSCEND: Revisiting Malware Classification in the Presence of Concept Drift](https://ieeexplore.ieee.org/document/9833659/) |
|                                        | [Copy, Right? A Testing Framework for Copyright Protection of Deep Learning Models](https://ieeexplore.ieee.org/document/9833747/) |
| **Poisoning & Model Stealing Attacks** | [Property Inference from Poisoning](https://ieeexplore.ieee.org/document/9833623/) |
|                                        | [Reconstructing Training Data with Informed Adversaries](https://ieeexplore.ieee.org/document/9833677/) |
|                                        | [DeepSteal: Advanced Model Extractions Leveraging Efficient Weight Stealing in Memories](https://ieeexplore.ieee.org/document/9833743/) |
|                                        | [Model Stealing Attacks Against Inductive Graph Neural Networks](https://ieeexplore.ieee.org/document/9833607/) |
| **ML Attacks I**                       | [Back to the Drawing Board: A Critical Evaluation of Poisoning Attacks on Production Federated Learning](https://ieeexplore.ieee.org/document/9833647/) |
|                                        | [Model Orthogonalization: Class Distance Hardening in Neural Networks for Better Security](https://ieeexplore.ieee.org/document/9833688/) |
|                                        | [Universal 3-Dimensional Perturbations for Black-Box Attacks on Video Recognition Systems](https://ieeexplore.ieee.org/document/9833776/) |
|                                        | [“Adversarial Examples” for Proof-of-Learning](https://ieeexplore.ieee.org/document/9833596/) |
|                                        | [Transfer Attacks Revisited: A Large-Scale Empirical Study in Real Computer Vision Settings](https://ieeexplore.ieee.org/document/9833783/) |
|                                        | [Membership Inference Attacks From First Principles](https://ieeexplore.ieee.org/document/9833649/) |
| **ML Attacks II**                      | [Bad Characters: Imperceptible NLP Attacks](https://ieeexplore.ieee.org/document/9833641/) |
|                                        | [LinkTeller: Recovering Private Edges from Graph Neural Networks via Influence Analysis](https://ieeexplore.ieee.org/document/9833806/) |
|                                        | [PICCOLO: Exposing Complex Backdoors in NLP Transformer Models](https://ieeexplore.ieee.org/document/9833579/) |
|                                        | [BadEncoder: Backdoor Attacks to Pre-trained Encoders in Self-Supervised Learning](https://ieeexplore.ieee.org/document/9833644/) |
| **Privacy Applications**               | [Sphinx: Enabling Privacy-Preserving Online Learning over the Cloud](https://ieeexplore.ieee.org/document/9833648/) |



### S&P 2021

**[Accepted Papers](https://www.ieee-security.org/TC/SP2021/program-papers.html)**

| Topic                           | Title                                                        |
| ------------------------------- | ------------------------------------------------------------ |
| **Adversarial ML & Unlearning** | [Detecting AI Trojans Using Meta Neural Analysis](https://ieeexplore.ieee.org/document/9519467/) |
|                                 | [Adversarial Watermarking Transformer: Towards Tracing Text Provenance with Data Hiding](https://ieeexplore.ieee.org/document/9519400/) |
|                                 | [Machine Unlearning](https://ieeexplore.ieee.org/document/9519428/) |
| **Privacy**                     | [Defensive Technology Use by Political Activists During the Sudanese Revolution](https://ieeexplore.ieee.org/document/9519493/) |
|                                 | [DP-Sniper: Black-Box Discovery of Differential Privacy Violations using Classifiers](https://ieeexplore.ieee.org/document/9519405/) |
|                                 | [Is Private Learning Possible with Instance Encoding?](https://ieeexplore.ieee.org/document/9519450/) |
| **Speech Systems**              | [Who is Real Bob? Adversarial Attacks on Speaker Recognition Systems](https://ieeexplore.ieee.org/document/9519486/) |
|                                 | [Hear "No Evil", See "Kenansville": Efficient and Transferable Black-Box Attacks on Speech Recognition and Voice Identification Systems](https://ieeexplore.ieee.org/document/9519472/) |
|                                 | [SoK: The Faults in our ASRs: An Overview of Attacks against Automatic Speech Recognition and Speaker Identification Systems](https://ieeexplore.ieee.org/document/9519395/) |
| **Differential Privacy**        | [Learning Differentially Private Mechanisms](https://ieeexplore.ieee.org/document/9519410/) |
|                                 | [Adversary Instantiation: Lower Bounds for Differentially Private Machine Learning](https://ieeexplore.ieee.org/document/9519424/) |
|                                 | [Manipulation Attacks in Local Differential Privacy](https://ieeexplore.ieee.org/document/9519418/) |
| **ML Security & Privacy**       | [SIRNN: A Math Library for Secure RNN Inference](https://ieeexplore.ieee.org/document/9519413/) |
|                                 | [CryptGPU: Fast Privacy-Preserving Machine Learning on the GPU](https://ieeexplore.ieee.org/document/9519386/) |
|                                 | [Proof-of-Learning: Definitions and Practice](https://ieeexplore.ieee.org/document/9519402/) |
| **Autonomous Vehicles**         | [Poltergeist: Acoustic Adversarial Machine Learning against Cameras and Computer Vision](https://ieeexplore.ieee.org/document/9519394/) |
|                                 | [Invisible for both Camera and LiDAR: Security of Multi-Sensor Fusion based Perception in Autonomous Driving Under Physical-World Attacks](https://ieeexplore.ieee.org/document/9519442/) |



### S&P 2020

**[Accepted Papers](https://www.ieee-security.org/TC/SP2020/program-papers.html)**

| Topic                    | Title                                                        |
| ------------------------ | ------------------------------------------------------------ |
| **ML & Privacy**         | [The Value of Collaboration in Convex Machine Learning with Differential Privacy](https://ieeexplore.ieee.org/document/9152691/) |
|                          | [Automatically Detecting Bystanders in Photos to Reduce Privacy Risks](https://ieeexplore.ieee.org/document/9152778/) |
|                          | [CrypTFlow: Secure TensorFlow Inference](https://ieeexplore.ieee.org/document/9152660/) |
| **Differential Privacy** | [SoK: Differential Privacy as a Causal Property](https://ieeexplore.ieee.org/document/9152780/) |
|                          | [Private resource allocators and their applications](https://ieeexplore.ieee.org/document/9152764/) |
|                          | [Towards Effective Differential Privacy Communication for Users’ Data Sharing Decision and Comprehension](https://ieeexplore.ieee.org/document/9152658/) |
|                          | [A Programming Framework for Differential Privacy with Accuracy Concentration Bounds](https://ieeexplore.ieee.org/document/9152641/) |
| **Adversarial ML**       | [HopSkipJumpAttack: A Query-Efficient Decision-Based Attack](https://ieeexplore.ieee.org/document/9152788/) |
|                          | [Humpty Dumpty: Controlling Word Meanings via Corpus Poisoning](https://ieeexplore.ieee.org/document/9152608/) |
|                          | [Privacy Risks of General-Purpose Language Models](https://ieeexplore.ieee.org/document/9152761/) |
|                          | [Intriguing Properties of Adversarial ML Attacks in the Problem Space](https://ieeexplore.ieee.org/document/9152781/) |
| **Attacks & Forensics**  | [Throwing Darts in the Dark? Detecting Bots with Limited Data using Neural Data Augmentation](https://ieeexplore.ieee.org/document/9152805/) |



---



### S&P Workshops 2022

**[DLS'22: Deep Learning and Security Workshop](https://dls2022.ieee-security.org/)**

| Title                                                        |
| ------------------------------------------------------------ |
| [Misleading Deep-Fake Detection with GAN Fingerprints](https://ieeexplore.ieee.org/document/9833860/) |
| [Concept-based Adversarial Attacks: Tricking Humans and Classifiers Alike](https://ieeexplore.ieee.org/document/9833874/) |
| [Ares: A System-Oriented Wargame Framework for Adversarial ML](https://ieeexplore.ieee.org/document/9833895/) |
| [Parameterizing Activation Functions for Adversarial Robustness](https://ieeexplore.ieee.org/document/9833884/) |



### S&P Workshops 2021

**[DLS'21: Deep Learning and Security Workshop](https://www.ieee-security.org/TC/SP2021/SPW2021/dls_website/)**

| Title                                                        |
| ------------------------------------------------------------ |
| [Innocent Until Proven Guilty (IUPG): Building Deep Learning Models with Embedded Robustness to Out-Of-Distribution Content](https://ieeexplore.ieee.org/document/9474279/) |
| [SAFELearn: Secure Aggregation for private FEderated Learning](https://ieeexplore.ieee.org/document/9474309/) |
| [Applying Deep Learning to Combat Mass Robocalls](https://ieeexplore.ieee.org/document/9474299/) |
| [MMGuard: Automatically Protecting On-Device Deep Learning Models in Android Apps](https://ieeexplore.ieee.org/document/9474328/) |
| [BODMAS: An Open Dataset for Learning based Temporal Analysis of PE Malware](https://ieeexplore.ieee.org/document/9474321/) |
| [Binary Black-Box Attacks Against Static Malware Detectors with Reinforcement Learning in Discrete Action Spaces](https://ieeexplore.ieee.org/document/9474314/) |



### S&P Workshops 2020

**[DLS'20: Deep Learning and Security Workshop](https://www.ieee-security.org/TC/SPW2020/DLS/)**

| Title                                                        |
| ------------------------------------------------------------ |
| [Learning from Context: A Multi-View Deep Learning Architecture for Malware Detection](https://ieeexplore.ieee.org/document/9283862/) |
| [Attributing and Detecting Fake Images Generated by Known GANs](https://ieeexplore.ieee.org/document/9283853/) |
| [Adversarial Attacks Against LipNet: End-to-End Sentence Level Lipreading](https://ieeexplore.ieee.org/document/9283831/) |
| [Detecting Cyber Threats in Non-English Hacker Forums: An Adversarial Cross-Lingual Knowledge Transfer Approach](https://ieeexplore.ieee.org/document/9283883/) |
| [RTA3: A Real Time Adversarial Attack on Recurrent Neural Networks](https://ieeexplore.ieee.org/document/9283844/) |
| [Minimum-Norm Adversarial Examples on KNN and KNN based Models](https://ieeexplore.ieee.org/document/9283888/) |
| [Backdooring and Poisoning Neural Networks with Image-Scaling Attacks](https://ieeexplore.ieee.org/document/9283824/) |
| [SentiNet: Detecting Localized Universal Attacks Against Deep Learning Systems](https://ieeexplore.ieee.org/document/9283822/) |
| [Clipped BagNet: Defending Against Sticker Attacks with Clipped Bag-of-features](https://ieeexplore.ieee.org/document/9283860/) |
| [On the Robustness of Cooperative Multi-Agent Reinforcement Learning](https://ieeexplore.ieee.org/document/9283830/) |
| [Adversarial Machine Learning-Industry Perspectives](https://ieeexplore.ieee.org/document/9283867/) |

